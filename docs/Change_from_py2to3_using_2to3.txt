Changes made from Python2 to Python3 version of PyFluxPro

1) Copy directory PyFluxPro into new directory PyFluxPro3
2) Run 2to3 on PyFluxPro3 inside the directory which should be converted

2to3 --help  # shows help message
2to3 .       # dry run, shows what will be changed between python2 and python3
2to3 -w -n . # writes the new (python3) files and does not create backups of the old (python2) files

Result:

[cilli@meso] 3:59pm /home/cilli/OzFlux/PyFluxPro2_org:2to3 .
RefactoringTool: Skipping optional fixer: buffer
RefactoringTool: Skipping optional fixer: idioms
RefactoringTool: Skipping optional fixer: set_literal
RefactoringTool: Skipping optional fixer: ws_comma
RefactoringTool: Refactored ./PyFluxPro.py
--- ./PyFluxPro.py	(original)
+++ ./PyFluxPro.py	(refactored)
@@ -371,7 +371,7 @@
         """ Return true if a control file is an L1 file."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             # remove the common sections
             common_sections = ["level", "controlfile_name", "Files", "Global", "Output",
                                "Plots", "General", "Options", "Soil", "Massman", "GUI"]
@@ -380,9 +380,9 @@
                     cfg_sections.remove(section)
             # now loop over the remaining sections
             for section in cfg_sections:
-                subsections = self.cfg[section].keys()
+                subsections = list(self.cfg[section].keys())
                 for subsection in subsections:
-                    if "Attr" in self.cfg[section][subsection].keys():
+                    if "Attr" in list(self.cfg[section][subsection].keys()):
                         result = True
                         break
         except:
@@ -394,7 +394,7 @@
         result = False
         try:
             got_sections = False
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             if (("Files" in cfg_sections) and
                 ("Variables" in cfg_sections)):
                 got_sections = True
@@ -403,10 +403,10 @@
             qc_list = ["RangeCheck", "DiurnalCheck", "ExcludeDates", "DependencyCheck", "UpperCheck",
                        "LowerCheck", "ExcludeHours", "Linear", "CorrectWindDirection"]
             for section in ["Variables"]:
-                subsections = self.cfg[section].keys()
+                subsections = list(self.cfg[section].keys())
                 for subsection in subsections:
                     for qc in qc_list:
-                        if qc in self.cfg[section][subsection].keys():
+                        if qc in list(self.cfg[section][subsection].keys()):
                             got_qc = True
                             break
             # final check
@@ -420,7 +420,7 @@
         """ Return true if a control file is an L3 file."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             if ((("General" in cfg_sections) or
                 ("Soil" in cfg_sections) or
                 ("Massman" in cfg_sections)) and
@@ -434,10 +434,10 @@
         """ Return true if control file is concatenation."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             if "Files" in cfg_sections:
-                if (("Out" in self.cfg["Files"].keys()) and
-                    ("In" in self.cfg["Files"].keys())):
+                if (("Out" in list(self.cfg["Files"].keys())) and
+                    ("In" in list(self.cfg["Files"].keys()))):
                     result = True
         except:
             result = False
@@ -447,13 +447,13 @@
         """ Return true if control file is L4."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             for section in cfg_sections:
                 if section in ["Variables", "Drivers", "Fluxes"]:
-                    subsections = self.cfg[section].keys()
+                    subsections = list(self.cfg[section].keys())
                     for subsection in subsections:
-                        if (("GapFillFromAlternate" in self.cfg[section][subsection].keys()) or
-                            ("GapFillFromClimatology" in self.cfg[section][subsection].keys())):
+                        if (("GapFillFromAlternate" in list(self.cfg[section][subsection].keys())) or
+                            ("GapFillFromClimatology" in list(self.cfg[section][subsection].keys()))):
                             result = True
                             break
         except:
@@ -464,13 +464,13 @@
         """ Return true if control file is L5."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             for section in cfg_sections:
                 if section in ["Variables", "Drivers", "Fluxes"]:
-                    subsections = self.cfg[section].keys()
+                    subsections = list(self.cfg[section].keys())
                     for subsection in subsections:
-                        if (("GapFillUsingSOLO" in self.cfg[section][subsection].keys()) or
-                            ("GapFillUsingMDS" in self.cfg[section][subsection].keys())):
+                        if (("GapFillUsingSOLO" in list(self.cfg[section][subsection].keys())) or
+                            ("GapFillUsingMDS" in list(self.cfg[section][subsection].keys()))):
                             result = True
                             break
         except:
@@ -481,7 +481,7 @@
         """ Return true if control file is L6."""
         result = False
         try:
-            cfg_sections = self.cfg.keys()
+            cfg_sections = list(self.cfg.keys())
             if ("EcosystemRespiration" in cfg_sections or
                 "NetEcosystemExchange" in cfg_sections or
                 "GrossPrimaryProductivity" in cfg_sections):
@@ -552,7 +552,7 @@
         self.tabs.tab_dict[tab_index_current].cfg_changed = False
 
     def edit_preferences(self):
-        print "Edit/Preferences goes here"
+        print("Edit/Preferences goes here")
         pass
 
     def tabSelected(self, arg=None):
@@ -610,13 +610,13 @@
         # remove the corresponding entry in cfg_dict
         self.tabs.cfg_dict.pop(currentIndex)
         # and renumber the keys
-        for n in self.tabs.cfg_dict.keys():
+        for n in list(self.tabs.cfg_dict.keys()):
             if n > currentIndex:
                 self.tabs.cfg_dict[n-1] = self.tabs.cfg_dict.pop(n)
         # remove the corresponding entry in tab_dict
         self.tabs.tab_dict.pop(currentIndex)
         # and renumber the keys
-        for n in self.tabs.tab_dict.keys():
+        for n in list(self.tabs.tab_dict.keys()):
             if n > currentIndex:
                 self.tabs.tab_dict[n-1] = self.tabs.tab_dict.pop(n)
         # decrement the tab index
RefactoringTool: Refactored ./pfp_batch.py
--- ./pfp_batch.py	(original)
+++ ./pfp_batch.py	(refactored)
@@ -21,7 +21,7 @@
 
 def do_L1_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L1")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         logger.info("Starting L1 processing with %s", cf_file_name[1])
         try:
@@ -42,7 +42,7 @@
     return
 def do_L2_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L2")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting L2 processing with " + cf_file_name[1]
         logger.info(msg)
@@ -66,7 +66,7 @@
     return
 def do_L3_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L3")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting L3 processing with " + cf_file_name[1]
         logger.info(msg)
@@ -91,7 +91,7 @@
     return
 def do_ecostress_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "ecostress")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting ECOSTRESS output with " + cf_file_name[1]
         logger.info(msg)
@@ -110,7 +110,7 @@
     return
 def do_fluxnet_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "fluxnet")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting FluxNet output with " + cf_file_name[1]
         logger.info(msg)
@@ -122,7 +122,7 @@
     return
 def do_reddyproc_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "reddyproc")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting REddyProc output with " + cf_file_name[1]
         logger.info(msg)
@@ -134,7 +134,7 @@
     return
 def do_concatenate_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "concatenate")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         if not os.path.isfile(cf_level[i]):
             msg = " Control file " + cf_level[i] + " not found"
             logger.error(msg)
@@ -183,7 +183,7 @@
     return
 def do_climatology_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "climatology")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         if not os.path.isfile(cf_level[i]):
             msg = " Control file " + cf_level[i] + " not found"
             logger.error(msg)
@@ -206,7 +206,7 @@
     return
 def do_cpd_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "cpd")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting CPD with " + cf_file_name[1]
         logger.info(msg)
@@ -229,7 +229,7 @@
     return
 def do_mpt_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "mpt")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         cf_file_name = os.path.split(cf_level[i])
         msg = "Starting MPT with " + cf_file_name[1]
         logger.info(msg)
@@ -252,7 +252,7 @@
     return
 def do_L4_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L4")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         if not os.path.isfile(cf_level[i]):
             msg = " Control file " + cf_level[i] + " not found"
             logger.error(msg)
@@ -305,7 +305,7 @@
     return
 def do_L5_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L5")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         if not os.path.isfile(cf_level[i]):
             msg = " Control file " + cf_level[i] + " not found"
             logger.error(msg)
@@ -359,7 +359,7 @@
     return
 def do_L6_batch(cf_level):
     logger = pfp_log.change_logger_filename("pfp_log", "L6")
-    for i in cf_level.keys():
+    for i in list(cf_level.keys()):
         if not os.path.isfile(cf_level[i]):
             msg = " Control file " + cf_level[i] + " not found"
             logger.error(msg)
@@ -467,7 +467,7 @@
     # get the control file name
     if len(sys.argv) == 1:
         # not on the command line, so ask the user
-        cfg_file_path = raw_input("Enter the control file name: ")
+        cfg_file_path = input("Enter the control file name: ")
         # exit if nothing selected
         if len(cfg_file_path) == 0:
             sys.exit()
RefactoringTool: No changes to ./scripts/cfg.py
RefactoringTool: No changes to ./scripts/constants.py
RefactoringTool: No changes to ./scripts/meteorologicalfunctions.py
RefactoringTool: No changes to ./scripts/pfp_cfg.py
RefactoringTool: Refactored ./scripts/pfp_ck.py
--- ./scripts/pfp_ck.py	(original)
+++ ./scripts/pfp_ck.py	(refactored)
@@ -198,7 +198,7 @@
         flag_filtered[idx] = numpy.int32(61)
         attr_filtered = copy.deepcopy(attr)
         # update the series attributes
-        for item in indicators["final"]["attr"].keys():
+        for item in list(indicators["final"]["attr"].keys()):
             attr_filtered[item] = indicators["final"]["attr"][item]
         # update the "description" attribute
         attr_filtered[descr_level] = pfp_utils.append_string(attr_filtered[descr_level],
@@ -258,7 +258,7 @@
             opt["filter_list"] = [filter_string]
     # check to see if the series are in the data structure
     for item in opt["filter_list"]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " Series "+item+" given in FilterList not found in data stucture"
             logger.warning(msg)
             opt["filter_list"].remove(item)
@@ -291,10 +291,10 @@
 def CreateNewSeries(cf,ds):
     '''Create a new series using the MergeSeries or AverageSeries instructions.'''
     logger.info(' Checking for new series to create')
-    for ThisOne in cf['Variables'].keys():
-        if 'MergeSeries' in cf['Variables'][ThisOne].keys():
+    for ThisOne in list(cf['Variables'].keys()):
+        if 'MergeSeries' in list(cf['Variables'][ThisOne].keys()):
             pfp_ts.MergeSeries(cf,ds,ThisOne)
-        if 'AverageSeries' in cf['Variables'][ThisOne].keys():
+        if 'AverageSeries' in list(cf['Variables'][ThisOne].keys()):
             pfp_ts.AverageSeriesByElements(cf,ds,ThisOne)
 
 def do_SONICcheck(cf, ds, code=3):
@@ -318,7 +318,7 @@
     if "Diag_SONIC" in series_list:
         pass
     elif "Diag_CSAT" in series_list:
-        ds.series[unicode("Diag_SONIC")] = copy.deepcopy(ds.series["Diag_CSAT"])
+        ds.series[str("Diag_SONIC")] = copy.deepcopy(ds.series["Diag_CSAT"])
     else:
         msg = " Sonic diagnostics not found in data, skipping sonic checks ..."
         logger.warning(msg)
@@ -341,7 +341,7 @@
     msg = "  SONICCheck: Diag_SONIC rejected "+str(numpy.size(index))+" points"
     logger.info(msg)
     for label in sonic_list:
-        if label in ds.series.keys():
+        if label in list(ds.series.keys()):
             ds.series[label]["Data"][index] = numpy.float64(c.missing_value)
             ds.series[label]["Flag"][index] = numpy.int32(code)
         else:
@@ -361,7 +361,7 @@
         section = pfp_utils.get_cfsection(cf, series, mode='quiet')
         if section == None:
             return
-    if "DependencyCheck" not in cf[section][series].keys():
+    if "DependencyCheck" not in list(cf[section][series].keys()):
         return
     if "Source" not in cf[section][series]["DependencyCheck"]:
         msg = " DependencyCheck: keyword Source not found for series " + series + ", skipping ..."
@@ -386,7 +386,7 @@
     # loop over the precursor source list
     for item in source_list:
         # check the precursor is in the data structure
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " DependencyCheck: "+series+" precursor series "+item+" not found, skipping ..."
             logger.warning(msg)
             continue
@@ -422,9 +422,9 @@
     Author: PRI
     Date: Back in the day
     """
-    if 'DiurnalCheck' not in cf[section][series].keys():
-        return
-    if 'numsd' not in cf[section][series]["DiurnalCheck"].keys():
+    if 'DiurnalCheck' not in list(cf[section][series].keys()):
+        return
+    if 'numsd' not in list(cf[section][series]["DiurnalCheck"].keys()):
         return
     dt = ds.series["DateTime"]["Data"]
     Hdh = numpy.array([d.hour+d.minute/float(60) for d in dt])
@@ -464,7 +464,7 @@
     Date: September 2015
     """
     # check to see if we have a Diag_IRGA series to work with
-    if "Diag_IRGA" not in ds.series.keys():
+    if "Diag_IRGA" not in list(ds.series.keys()):
         msg = " Diag_IRGA not found in data, skipping IRGA checks ..."
         logger.warning(msg)
         return
@@ -483,7 +483,7 @@
     used_CO2 = False
     EC155_dependents = []
     for item in ['Signal_H2O','Signal_CO2','H2O_IRGA_Sd','CO2_IRGA_Sd']:
-        if item in ds.series.keys():
+        if item in list(ds.series.keys()):
             if ("Signal_H2O" in item) or ("Signal_CO2" in item):
                 used_Signal = True
             if ("H2O" in item) or ("Ah" in item):
@@ -510,7 +510,7 @@
     msg = "  "+irga_type+"Check: Total rejected " + str(numpy.size(idx))
     logger.info(msg)
     for ThisOne in EC155_list:
-        if ThisOne in ds.series.keys():
+        if ThisOne in list(ds.series.keys()):
             ds.series[ThisOne]['Data'][idx] = numpy.float64(c.missing_value)
             ds.series[ThisOne]['Flag'][idx] = numpy.int32(4)
         #else:
@@ -525,14 +525,14 @@
     Date: August 2017
     """
     # return if "EPQCFlagCheck" not used for this variable
-    if "EPQCFlagCheck" not in cf[section][series].keys():
+    if "EPQCFlagCheck" not in list(cf[section][series].keys()):
         return
     # check the "source" key exists and is a string
     if "source" not in cf[section][series]["EPQCFlagCheck"]:
         msg = "  EPQCFlagCheck: 'source' key not found for (" + series + ")"
         logger.error(msg)
         return
-    if not isinstance(cf[section][series]["EPQCFlagCheck"]["source"], basestring):
+    if not isinstance(cf[section][series]["EPQCFlagCheck"]["source"], str):
         msg = "  EPQCFlagCheck: 'source' value must be a string (" + series + ")"
         logger.error(msg)
         return
@@ -543,7 +543,7 @@
         msg = "  EPQCFlagCheck: 'reject' key not found for (" + series + ")"
         logger.error(msg)
         return
-    if not isinstance(cf[section][series]["EPQCFlagCheck"]["reject"], basestring):
+    if not isinstance(cf[section][series]["EPQCFlagCheck"]["reject"], str):
         msg = "  EPQCFlagCheck: 'reject' value must be a string (" + series + ")"
         logger.error(msg)
         return
@@ -565,10 +565,10 @@
     return
 
 def do_excludedates(cf,ds,section,series,code=6):
-    if 'ExcludeDates' not in cf[section][series].keys():
+    if 'ExcludeDates' not in list(cf[section][series].keys()):
         return
     ldt = ds.series['DateTime']['Data']
-    ExcludeList = cf[section][series]['ExcludeDates'].keys()
+    ExcludeList = list(cf[section][series]['ExcludeDates'].keys())
     NumExclude = len(ExcludeList)
     for i in range(NumExclude):
         exclude_dates_string = cf[section][series]['ExcludeDates'][str(i)]
@@ -604,9 +604,9 @@
     return
 
 def do_excludehours(cf,ds,section,series,code=7):
-    if 'ExcludeHours' not in cf[section][series].keys(): return
+    if 'ExcludeHours' not in list(cf[section][series].keys()): return
     ldt = ds.series['DateTime']['Data']
-    ExcludeList = cf[section][series]['ExcludeHours'].keys()
+    ExcludeList = list(cf[section][series]['ExcludeHours'].keys())
     NumExclude = len(ExcludeList)
     for i in range(NumExclude):
         exclude_hours_string = cf[section][series]['ExcludeHours'][str(i)]
@@ -679,7 +679,7 @@
         pass
     elif "Diag_7500" in series_list:
         # backward compatibility with early OFQC
-        ds.series[unicode("Diag_IRGA")] = copy.deepcopy(ds.series["Diag_7500"])
+        ds.series[str("Diag_IRGA")] = copy.deepcopy(ds.series["Diag_7500"])
     else:
         msg = " IRGA diagnostics not found in data, skipping IRGA checks ..."
         logger.warning(msg)
@@ -762,7 +762,7 @@
        LI75Lisat.  Additional checks are done for AGC_7500 (the LI-7500 AGC value),
        Ah_7500_Sd (standard deviation of absolute humidity) and Cc_7500_Sd (standard
        deviation of CO2 concentration).'''
-    if "Diag_IRGA" not in ds.series.keys():
+    if "Diag_IRGA" not in list(ds.series.keys()):
         msg = " Diag_IRGA not found in data, skipping IRGA checks ..."
         logger.warning(msg)
         return
@@ -777,7 +777,7 @@
     used_CO2 = False
     LI75_dependents = []
     for item in ['Signal_H2O','Signal_CO2','H2O_IRGA_Sd','CO2_IRGA_Sd','H2O_IRGA_Vr','CO2_IRGA_Vr']:
-        if item in ds.series.keys():
+        if item in list(ds.series.keys()):
             if ("Signal_H2O" in item) or ("Signal_CO2" in item):
                 used_Signal = True
             if ("H2O" in item) or ("Ah" in item):
@@ -800,14 +800,14 @@
         logger.warning(msg)
     flag = numpy.copy(ds.series['Diag_IRGA']['Flag'])
     for item in LI75_dependents:
-        if item in ds.series.keys():
+        if item in list(ds.series.keys()):
             idx = numpy.where(ds.series[item]['Flag']!=0)
             logger.info('  7500ACheck: '+item+' rejected '+str(numpy.size(idx))+' points')
             flag[idx] = numpy.int32(1)
     idx = numpy.where(flag != 0)[0]
     logger.info('  7500ACheck: Total ' + str(numpy.size(idx)))
     for ThisOne in LI75List:
-        if ThisOne in ds.series.keys():
+        if ThisOne in list(ds.series.keys()):
             ds.series[ThisOne]['Data'][idx] = numpy.float64(c.missing_value)
             ds.series[ThisOne]['Flag'][idx] = numpy.int32(4)
         else:
@@ -816,7 +816,7 @@
 
 def do_linear(cf,ds):
     level = ds.globalattributes['nc_level']
-    for ThisOne in cf['Variables'].keys():
+    for ThisOne in list(cf['Variables'].keys()):
         if pfp_utils.haskey(cf,ThisOne,'Linear'):
             pfp_ts.ApplyLinear(cf,ds,ThisOne)
         if pfp_utils.haskey(cf,ThisOne,'Drift'):
@@ -864,11 +864,11 @@
     Date: Back in the day
     """
     # check that RangeCheck has been requested for this series
-    if 'RangeCheck' not in cf[section][series].keys():
+    if 'RangeCheck' not in list(cf[section][series].keys()):
         return
     # check that the upper and lower limits have been given
-    if ("lower" not in cf[section][series]["RangeCheck"].keys() or
-        "upper" not in cf[section][series]["RangeCheck"].keys()):
+    if ("lower" not in list(cf[section][series]["RangeCheck"].keys()) or
+        "upper" not in list(cf[section][series]["RangeCheck"].keys())):
         msg = "RangeCheck: key not found in control file for "+series+", skipping ..."
         logger.warning(msg)
         return
@@ -918,7 +918,7 @@
     for item in ["Variables","Drivers","Fluxes"]:
         if item in cf:
             section = item
-            series_list = cf[item].keys()
+            series_list = list(cf[item].keys())
     if len(series_list)==0:
         msg = " do_qcchecks: Variables, Drivers or Fluxes section not found in control file, skipping QC checks ..."
         logger.warning(msg)
@@ -927,7 +927,7 @@
     # first time for general QC checks
     for series in series_list:
         # check the series is in the data structure
-        if series not in ds.series.keys():
+        if series not in list(ds.series.keys()):
             if mode!="quiet":
                 msg = " do_qcchecks: series "+series+" not found in data structure, skipping ..."
                 logger.warning(msg)
@@ -938,7 +938,7 @@
     # second time for dependencies
     for series in series_list:
         # check the series is in the data structure
-        if series not in ds.series.keys():
+        if series not in list(ds.series.keys()):
             if mode!="quiet":
                 msg = " do_qcchecks: series "+series+" not found in data structure, skipping ..."
                 logger.warning(msg)
@@ -969,7 +969,7 @@
     do_winddirectioncorrection(cf,ds,section,series)
 
 def do_winddirectioncorrection(cf, ds, section, series):
-    if "CorrectWindDirection" not in cf[section][series].keys():
+    if "CorrectWindDirection" not in list(cf[section][series].keys()):
         return
     pfp_ts.CorrectWindDirection(cf, ds, series)
 
@@ -1006,7 +1006,7 @@
     if "LowerCheck" not in cf[section][series]:
         return
     # Check to see if limits have been specified
-    if len(cf[section][series]["LowerCheck"].keys()) == 0:
+    if len(list(cf[section][series]["LowerCheck"].keys())) == 0:
         msg = "do_lowercheck: no date ranges specified"
         logger.info(msg)
         return
@@ -1053,7 +1053,7 @@
     if "UpperCheck" not in cf[section][series]:
         return
     # Check to see if limits have been specified
-    if len(cf[section][series]["UpperCheck"].keys()) == 0:
+    if len(list(cf[section][series]["UpperCheck"].keys())) == 0:
         msg = "do_uppercheck: no date ranges specified"
         logger.info(msg)
         return
RefactoringTool: Refactored ./scripts/pfp_clim.py
--- ./scripts/pfp_clim.py	(original)
+++ ./scripts/pfp_clim.py	(refactored)
@@ -186,8 +186,8 @@
     upper = float(upr_def)
     lower = float(lwr_def)
     for section in ['Variables']:
-        if label in cf[section].keys():
-            if 'RangeCheck' in cf[section][label].keys():
+        if label in list(cf[section].keys()):
+            if 'RangeCheck' in list(cf[section][label].keys()):
                 upper = float(cf[section][label]['RangeCheck']['Upper'])
                 lower = float(cf[section][label]['RangeCheck']['Lower'])
     return upper,lower
@@ -195,8 +195,8 @@
 def get_formatstring(cf,label,fmt_def=''):
     fmt_str = fmt_def
     for section in ['Variables']:
-        if label in cf[section].keys():
-            if 'Format' in cf[section][label].keys():
+        if label in list(cf[section].keys()):
+            if 'Format' in list(cf[section][label].keys()):
                 fmt_str = str(cf[section][label]['Format'])
     return fmt_str
 
@@ -208,8 +208,8 @@
     ds = pfp_io.nc_read_series(nc_filename)
     # calculate Fa if it is not in the data structure
     got_Fa = True
-    if "Fa" not in ds.series.keys():
-        if "Fn" in ds.series.keys() and "Fg" in ds.series.keys():
+    if "Fa" not in list(ds.series.keys()):
+        if "Fn" in list(ds.series.keys()) and "Fg" in list(ds.series.keys()):
             pfp_ts.CalculateAvailableEnergy(ds,Fa_out='Fa',Fn_in='Fn',Fg_in='Fg')
         else:
             got_Fa = False
@@ -237,9 +237,9 @@
     ntsInDay = int(24.0*60.0/float(ts))
     nDays = int(len(ldt))/ntsInDay
 
-    for ThisOne in cf['Variables'].keys():
-        if "AltVarName" in cf['Variables'][ThisOne].keys(): ThisOne = cf['Variables'][ThisOne]["AltVarName"]
-        if ThisOne in ds.series.keys():
+    for ThisOne in list(cf['Variables'].keys()):
+        if "AltVarName" in list(cf['Variables'][ThisOne].keys()): ThisOne = cf['Variables'][ThisOne]["AltVarName"]
+        if ThisOne in list(ds.series.keys()):
             logger.info(" Doing climatology for "+ThisOne)
             data,f,a = pfp_utils.GetSeriesasMA(ds,ThisOne,si=si,ei=ei)
             if numpy.ma.count(data)==0:
RefactoringTool: Refactored ./scripts/pfp_compliance.py
--- ./scripts/pfp_compliance.py	(original)
+++ ./scripts/pfp_compliance.py	(refactored)
@@ -199,14 +199,14 @@
     Date: November 2019
     """
     # save Fc_single if it exists - debug only
-    labels = ds.series.keys()
+    labels = list(ds.series.keys())
     if "Fc_single" in labels:
         variable = pfp_utils.GetVariable(ds, "Fc_single")
         variable["Label"] = "Fc_sinorg"
         pfp_utils.CreateVariable(ds, variable)
         pfp_utils.DeleteVariable(ds, "Fc_single")
     # do nothing if Fc_single exists
-    labels = ds.series.keys()
+    labels = list(ds.series.keys())
     if "Fc_single" in labels:
         pass
     # Fc_single may be called Fc_storage
@@ -285,7 +285,7 @@
             inc["OK"] = False
             return info
     # check the [In] section contains at least 1 entry
-    if len(cf["Files"]["In"].keys()) < 2:
+    if len(list(cf["Files"]["In"].keys())) < 2:
         msg = " Less than 2 input files specified"
         logger.error(msg)
         inc["OK"] = False
@@ -549,7 +549,7 @@
     """
     for attr in attributes:
         value = attributes[attr]
-        if not isinstance(value, basestring):
+        if not isinstance(value, str):
             continue
         if attr in ["rangecheck_lower", "rangecheck_upper", "diurnalcheck_numsd"]:
             if ("[" in value) and ("]" in value) and ("*" in value):
@@ -598,23 +598,23 @@
     # check site_name is in ds.globalattributes
     gattr_list = list(ds.globalattributes.keys())
     if "site_name" not in gattr_list:
-        print "Global attributes: site_name not found"
+        print("Global attributes: site_name not found")
     # check latitude and longitude are in ds.globalattributes
     if "latitude" not in gattr_list:
-        print "Global attributes: latitude not found"
+        print("Global attributes: latitude not found")
     else:
         lat_string = str(ds.globalattributes["latitude"])
         if len(lat_string) == 0:
-            print "Global attributes: latitude empty"
+            print("Global attributes: latitude empty")
         else:
             lat = pfp_utils.convert_anglestring(lat_string)
         ds.globalattributes["latitude"] = str(lat)
     if "longitude" not in gattr_list:
-        print "Global attributes: longitude not found"
+        print("Global attributes: longitude not found")
     else:
         lon_string = str(ds.globalattributes["longitude"])
         if len(lon_string) == 0:
-            print "Global attributes: longitude empty"
+            print("Global attributes: longitude empty")
         else:
             lon = pfp_utils.convert_anglestring(lon_string)
         ds.globalattributes["longitude"] = str(lon)
@@ -636,14 +636,14 @@
                     tz = tf.timezone_at(lng=lon, lat=lat)
                     ds.globalattributes["time_zone"] = tz
                 else:
-                    print "Global attributes: unable to define time zone"
+                    print("Global attributes: unable to define time zone")
                     ds.globalattributes["time_zone"] = ""
     # add or change global attributes as required
     gattr_list = sorted(list(cfg["Global"].keys()))
     for gattr in gattr_list:
         ds.globalattributes[gattr] = cfg["Global"][gattr]
     # remove deprecated global attributes
-    flag_list = [g for g in ds.globalattributes.keys() if "Flag" in g]
+    flag_list = [g for g in list(ds.globalattributes.keys()) if "Flag" in g]
     others_list = ["end_datetime", "start_datetime", "Functions", "doi"]
     remove_list = others_list + flag_list
     for gattr in list(ds.globalattributes.keys()):
@@ -1155,7 +1155,7 @@
     # initialise the return logical
     ok = True
     # check to see if we have an old style L6 control file
-    if "ER" in cfg.keys() or "NEE" in cfg.keys() or "GPP" in cfg.keys():
+    if "ER" in list(cfg.keys()) or "NEE" in list(cfg.keys()) or "GPP" in list(cfg.keys()):
         ok = False
         msg = "This is an old version of the L6 control file.\n"
         msg = msg + "Close the L6 control file and create a new one from\n"
@@ -1217,7 +1217,7 @@
 def parse_cfg_plots_title(cfg, key1, key2):
     """ Parse the [Plots] section for a title."""
     title = key2
-    for item in cfg[key1][key2].keys():
+    for item in list(cfg[key1][key2].keys()):
         if item.lower() == "title":
             title = cfg[key1][key2][item]
             del cfg[key1][key2][item]
RefactoringTool: Refactored ./scripts/pfp_cpd.py
--- ./scripts/pfp_cpd.py	(original)
+++ ./scripts/pfp_cpd.py	(refactored)
@@ -201,7 +201,7 @@
         if bootstrap_flag == False:
             #if 'results_output_path' in d.keys():
                 #results_df.to_csv(os.path.join(d['results_output_path'],'Observational_ustar_threshold_statistics.csv'))
-            if 'plot_path' in d.keys() and d["plot_tclass"]:
+            if 'plot_path' in list(d.keys()) and d["plot_tclass"]:
                 logger.info('Doing plotting for observational data')
                 d["nFig"] = 0
                 fig_nums = plt.get_fignums()
@@ -254,7 +254,7 @@
 
     # If requested by user, plot: 1) histograms of u* thresholds for each year;
     #                             2) normalised a1 and a2 values
-    if 'plot_path' in d.keys():
+    if 'plot_path' in list(d.keys()):
         logger.info(' Plotting u* histograms for all valid b model thresholds for all valid years')
         for j in output_stats_df.index:
             if j in all_results_df.index:
@@ -305,10 +305,10 @@
     results_path = path_out
     if not os.path.isdir(results_path): os.makedirs(results_path)
     # get a dictionary of the variable names
-    var_list = cf["Variables"].keys()
+    var_list = list(cf["Variables"].keys())
     names = {}
     for item in var_list:
-        if "AltVarName" in cf["Variables"][item].keys():
+        if "AltVarName" in list(cf["Variables"][item].keys()):
             names[item] = cf["Variables"][item]["AltVarName"]
         else:
             names[item] = item
@@ -325,16 +325,16 @@
     # now get the data
     d = {}
     f = {}
-    for item in names.keys():
+    for item in list(names.keys()):
         data,flag,attr = pfp_utils.GetSeries(ds,names[item])
         d[item] = np.where(data==c.missing_value,np.nan,data)
         f[item] = flag
     # set all data to NaNs if any flag not 0 or 10
-    for item in f.keys():
+    for item in list(f.keys()):
         for f_OK in [0,10]:
             idx = np.where(f[item]!=0)[0]
             if len(idx)!=0:
-                for itemd in d.keys():
+                for itemd in list(d.keys()):
                     d[itemd][idx] = np.nan
     d["Year"] = np.array([ldt.year for ldt in dt])
     df=pd.DataFrame(d,index=dt)
RefactoringTool: Refactored ./scripts/pfp_cpd2.py
--- ./scripts/pfp_cpd2.py	(original)
+++ ./scripts/pfp_cpd2.py	(refactored)
@@ -53,10 +53,10 @@
     if not os.path.isdir(results_path):
         os.makedirs(results_path)
     # get a dictionary of the variable names
-    var_list = cf["Variables"].keys()
+    var_list = list(cf["Variables"].keys())
     names = {}
     for item in var_list:
-        if "AltVarName" in cf["Variables"][item].keys():
+        if "AltVarName" in list(cf["Variables"][item].keys()):
             names[item] = cf["Variables"][item]["AltVarName"]
         else:
             names[item] = item
@@ -64,7 +64,7 @@
     logger.info(" Reading netCDF file " + file_in)
     ds = pfp_io.nc_read_series(file_in)
     # get the single-point storage, Fc_single, if available
-    if apply_storage and "Fc_storage" not in ds.series.keys():
+    if apply_storage and "Fc_storage" not in list(ds.series.keys()):
         pfp_ts.CalculateFcStorageSinglePoint(cf, ds, Fc_out="Fc_single")
         Fc_single = pfp_utils.GetVariable(ds, "Fc_single")
         Fc_single["Label"] = "Fc_storage"
@@ -379,7 +379,7 @@
     nPerSeason = round(float(ntAnnual) / float(nSeasons))
     # Move December to beginning of year and date as previous year.
     itD = numpy.where(M == 12)[0]
-    itReOrder = numpy.concatenate([range(min(itD), nt), range(0, (min(itD)))])
+    itReOrder = numpy.concatenate([list(range(min(itD), nt)), list(range(0, (min(itD))))])
     # PRI Spetember 2019 - I don't think this works as intended using t as generated
     t[itD] = t[itD] - EndDOY
     t = t[itReOrder]
@@ -404,12 +404,12 @@
         xls_out["cpdBin_input"][iSeason] = {}
         xls_out["cpdBin_output"][iSeason] = {}
         if iSeason == 0:
-            jtSeason = range(0, int(nPerSeason))
+            jtSeason = list(range(0, int(nPerSeason)))
         else:
             if iSeason == nSeasons-1:
-                jtSeason = range(int((nSeasons - 1)*nPerSeason), int(ntAnnual))
+                jtSeason = list(range(int((nSeasons - 1)*nPerSeason), int(ntAnnual)))
             else:
-                jtSeason = range(int(iSeason*nPerSeason), int((iSeason + 1)*nPerSeason))
+                jtSeason = list(range(int(iSeason*nPerSeason), int((iSeason + 1)*nPerSeason)))
         itSeason = itAnnual[jtSeason]
         ntSeason = len(itSeason)
         nStrata = numpy.floor(ntSeason / (nBins*nPerBin))
@@ -1079,9 +1079,9 @@
     FracModeD = []
     FracSelect = []
     # Compute window sizes etc.
-    nBoot = len(Stats.keys())
-    nWindows = len(Stats[0].keys())
-    nStrata = len(Stats[0][0].keys())
+    nBoot = len(list(Stats.keys()))
+    nWindows = len(list(Stats[0].keys()))
+    nStrata = len(list(Stats[0][0].keys()))
     if nBoot == 1:
         nStrataN = 0.5
     else:
RefactoringTool: Refactored ./scripts/pfp_func.py
--- ./scripts/pfp_func.py	(original)
+++ ./scripts/pfp_func.py	(refactored)
@@ -28,11 +28,11 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     for item in [RH_in,Ta_in]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " AhfromRH: Requested series "+item+" not found, "+Ah_out+" not calculated"
             logger.error(msg)
             return 0
-    if Ah_out in ds.series.keys():
+    if Ah_out in list(ds.series.keys()):
         msg = " AhfromRH: Output series "+Ah_out+" already exists, skipping ..."
         logger.error(msg)
         return 0
@@ -64,11 +64,11 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     for item in [MR_in,Ta_in,ps_in]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " AhfromMR: Requested series "+item+" not found, "+Ah_out+" not calculated"
             logger.error(msg)
             return 0
-    if Ah_out in ds.series.keys():
+    if Ah_out in list(ds.series.keys()):
         msg = " AhfromMR: Output series "+Ah_out+" already exists, skipping ..."
         logger.error(msg)
         return 0
@@ -93,7 +93,7 @@
     Author: PRI
     Date: February 2018
     """
-    if T_in not in ds.series.keys():
+    if T_in not in list(ds.series.keys()):
         msg = " ConvertK2C: variable " + T_in + " not found, skipping ..."
         logger.warning(msg)
         return 0
@@ -175,7 +175,7 @@
     ds.series[dt_out]["Attr"]["long_name"] = "Datetime in local timezone"
     ds.series[dt_out]["Attr"]["units"] = "None"
     # now remove any "data"" from empty lines
-    series_list = ds.series.keys()
+    series_list = list(ds.series.keys())
     if dt_out in series_list: series_list.remove(dt_out)
     for item in series_list:
         ds.series[item]["Data"] = ds.series[item]["Data"][idx]
@@ -184,7 +184,7 @@
     return 1
 
 def DateTimeFromTimeStamp(ds, dt_out, TimeStamp_in, fmt=""):
-    if TimeStamp_in not in ds.series.keys():
+    if TimeStamp_in not in list(ds.series.keys()):
         logger.error(" Required series "+TimeStamp_in+" not found")
         return 0
     TimeStamp = ds.series[TimeStamp_in]["Data"]
@@ -209,7 +209,7 @@
     ds.series[dt_out]["Attr"]["long_name"] = "Datetime in local timezone"
     ds.series[dt_out]["Attr"]["units"] = "None"
     # now remove any "data"" from empty lines
-    series_list = ds.series.keys()
+    series_list = list(ds.series.keys())
     if dt_out in series_list: series_list.remove(dt_out)
     for item in series_list:
         ds.series[item]["Data"] = ds.series[item]["Data"][idx]
@@ -231,10 +231,10 @@
     return 1
 
 def DateTimeFromDateAndTimeString(ds, dt_out, Date, Time):
-    if Date not in ds.series.keys():
+    if Date not in list(ds.series.keys()):
         logger.error(" Requested date series "+Date+" not found")
         return 0
-    if Time not in ds.series.keys():
+    if Time not in list(ds.series.keys()):
         logger.error(" Requested time series "+Time+" not found")
         return 0
     DateString = ds.series[Date]["Data"]
@@ -252,7 +252,7 @@
     ds.series[dt_out]["Attr"]["long_name"] = "Datetime in local timezone"
     ds.series[dt_out]["Attr"]["units"] = "None"
     # now remove any "data"" from empty lines
-    series_list = ds.series.keys()
+    series_list = list(ds.series.keys())
     if dt_out in series_list: series_list.remove(dt_out)
     for item in series_list:
         ds.series[item]["Data"] = ds.series[item]["Data"][idx]
@@ -269,11 +269,11 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     for item in [Ah_in, Ta_in, ps_in]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " MRfromAh: Requested series "+item+" not found, "+MR_out+" not calculated"
             logger.error(msg)
             return 0
-    if MR_out in ds.series.keys():
+    if MR_out in list(ds.series.keys()):
         msg = " MRfromAh: Output series "+MR_out+" already exists, skipping ..."
         logger.error(msg)
         return 0
@@ -297,11 +297,11 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     for item in [RH_in, Ta_in, ps_in]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " MRfromRH: Requested series "+item+" not found, "+MR_out+" not calculated"
             logger.error(msg)
             return 0
-    if MR_out in ds.series.keys():
+    if MR_out in list(ds.series.keys()):
         msg = " MRfromRH: Output series "+MR_out+" already exists, skipping ..."
         logger.error(msg)
         return 0
RefactoringTool: Refactored ./scripts/pfp_gf.py
--- ./scripts/pfp_gf.py	(original)
+++ ./scripts/pfp_gf.py	(refactored)
@@ -35,13 +35,13 @@
     ts = int(ds.globalattributes["time_step"])
     ldt = pfp_utils.GetVariable(ds, "DateTime")
     gf_drivers = []
-    for label in info[called_by]["outputs"].keys():
+    for label in list(info[called_by]["outputs"].keys()):
         gf_drivers = gf_drivers + info[called_by]["outputs"][label]["drivers"]
     drivers = list(set(gf_drivers))
     drivers_with_missing = {}
     # loop over the drivers and check for missing data
     for label in drivers:
-        if label not in ds.series.keys():
+        if label not in list(ds.series.keys()):
             msg = "  Requested driver (" + label + ") not found in data structure"
             logger.error(msg)
             ds.returncodes["message"] = msg
@@ -54,17 +54,17 @@
             drivers_with_missing[label] = {"count": len(idx),
                                            "dates": ldt["Data"][idx]}
     # check to see if any of the drivers have missing data
-    if len(drivers_with_missing.keys()) == 0:
+    if len(list(drivers_with_missing.keys())) == 0:
         msg = "  No missing data found in drivers"
         logger.info(msg)
         return
     # deal with drivers that contain missing data points
     logger.warning("!!!!!")
-    s = ','.join(drivers_with_missing.keys())
+    s = ','.join(list(drivers_with_missing.keys()))
     msg = "!!!!! The following variables contain missing data " + s
     logger.warning(msg)
     logger.warning("!!!!!")
-    for label in drivers_with_missing.keys():
+    for label in list(drivers_with_missing.keys()):
         var = pfp_utils.GetVariable(ds, label)
         # check to see if this variable was imported
         if "end_date" in var["Attr"]:
@@ -73,10 +73,10 @@
             # it was, so perhaps this variable finishes before the tower data
             drivers_with_missing[label]["end_date"].append(dateutil.parser.parse(var["Attr"]["end_date"]))
     # check to see if any variables with missing data have an end date
-    dwmwed = [l for l in drivers_with_missing.keys() if "end_date" in drivers_with_missing[l]]
+    dwmwed = [l for l in list(drivers_with_missing.keys()) if "end_date" in drivers_with_missing[l]]
     if len(dwmwed) == 0:
         # return with error message if no variables have end date
-        s = ','.join(drivers_with_missing.keys())
+        s = ','.join(list(drivers_with_missing.keys()))
         msg = "  Unable to resolve missing data in variables " + s
         logger.error(msg)
         ds.returncodes["message"] = msg
@@ -91,11 +91,11 @@
         return
     msg = "  Truncating data to end date of imported variable"
     logger.info(msg)
-    dwmed = [drivers_with_missing[l]["end_date"] for l in drivers_with_missing.keys()]
+    dwmed = [drivers_with_missing[l]["end_date"] for l in list(drivers_with_missing.keys())]
     end_date = numpy.min(dwmed)
     ei = pfp_utils.GetDateIndex(ldt["Data"], end_date, ts=ts)
     # loop over the variables in the data structure
-    for label in ds.series.keys():
+    for label in list(ds.series.keys()):
         var = pfp_utils.GetVariable(ds, label, start=0, end=ei)
         pfp_utils.CreateVariable(ds, var)
     # update the global attributes
@@ -113,9 +113,9 @@
                                            "dates": ldt["Data"][idx],
                                            "end_date":[]}
     # check to see if any of the drivers still have missing data
-    if len(drivers_with_missing.keys()) != 0:
+    if len(list(drivers_with_missing.keys())) != 0:
         # return with error message if no variables have end date
-        s = ','.join(drivers_with_missing.keys())
+        s = ','.join(list(drivers_with_missing.keys()))
         msg = "  Unable to resolve missing data in variables " + s
         logger.error(msg)
         ds.returncodes["message"] = msg
@@ -149,7 +149,7 @@
     nperday = 24 * 60/ts
     max_short_gap_records = max_short_gap_days * nperday
     # get a list of variables being gap filled
-    targets = cf["Fluxes"].keys()
+    targets = list(cf["Fluxes"].keys())
     targets_with_long_gaps = []
     # loop over the targets, get the duration and check to see if any exceed the maximum
     for target in targets:
@@ -158,7 +158,7 @@
                                               "got_long_gap_method": False}
         # loop over possible long gap filling methods
         for long_gap_method in ["GapFillLongSOLO"]:
-            if long_gap_method in cf["Fluxes"][target].keys():
+            if long_gap_method in list(cf["Fluxes"][target].keys()):
                 # set logical true if long gap filling method present
                 l5_info["CheckGapLengths"][target]["got_long_gap_method"] = True
         # get the data
@@ -235,23 +235,23 @@
     opt = pfp_utils.get_keyvaluefromcf(cf, ["Options"], "KeepIntermediateSeries", default="No")
     l4_info["RemoveIntermediateSeries"] = {"KeepIntermediateSeries": opt, "not_output": []}
     # loop over target variables
-    for target in cf["Drivers"].keys():
-        if "GapFillFromAlternate" in cf["Drivers"][target].keys():
+    for target in list(cf["Drivers"].keys()):
+        if "GapFillFromAlternate" in list(cf["Drivers"][target].keys()):
             gfalternate_createdict(cf, ds, l4_info, target, "GapFillFromAlternate")
             # check to see if something went wrong
             if ds.returncodes["value"] != 0:
                 # if it has, return to calling routine
                 return l4_info
-        if "GapFillFromClimatology" in cf["Drivers"][target].keys():
+        if "GapFillFromClimatology" in list(cf["Drivers"][target].keys()):
             gfClimatology_createdict(cf, ds, l4_info, target, "GapFillFromClimatology")
             if ds.returncodes["value"] != 0:
                 return l4_info
-        if "MergeSeries" in cf["Drivers"][target].keys():
+        if "MergeSeries" in list(cf["Drivers"][target].keys()):
             gfMergeSeries_createdict(cf, ds, l4_info, target, "MergeSeries")
     # check to make sure at least 1 output is defined
     outputs = []
     for method in ["GapFillFromAlternate", "GapFillFromClimatology"]:
-        outputs = outputs + l4_info[method]["outputs"].keys()
+        outputs = outputs + list(l4_info[method]["outputs"].keys())
     if len(outputs) == 0:
         msg = " No output variables defined, quitting L4 ..."
         logger.error(msg)
@@ -274,22 +274,22 @@
     # add key for suppressing output of intermediate variables e.g. Ta_aws
     opt = pfp_utils.get_keyvaluefromcf(cf, ["Options"], "KeepIntermediateSeries", default="No")
     l5_info["RemoveIntermediateSeries"] = {"KeepIntermediateSeries": opt, "not_output": []}
-    for target in cf["Fluxes"].keys():
-        if "GapFillUsingSOLO" in cf["Fluxes"][target].keys():
+    for target in list(cf["Fluxes"].keys()):
+        if "GapFillUsingSOLO" in list(cf["Fluxes"][target].keys()):
             gfSOLO_createdict(cf, ds, l5_info, target, "GapFillUsingSOLO", 510)
             # check to see if something went wrong
             if ds.returncodes["value"] != 0:
                 # if it has, return to calling routine
                 return l5_info
-        if "GapFillLongSOLO" in cf["Fluxes"][target].keys():
+        if "GapFillLongSOLO" in list(cf["Fluxes"][target].keys()):
             gfSOLO_createdict(cf, ds, l5_info, target, "GapFillLongSOLO", 520)
             if ds.returncodes["value"] != 0:
                 return l5_info
-        if "GapFillUsingMDS" in cf["Fluxes"][target].keys():
+        if "GapFillUsingMDS" in list(cf["Fluxes"][target].keys()):
             gfMDS_createdict(cf, ds, l5_info, target, "GapFillUsingMDS", 530)
             if ds.returncodes["value"] != 0:
                 return l5_info
-        if "MergeSeries" in cf["Fluxes"][target].keys():
+        if "MergeSeries" in list(cf["Fluxes"][target].keys()):
             gfMergeSeries_createdict(cf, ds, l5_info, target, "MergeSeries")
     return l5_info
 
@@ -297,7 +297,7 @@
     ds_alt = {}
     l4ao = l4_info["GapFillFromAlternate"]["outputs"]
     # get a list of file names
-    files = [l4ao[output]["file_name"] for output in l4ao.keys()]
+    files = [l4ao[output]["file_name"] for output in list(l4ao.keys())]
     # read the alternate files
     for f in files:
         # if the file has not already been read, do it now
@@ -322,7 +322,7 @@
     descr_level = "description_" + ds.globalattributes["nc_level"]
     ds.series[label]["Attr"][descr_level] = ""
     # create the alternate data settings directory
-    if called_by not in l4_info.keys():
+    if called_by not in list(l4_info.keys()):
         # create the GapFillFromAlternate dictionary
         l4_info[called_by] = {"outputs": {}, "info": {}, "gui": {}}
         # only need to create the ["info"] dictionary on the first pass
@@ -334,9 +334,9 @@
     # get the outputs section
     gfalternate_createdict_outputs(cf, l4_info, label, called_by)
     # create an empty series in ds if the alternate output series doesn't exist yet
-    outputs = l4_info[called_by]["outputs"].keys()
+    outputs = list(l4_info[called_by]["outputs"].keys())
     for output in outputs:
-        if output not in ds.series.keys():
+        if output not in list(ds.series.keys()):
             l4_info["RemoveIntermediateSeries"]["not_output"].append(output)
             variable = pfp_utils.CreateEmptyVariable(output, nrecs)
             variable["Attr"][descr_level] = l4_info[called_by]["outputs"][output]["source"]
@@ -400,7 +400,7 @@
 def gfalternate_createdict_outputs(cf, l4_info, label, called_by):
     flag_codes = {"default": 400, "aws": 410, "access": 420, "erai": 430, "era5": 440}
     # name of alternate output series in ds
-    outputs = cf["Drivers"][label][called_by].keys()
+    outputs = list(cf["Drivers"][label][called_by].keys())
     # loop over the outputs listed in the control file
     l4ao = l4_info[called_by]["outputs"]
     cfalt = cf["Drivers"][label][called_by]
@@ -417,11 +417,11 @@
         l4ao[output]["source"] = opt.lower()
         # output QC flag code
         l4ao[output]["flag_code"] = flag_codes["default"]
-        if l4ao[output]["source"] in flag_codes.keys():
+        if l4ao[output]["source"] in list(flag_codes.keys()):
             l4ao[output]["flag_code"] = flag_codes[l4ao[output]["source"]]
         # alternate data file name
         # first, look in the [Files] section for a generic file name
-        file_list = cf["Files"].keys()
+        file_list = list(cf["Files"].keys())
         lower_file_list = [item.lower() for item in file_list]
         if l4ao[output]["source"].lower() in lower_file_list:
             # found a generic file name
@@ -590,7 +590,7 @@
             logger.error(" Something went badly wrong and I'm giving up")
             sys.exit()
         # get a list of alternate series
-        alternate_series_list = [item for item in ds_alternate.series.keys() if "_QCFlag" not in item]
+        alternate_series_list = [item for item in list(ds_alternate.series.keys()) if "_QCFlag" not in item]
         # number of records in truncated or padded alternate data
         nRecs_tower = len(ldt_tower)
         # force the alternate dattime to be the tower date time
@@ -616,7 +616,7 @@
         nRecs = len(ldt_tower)
         ds_alternate.globalattributes["nc_nrecs"] = nRecs
         ds_alternate.series["DateTime"] = ds.series["DateTime"]
-        alternate_series_list = [item for item in ds_alternate.series.keys() if "_QCFlag" not in item]
+        alternate_series_list = [item for item in list(ds_alternate.series.keys()) if "_QCFlag" not in item]
         for series in alternate_series_list:
             if series in ["DateTime","DateTime_UTC"]:
                 continue
@@ -642,10 +642,10 @@
     # flag codes
     flag_codes = {"interpolated daily": 450, "monthly": 460}
     # create the climatology directory in the data structure
-    if called_by not in l4_info.keys():
+    if called_by not in list(l4_info.keys()):
         l4_info[called_by] = {"outputs": {}}
     # name of alternate output series in ds
-    outputs = cf["Drivers"][label][called_by].keys()
+    outputs = list(cf["Drivers"][label][called_by].keys())
     # loop over the outputs listed in the control file
     l4co = l4_info[called_by]["outputs"]
     cfcli = cf["Drivers"][label][called_by]
@@ -660,7 +660,7 @@
         # get the source
         l4co[output]["source"] = pfp_utils.get_keyvaluefromcf(cf, sl, "source", default="climatology")
         # Climatology file name
-        file_list = cf["Files"].keys()
+        file_list = list(cf["Files"].keys())
         lower_file_list = [item.lower() for item in file_list]
         # first, look in the [Files] section for a generic file name
         if l4co[output]["source"] in lower_file_list:
@@ -692,7 +692,7 @@
         # get the flag code
         l4co[output]["flag_code"] = flag_codes[l4co[output]["method"]]
         # create an empty series in ds if the climatology output series doesn't exist yet
-        if output not in ds.series.keys():
+        if output not in list(ds.series.keys()):
             data, flag, attr = pfp_utils.MakeEmptySeries(ds, output)
             attr[descr_level] = "CLI"
             pfp_utils.CreateSeries(ds, output, data, flag, attr)
@@ -742,7 +742,7 @@
     nperday = 24 * 60/ts
     l5_info[called_by]["info"]["MaxShortGapRecords"] = max_short_gap_days * nperday
     # name of MDS output series in ds
-    outputs = cf["Fluxes"][label]["GapFillUsingMDS"].keys()
+    outputs = list(cf["Fluxes"][label]["GapFillUsingMDS"].keys())
     # loop over the outputs listed in the control file
     l5mo = l5_info[called_by]["outputs"]
     for output in outputs:
@@ -835,7 +835,7 @@
     merge_order = "standard"
     if label in merge_prereq_list:
         merge_order = "prerequisite"
-    if merge_order not in info[called_by].keys():
+    if merge_order not in list(info[called_by].keys()):
         info[called_by][merge_order] = {}
     # create the dictionary keys for this series
     info[called_by][merge_order][label] = {}
@@ -846,7 +846,7 @@
     src_list = pfp_utils.GetMergeSeriesKeys(cf, label, section=section)
     info[called_by][merge_order][label]["source"] = src_list
     # create an empty series in ds if the output series doesn't exist yet
-    if label not in ds.series.keys():
+    if label not in list(ds.series.keys()):
         data, flag, attr = pfp_utils.MakeEmptySeries(ds, label)
         pfp_utils.CreateSeries(ds, label, data, flag, attr)
 
@@ -865,7 +865,7 @@
     descr_level = "description_" + ds.globalattributes["nc_level"]
     ds.series[target_label]["Attr"][descr_level] = ""
     # create the solo settings directory
-    if called_by not in l5_info.keys():
+    if called_by not in list(l5_info.keys()):
         # create the GapFillUsingSOLO dictionary
         l5_info[called_by] = {"outputs": {}, "info": {}, "gui": {}}
         # only need to create the ["info"] dictionary on the first pass
@@ -880,9 +880,9 @@
     if "SummaryPlots" in cf:
         l5_info[called_by]["SummaryPlots"] = cf["SummaryPlots"]
     # create an empty series in ds if the SOLO output series doesn't exist yet
-    outputs = cf["Fluxes"][target_label][called_by].keys()
+    outputs = list(cf["Fluxes"][target_label][called_by].keys())
     for output in outputs:
-        if output not in ds.series.keys():
+        if output not in list(ds.series.keys()):
             # disable output to netCDF file for this variable
             l5_info["RemoveIntermediateSeries"]["not_output"].append(output)
             # create an empty variable
@@ -1070,7 +1070,7 @@
         msg = "Unrecognised control file level (must be L5 or L6)"
         logger.error(msg)
         return
-    outputs = cf[section][target][called_by].keys()
+    outputs = list(cf[section][target][called_by].keys())
     for output in outputs:
         # disable output to netCDF file for this variable
         iris["not_output"].append(output)
@@ -1109,7 +1109,7 @@
     Gap fill missing data using data from the climatology spreadsheet produced by
     the climatology.py script.
     '''
-    if called_by not in l4_info.keys():
+    if called_by not in list(l4_info.keys()):
         return
     l4co = l4_info[called_by]["outputs"]
     # tell the user what we are going to do
@@ -1117,7 +1117,7 @@
     logger.info(msg)
     # loop over the series to be gap filled using climatology
     cli_xlbooks = {}
-    for output in l4co.keys():
+    for output in list(l4co.keys()):
         # check to see if there are any gaps in "series"
         #index = numpy.where(abs(ds.series[label]['Data']-float(c.missing_value))<c.eps)[0]
         #if len(index)==0: continue                      # no gaps found in "series"
@@ -1280,19 +1280,19 @@
         loc = mdt.MinuteLocator()
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(minutes=30):
-        loc = mdt.MinuteLocator(byminute=range(0,60,5))
+        loc = mdt.MinuteLocator(byminute=list(range(0,60,5)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(hours=1):
-        loc = mdt.MinuteLocator(byminute=range(0,60,15))
+        loc = mdt.MinuteLocator(byminute=list(range(0,60,15)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(hours=6):
         loc = mdt.HourLocator()
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(days=1):
-        loc = mdt.HourLocator(byhour=range(0,24,3))
+        loc = mdt.HourLocator(byhour=list(range(0,24,3)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(days=3):
-        loc = mdt.HourLocator(byhour=range(0,24,12))
+        loc = mdt.HourLocator(byhour=list(range(0,24,12)))
         fmt = mdt.DateFormatter('%d/%m %H')
     elif delta <= td(weeks=2):
         loc = mdt.DayLocator()
@@ -1313,7 +1313,7 @@
 
 def ImportSeries(cf,ds):
     # check to see if there is an Imports section
-    if "Imports" not in cf.keys():
+    if "Imports" not in list(cf.keys()):
         return
     # number of records
     nRecs = int(ds.globalattributes["nc_nrecs"])
@@ -1322,7 +1322,7 @@
     start_date = ldt[0]
     end_date = ldt[-1]
     # loop over the series in the Imports section
-    for label in cf["Imports"].keys():
+    for label in list(cf["Imports"].keys()):
         import_filename = pfp_utils.get_keyvaluefromcf(cf, ["Imports", label], "file_name", default="")
         if import_filename == "":
             msg = " ImportSeries: import filename not found in control file, skipping ..."
RefactoringTool: Refactored ./scripts/pfp_gfALT.py
--- ./scripts/pfp_gfALT.py	(original)
+++ ./scripts/pfp_gfALT.py	(refactored)
@@ -114,7 +114,7 @@
             dt_alternate = ds_alternate.series["DateTime"]["Data"]
             si_alternate = pfp_utils.GetDateIndex(dt_alternate, l4a["gui"]["startdate"], ts=ts, default=0)
             ei_alternate = pfp_utils.GetDateIndex(dt_alternate, l4a["gui"]["enddate"], ts=ts, default=nRecs-1)
-            alt_series_list = [item for item in ds_alternate.series.keys() if "_QCFlag" not in item]
+            alt_series_list = [item for item in list(ds_alternate.series.keys()) if "_QCFlag" not in item]
             alt_series_list = [item for item in alt_series_list if l4a["outputs"][label_output]["target"] in item]
             for label_alternate in alt_series_list:
                 data_alt, _, _ = pfp_utils.GetSeriesasMA(ds_alternate, label_alternate, si=si_alternate, ei=ei_alternate)
@@ -153,8 +153,8 @@
                 logger.info(msg)
             min_points = max([int(((gap[1]-gap[0])+1)*l4a["gui"]["min_percent"]/100), 3*l4a["gui"]["nperhr"]])
             num_good_points = 0
-            num_points_list = data_all.keys()
-            for label in data_all.keys():
+            num_points_list = list(data_all.keys())
+            for label in list(data_all.keys()):
                 if numpy.ma.count(data_all[label][gap[0]:gap[1]]) < min_points:
                     num_points_list.remove(label)
                     continue
@@ -317,7 +317,7 @@
     Author: PRI
     Date: August 2014
     """
-    alternate_var_list = [item for item in ds_alternate.series.keys() if label in item]
+    alternate_var_list = [item for item in list(ds_alternate.series.keys()) if label in item]
     # remove any extraneous Fn labels (alternate has Fn_lw and Fn_sw)
     if label=="Fn":
         alternate_var_list = [item for item in alternate_var_list if "lw" not in item]
@@ -357,10 +357,10 @@
     diel_avg = {}
     for label_output in output_list:
         diel_avg[label_output] = {}
-        if "data" in data_dict[label_output].keys():
+        if "data" in list(data_dict[label_output].keys()):
             data_2d = gfalternate_getdataas2d(odt, data_dict[label_output]["data"], l4a)
             diel_avg[label_output]["data"] = numpy.ma.average(data_2d, axis=0)
-        if "fitcorr" in data_dict[label_output].keys():
+        if "fitcorr" in list(data_dict[label_output].keys()):
             data_2d = gfalternate_getdataas2d(odt, data_dict[label_output]["fitcorr"], l4a)
             diel_avg[label_output]["fitcorr"] = numpy.ma.average(data_2d, axis=0)
     return diel_avg
@@ -385,9 +385,9 @@
 def gfalternate_getlabeloutputlist(l4_info, label_tower):
     l4a = l4_info["GapFillFromAlternate"]
     l4m = l4_info["MergeSeries"]
-    olist = [item for item in l4a["outputs"].keys() if l4a["outputs"][item]["target"] == label_tower]
-    for item in l4m.keys():
-        if label_tower in l4m[item].keys():
+    olist = [item for item in list(l4a["outputs"].keys()) if l4a["outputs"][item]["target"] == label_tower]
+    for item in list(l4m.keys()):
+        if label_tower in list(l4m[item].keys()):
             mlist = l4m[item][label_tower]["source"]
     label_output_list = []
     for item in mlist:
@@ -736,7 +736,7 @@
     nts = len(output_list) + 1
     pd["ts_bottom"] = pd["margin_bottom"] + pd["xy_height"] + pd["xyts_space"]
     pd["ts_height"] = (1.0 - pd["margin_top"] - pd["ts_bottom"])/nts
-    for key, value in kwargs.iteritems():
+    for key, value in kwargs.items():
         pd[key] = value
     return pd
 
@@ -1006,7 +1006,7 @@
     start_date = ldt[0].strftime("%Y-%m-%d")
     end_date = ldt[-1].strftime("%Y-%m-%d")
     # list of targets to plot
-    targets = [l4a["outputs"][output]["target"] for output in l4a["outputs"].keys()]
+    targets = [l4a["outputs"][output]["target"] for output in list(l4a["outputs"].keys())]
     targets = list(set(targets))
     ylabel_list = [""] + targets + [""]
     ylabel_right_list = [""]
@@ -1028,19 +1028,19 @@
     fig.canvas.set_window_title(title)
     plt.ylim([0, len(targets) + 1])
     plt.xlim([ldt[0], ldt[-1]])
-    for label, n in zip(targets, range(1, len(targets) + 1)):
+    for label, n in zip(targets, list(range(1, len(targets) + 1))):
         data_series, _, _ = pfp_utils.GetSeriesasMA(ds_tower, label)
         percent = 100*numpy.ma.count(data_series)/len(data_series)
         ylabel_right_list.append("{0:.0f}%".format(percent))
         ind_series = numpy.ma.ones(len(data_series))*float(n)
         ind_series = numpy.ma.masked_where(numpy.ma.getmaskarray(data_series) == True, ind_series)
         plt.plot(ldt, ind_series, color=colors[numpy.mod(n, 8)], linewidth=1)
-        if label+"_composite" in ds_tower.series.keys():
+        if label+"_composite" in list(ds_tower.series.keys()):
             data_composite, _, _ = pfp_utils.GetSeriesasMA(ds_tower, label+"_composite")
             ind_composite = numpy.ma.ones(len(data_composite))*float(n)
             ind_composite = numpy.ma.masked_where(numpy.ma.getmaskarray(data_composite) == True, ind_composite)
             plt.plot(ldt, ind_composite, color=colors[numpy.mod(n,8)], linewidth=4)
-    ylabel_posn = range(0, len(targets)+2)
+    ylabel_posn = list(range(0, len(targets)+2))
     pylab.yticks(ylabel_posn, ylabel_list)
     ylabel_right_list.append("")
     ax2 = ax1.twinx()
@@ -1126,7 +1126,7 @@
     """
     l4a = l4_info[called_by]
     # get a list of target variables
-    series_list = [l4a["outputs"][item]["target"] for item in l4a["outputs"].keys()]
+    series_list = [l4a["outputs"][item]["target"] for item in list(l4a["outputs"].keys())]
     l4a["gui"]["series_list"] = sorted(list(set(series_list)))
     logger.info(" Gap filling %s using alternate data", l4a["gui"]["series_list"])
     # initialise the l4_info["run"] dictionary
RefactoringTool: Refactored ./scripts/pfp_gfMDS.py
--- ./scripts/pfp_gfMDS.py	(original)
+++ ./scripts/pfp_gfMDS.py	(refactored)
@@ -161,7 +161,7 @@
     pd = {"margin_bottom":0.075, "margin_top":0.075, "margin_left":0.05, "margin_right":0.05,
           "xy_height":0.20, "xy_width":0.20, "xyts_space":0.05, "ts_width":0.9}
     # set the keyword arguments
-    for key, value in kwargs.iteritems():
+    for key, value in kwargs.items():
         pd[key] = value
     # calculate bottom of the first time series and the height of the time series plots
     pd["ts_bottom"] = pd["margin_bottom"]+pd["xy_height"]+pd["xyts_space"]
RefactoringTool: Refactored ./scripts/pfp_gfSOLO.py
--- ./scripts/pfp_gfSOLO.py	(original)
+++ ./scripts/pfp_gfSOLO.py	(refactored)
@@ -96,7 +96,7 @@
         return
     ldt = ds.series["DateTime"]["Data"]
     nRecs = len(ldt)
-    for output in l5s["outputs"].keys():
+    for output in list(l5s["outputs"].keys()):
         not_enough_points = False
         target = l5s["outputs"][output]["target"]
         data_solo, _, _ = pfp_utils.GetSeriesasMA(ds, output)
@@ -149,16 +149,16 @@
 
 def gfSOLO_getserieslist(cf):
     series_list = []
-    if "Drivers" in cf.keys():
-        for series in cf["Drivers"].keys():
+    if "Drivers" in list(cf.keys()):
+        for series in list(cf["Drivers"].keys()):
             if "GapFillUsingSOLO" in cf["Drivers"][series]:
                 series_list.append(series)
-    elif "Fluxes" in cf.keys():
-        for series in cf["Fluxes"].keys():
+    elif "Fluxes" in list(cf.keys()):
+        for series in list(cf["Fluxes"].keys()):
             if "GapFillUsingSOLO" in cf["Fluxes"][series]:
                 series_list.append(series)
-    elif "Variables" in cf.keys():
-        for series in cf["Variables"].keys():
+    elif "Variables" in list(cf.keys()):
+        for series in list(cf["Variables"].keys()):
             if "GapFillUsingSOLO" in cf["Variables"][series]:
                 series_list.append(series)
     else:
@@ -194,7 +194,7 @@
     l5s["gui"]["min_points"] = int((ei-si)*l5s["gui"]["min_percent"]/100)
     # loop over the series to be gap filled using solo
     if outputs == None:
-        outputs = l5s["outputs"].keys()
+        outputs = list(l5s["outputs"].keys())
     for output in outputs:
         # get the QC flag code
         flag_code = l5s["outputs"][output]["flag_code"]
@@ -212,7 +212,7 @@
             msg = "SOLO: Less than " + str(l5s["gui"]["min_points"]) + " points available for target " + target
             logger.warning(msg)
             l5s["outputs"][output]["results"]["No. points"].append(float(0))
-            results = l5s["outputs"][output]["results"].keys()
+            results = list(l5s["outputs"][output]["results"].keys())
             for item in ["startdate", "enddate", "No. points"]:
                 if item in results: results.remove(item)
             for item in results:
@@ -371,7 +371,7 @@
     ts_axes[0].text(0.05, 0.85, TextStr, color='b', horizontalalignment='left', transform=ts_axes[0].transAxes)
     TextStr = output + '(' + ds.series[output]['Attr']['units'] + ')'
     ts_axes[0].text(0.85, 0.85, TextStr, color='r', horizontalalignment='right', transform=ts_axes[0].transAxes)
-    for label, i in zip(drivers, range(1, len(drivers) + 1)):
+    for label, i in zip(drivers, list(range(1, len(drivers) + 1))):
         this_bottom = pd["ts_bottom"] + i*pd["ts_height"]
         rect = [pd["margin_left"], this_bottom, pd["ts_width"], pd["ts_height"]]
         ts_axes.append(plt.axes(rect, sharex=ts_axes[0]))
@@ -426,9 +426,9 @@
     start_date = ldt[0].strftime("%Y-%m-%d")
     end_date = ldt[-1].strftime("%Y-%m-%d")
     # list of outputs to plot
-    outputs = l5s["outputs"].keys()
+    outputs = list(l5s["outputs"].keys())
     # list of targets
-    targets = [l5s["outputs"][output]["target"] for output in l5s["outputs"].keys()]
+    targets = [l5s["outputs"][output]["target"] for output in list(l5s["outputs"].keys())]
     ylabel_list = [""] + targets + [""]
     ylabel_right_list = [""]
     colors = ["blue", "red", "green", "yellow", "magenta", "black", "cyan", "brown"]
@@ -449,7 +449,7 @@
     fig.canvas.set_window_title(title)
     plt.ylim([0, len(outputs) + 1])
     plt.xlim([ldt[0], ldt[-1]])
-    for olabel, tlabel, n in zip(outputs, targets, range(1, len(outputs)+1)):
+    for olabel, tlabel, n in zip(outputs, targets, list(range(1, len(outputs)+1))):
         output = pfp_utils.GetVariable(ds, olabel)
         target = pfp_utils.GetVariable(ds, tlabel)
         percent = 100*numpy.ma.count(target["Data"])/len(target["Data"])
@@ -460,7 +460,7 @@
         ind_output = numpy.ma.masked_where(numpy.ma.getmaskarray(output["Data"]) == True, ind_output)
         plt.plot(ldt, ind_target, color=colors[numpy.mod(n, 8)], linewidth=1)
         plt.plot(ldt, ind_output, color=colors[numpy.mod(n, 8)], linewidth=4)
-    ylabel_posn = range(0, len(outputs)+2)
+    ylabel_posn = list(range(0, len(outputs)+2))
     pylab.yticks(ylabel_posn, ylabel_list)
     ylabel_right_list.append("")
     ax2 = ax1.twinx()
@@ -478,7 +478,7 @@
     # find out who's calling
     called_by = solo["info"]["called_by"]
     # get a list of variables for which SOLO data is available
-    outputs = solo["outputs"].keys()
+    outputs = list(solo["outputs"].keys())
     # site name for titles
     site_name = ds.globalattributes["site_name"]
     # get the start and end dates of the SOLO windows
@@ -522,7 +522,7 @@
     # now loop over the variables in the group list
     for col, label in enumerate(outputs):
         # and loop over rows in plot
-        for row, rlabel, ylabel in zip(range(len(result_list)), result_list, ylabel_list):
+        for row, rlabel, ylabel in zip(list(range(len(result_list))), result_list, ylabel_list):
             # get the results to be plotted
             #result = numpy.ma.masked_equal(ds.solo[label]["results"][rlabel],float(c.missing_value))
             # put the data into the right order to be plotted
@@ -660,7 +660,7 @@
     """
     l5s = l5_info[called_by]
     # get a list of target variables
-    targets = [l5s["outputs"][output]["target"] for output in l5s["outputs"].keys()]
+    targets = [l5s["outputs"][output]["target"] for output in list(l5s["outputs"].keys())]
     l5s["gui"]["targets"] = sorted(list(set(targets)))
     msg = " Gap filling "+str(l5s["gui"]["targets"])+" using SOLO"
     logger.info(msg)
RefactoringTool: Refactored ./scripts/pfp_gui.py
--- ./scripts/pfp_gui.py	(original)
+++ ./scripts/pfp_gui.py	(refactored)
@@ -518,7 +518,7 @@
 
     def add_excludehours(self):
         """ Add an exclude hours check to a variable."""
-        print " add ExcludeHours here"
+        print(" add ExcludeHours here")
 
     def add_file_path(self):
         """ Add file_path to the 'Files' section."""
RefactoringTool: Refactored ./scripts/pfp_io.py
--- ./scripts/pfp_io.py	(original)
+++ ./scripts/pfp_io.py	(refactored)
@@ -89,9 +89,9 @@
             si = pfp_utils.GetDateIndex(dt_out,sd_file,ts=ts,default=0,match='exact')
             ei = pfp_utils.GetDateIndex(dt_out,ed_file,ts=ts,default=-1,match='exact')
             # now replace parts of ds_out with the data read from file
-            for ThisOne in ds_file.series.keys():
+            for ThisOne in list(ds_file.series.keys()):
                 # check to see if the L4 series exists in the L3 data
-                if ThisOne in ds_out.series.keys():
+                if ThisOne in list(ds_out.series.keys()):
                     # ds_out is the copy of the L3 data, now fill it with the L4 data read from file
                     ds_out.series[ThisOne]['Data'][si:ei+1] = ds_file.series[ThisOne]['Data']
                     ds_out.series[ThisOne]['Flag'][si:ei+1] = ds_file.series[ThisOne]['Flag']
@@ -176,8 +176,8 @@
     # to make sure the requested variables are in the csv file,
     # dump them if they aren't
     csv_varnames = {}
-    for item in cf["Variables"].keys():
-        if "csv" in cf["Variables"][item].keys():
+    for item in list(cf["Variables"].keys()):
+        if "csv" in list(cf["Variables"][item].keys()):
             opt = pfp_utils.get_keyvaluefromcf(cf, ["Variables", item, "csv"], "name", default="")
             if opt in info["header_line"]:
                 csv_varnames[item] = str(opt)
@@ -185,7 +185,7 @@
                 msg = "  "+str(opt)+" not found in CSV file, skipping ..."
                 logger.error(msg)
                 continue
-        elif "xl" in cf["Variables"][item].keys():
+        elif "xl" in list(cf["Variables"][item].keys()):
             opt = pfp_utils.get_keyvaluefromcf(cf, ["Variables", item, "xl"], "name", default="")
             if opt in info["header_line"]:
                 csv_varnames[item] = str(opt)
@@ -193,12 +193,12 @@
                 msg = "  "+str(opt)+" not found in CSV file, skipping ..."
                 logger.error(msg)
                 continue
-        elif "Function" not in cf["Variables"][item].keys():
+        elif "Function" not in list(cf["Variables"][item].keys()):
             msg = " No csv or Function section in control file for "+item
             logger.info(msg)
             continue
     info["csv_varnames"] = csv_varnames
-    info["var_list"] = csv_varnames.keys()
+    info["var_list"] = list(csv_varnames.keys())
     info["csv_list"] = [csv_varnames[x] for x in info["var_list"]]
     info["col_list"] = [info["header_list"].index(item) for item in info["csv_list"]]
 
@@ -226,7 +226,7 @@
     # get a data structure
     ds = DataStructure()
     # add the global atributes
-    for gattr in cf['Global'].keys():
+    for gattr in list(cf['Global'].keys()):
         ds.globalattributes[gattr] = cf['Global'][gattr]
     # parse the control file
     info = csv_read_parse_cf(cf)
@@ -270,7 +270,7 @@
         variable["Flag"] = flag
         # make the attribute dictionary ...
         variable["Attr"] = {}
-        for attr in cf["Variables"][label]["Attr"].keys():
+        for attr in list(cf["Variables"][label]["Attr"].keys()):
             variable["Attr"][attr] = cf["Variables"][label]["Attr"][attr]
         pfp_utils.CreateVariable(ds, variable)
     ## call the function given in the control file to convert the date/time string to a datetime object
@@ -293,7 +293,7 @@
     # read the netCDF file
     ds = nc_read_series(ncfilename,checktimestep=False)
     nRecs = int(ds.globalattributes["nc_nrecs"])
-    nCols = len(ds.series.keys())
+    nCols = len(list(ds.series.keys()))
     if outputlist!=None: nCols = len(outputlist)
     # xlwt seems to only handle 225 columns
     if nRecs<65535 and nCols<220:
@@ -411,13 +411,13 @@
     Hhh, _, _ = pfp_utils.GetSeries(ds,'Hdh', si=si, ei=ei)
     # get the data
     data = OrderedDict()
-    for label in cf["Variables"].keys():
+    for label in list(cf["Variables"].keys()):
         data[label] = {"ncname": cf["Variables"][label]["ncname"],
                        "format": cf["Variables"][label]["format"]}
-    series_list = data.keys()
+    series_list = list(data.keys())
     for series in series_list:
         ncname = data[series]["ncname"]
-        if ncname not in ds.series.keys():
+        if ncname not in list(ds.series.keys()):
             msg = "Series " + ncname + " not in netCDF file, skipping ..."
             logger.error(msg)
             series_list.remove(series)
@@ -574,7 +574,7 @@
 
 def smap_write_csv(cf):
     cfvars = cf["Variables"]
-    smap_list = cfvars.keys()
+    smap_list = list(cfvars.keys())
     ncFileName = get_infilenamefromcf(cf)
     csvFileName_base = get_outfilenamefromcf(cf)
     # read the netCDF file
@@ -585,7 +585,7 @@
     nperday = int(float(24)*nperhr+0.5)
     dt = ds.series["DateTime"]["Data"]
     # get a list of years in the data file
-    year_list = range(dt[0].year,dt[-1].year+1)
+    year_list = list(range(dt[0].year,dt[-1].year+1))
     years = numpy.array([item.year for item in dt])
     # loop over years in the data file
     data_dict = OrderedDict()
@@ -633,7 +633,7 @@
         for i in range(ndays):
             data_list = []
             data_list.append(smap_id[i])
-            for smap_label in data_dict.keys():
+            for smap_label in list(data_dict.keys()):
                 if smap_label=="DateTime": continue
                 strfmt = smap_parseformat(data_dict[smap_label]["fmt"])
                 if "d" in strfmt:
@@ -646,7 +646,7 @@
 def smap_writeheaders(cf,csvfile):
     writer = csv.writer(csvfile)
     # write the header lines to the csv file
-    series_list = cf["Variables"].keys()
+    series_list = list(cf["Variables"].keys())
     for item in cf["General"]:
         if item in ["SMAP_ID"]: continue
         writer.writerow([item,str(cf['General'][item])])
@@ -704,7 +704,7 @@
     dt = ds.series["DateTime"]["Data"]
     # get the data
     data = {}
-    series_list = cf["Variables"].keys()
+    series_list = list(cf["Variables"].keys())
     for series in series_list:
         ncname = cf["Variables"][series]["in_name"]
         data[series] = pfp_utils.GetVariable(ds, ncname)
@@ -808,12 +808,12 @@
     data = ep_biomet_get_data(cf, ds)
     # check and adjust units if required
     # get a list of the EddyPro series to be output
-    ep_series_list = data.keys()
+    ep_series_list = list(data.keys())
     ep_series_list.sort()
     for ep_series in ep_series_list:
         # loop over the netCDF series names and check they exist in constants.units_synonyms dictionary
         in_name = data[ep_series]["in_name"]
-        if in_name not in c.units_synonyms.keys():
+        if in_name not in list(c.units_synonyms.keys()):
             msg = "No entry for " + in_name + " in cfg.units_synonyms, skipping ..."
             logger.warning(msg)
             continue
@@ -849,10 +849,10 @@
 
 def ep_biomet_get_data(cfg, ds):
     data = {}
-    ep_series_list = cfg["Variables"].keys()
+    ep_series_list = list(cfg["Variables"].keys())
     for ep_series in ep_series_list:
         in_name = cfg["Variables"][ep_series]["in_name"]
-        if in_name not in ds.series.keys():
+        if in_name not in list(ds.series.keys()):
             logger.error("Series " + in_name + " not in netCDF file, skipping ...")
             ep_series_list.remove(ep_series)
             continue
@@ -941,7 +941,7 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     # Tumbarumba doesn't have RH in the netCDF files
-    if "RH" not in ds.series.keys():
+    if "RH" not in list(ds.series.keys()):
         Ah,f,a = pfp_utils.GetSeriesasMA(ds,'Ah')
         Ta,f,a = pfp_utils.GetSeriesasMA(ds,'Ta')
         RH = pfp_mf.RHfromabsolutehumidity(Ah, Ta)
@@ -958,7 +958,7 @@
         # requested start_datetime is after the start of the file
         logger.info(" Truncating start of file")
         si = pfp_utils.GetDateIndex(dt,str(start_datetime),ts=ts,match="exact")
-        for thisone in ds.series.keys():
+        for thisone in list(ds.series.keys()):
             ds.series[thisone]["Data"] = ds.series[thisone]["Data"][si:]
             ds.series[thisone]["Flag"] = ds.series[thisone]["Flag"][si:]
         ds.globalattributes["nc_nrecs"] = str(len(ds.series["DateTime"]["Data"]))
@@ -969,7 +969,7 @@
         data_patched = numpy.ones(len(dt_patched))*float(c.missing_value)
         flag_patched = numpy.ones(len(dt_patched))
         # list of series in the data structure
-        series_list = ds.series.keys()
+        series_list = list(ds.series.keys())
         # ds.series["DateTime"]["Data"] is a list not a numpy array so we must treat it differently
         ds.series["DateTime"]["Data"] = dt_patched+ds.series["DateTime"]["Data"]
         ds.series["DateTime"]["Flag"] = numpy.concatenate((flag_patched,ds.series["DateTime"]["Flag"]))
@@ -987,7 +987,7 @@
         msg = " Truncating end of file "+dt[-1].strftime("%Y-%m-%d %H:%M")+" "+end_datetime.strftime("%Y-%m-%d %H:%M")
         logger.info(msg)
         ei = pfp_utils.GetDateIndex(dt,str(end_datetime),ts=ts,match="exact")
-        for thisone in ds.series.keys():
+        for thisone in list(ds.series.keys()):
             ds.series[thisone]["Data"] = ds.series[thisone]["Data"][:ei+1]
             ds.series[thisone]["Flag"] = ds.series[thisone]["Flag"][:ei+1]
         ds.globalattributes["nc_nrecs"] = str(len(ds.series["DateTime"]["Data"]))
@@ -999,7 +999,7 @@
         data_patched = numpy.ones(len(dt_patched))*float(c.missing_value)
         flag_patched = numpy.ones(len(dt_patched))
         # list of series in the data structure
-        series_list = ds.series.keys()
+        series_list = list(ds.series.keys())
         # ds.series["DateTime"]["Data"] is a list not a numpy array so we must treat it differently
         ds.series["DateTime"]["Data"] = ds.series["DateTime"]["Data"]+dt_patched
         ds.series["DateTime"]["Flag"] = numpy.concatenate((ds.series["DateTime"]["Flag"],flag_patched))
@@ -1033,10 +1033,10 @@
     Minute,flag,attr = pfp_utils.GetSeries(ds,'Minute')
     # get the data
     data = {}
-    series_list = cf["Variables"].keys()
+    series_list = list(cf["Variables"].keys())
     for series in series_list:
         ncname = cf["Variables"][series]["ncname"]
-        if ncname not in ds.series.keys():
+        if ncname not in list(ds.series.keys()):
             logger.error("Series "+ncname+" not in netCDF file, skipping ...")
             series_list.remove(series)
             continue
@@ -1109,7 +1109,7 @@
     else:
         cf = ConfigObj()
     if "Files" in cf:
-        if "plot_path" not in cf["Files"].keys():
+        if "plot_path" not in list(cf["Files"].keys()):
             cf["Files"]["plot_path"] = "plots/"
     return cf
 
@@ -1231,7 +1231,7 @@
         dt = pfp_utils.GetVariable(data[file_name], "DateTime")
         inc["start_date"].append(dt["Data"][0])
         inc["end_date"].append(dt["Data"][-1])
-        inc["labels"] = inc["labels"] + data[file_name].series.keys()
+        inc["labels"] = inc["labels"] + list(data[file_name].series.keys())
     # get a list of files with start times in chronological order
     inc["chrono_files"] = [f for d, f in sorted(zip(inc["start_date"], inc["in_file_names"]))]
     # get a list of unique variable names and remove unwanted labels
@@ -1325,7 +1325,7 @@
         # loop over the variables
         for label in inc["labels"]:
             dout = ds_out.series[label]
-            if label in data[file_name].series.keys():
+            if label in list(data[file_name].series.keys()):
                 din = data[file_name].series[label]
                 # copy input data to output variable
                 # NOTE: using direct read from and write to the data structures here,
@@ -1462,13 +1462,13 @@
     # create the output data structure
     ds_out = DataStructure()
     # copy the global attributes
-    for item in ds_in.globalattributes.keys():
+    for item in list(ds_in.globalattributes.keys()):
         ds_out.globalattributes[item] = ds_in.globalattributes[item]
     # get the indices of the start and end datetimes
     si = pfp_utils.GetDateIndex(ldt_in,startdate,ts=ts,default=0,match="exact")
     ei = pfp_utils.GetDateIndex(ldt_in,enddate,ts=ts,default=len(ldt_in),match="exact")
     # get a list of the series in ds_in
-    series_list = [item for item in ds_in.series.keys() if "_QCFlag" not in item]
+    series_list = [item for item in list(ds_in.series.keys()) if "_QCFlag" not in item]
     # remove the Python datetime series
     for item in ["DateTime","DateTime_UTC"]:
         if item in series_list: series_list.remove(item)
@@ -1550,13 +1550,13 @@
         for gattr in gattrlist:
             ds.globalattributes[gattr] = getattr(ncFile, gattr)
     # get a list of the variables in the netCDF file (not their QC flags)
-    varlist = [x for x in ncFile.variables.keys() if "_QCFlag" not in x]
+    varlist = [x for x in list(ncFile.variables.keys()) if "_QCFlag" not in x]
     for ThisOne in varlist:
         # skip variables that do not have time as a dimension
         dimlist = [x.lower() for x in ncFile.variables[ThisOne].dimensions]
         if "time" not in dimlist: continue
         # create the series in the data structure
-        ds.series[unicode(ThisOne)] = {}
+        ds.series[str(ThisOne)] = {}
         # get the data and the QC flag
         data, flag, attr = nc_read_var(ncFile, ThisOne)
         ds.series[ThisOne]["Data"] = data
@@ -1564,7 +1564,7 @@
         ds.series[ThisOne]["Attr"] = attr
     ncFile.close()
     # get a series of Python datetime objects
-    if "time" in ds.series.keys():
+    if "time" in list(ds.series.keys()):
         time,f,a = pfp_utils.GetSeries(ds,"time")
         pfp_utils.get_datetimefromnctime(ds,time,a["units"])
     else:
@@ -1601,7 +1601,7 @@
         # for the variable, here we trap this and force the array in ds.series to be ndarray
         if numpy.ma.isMA(data): data,dummy = pfp_utils.MAtoSeries(data)
         # check for a QC flag
-        if ThisOne+'_QCFlag' in ncFile.variables.keys():
+        if ThisOne+'_QCFlag' in list(ncFile.variables.keys()):
             # load it from the netCDF file
             flag = ncFile.variables[ThisOne+'_QCFlag'][:]
         else:
@@ -1622,7 +1622,7 @@
         # may not be needed after adding ncFile.set_auto_mask(False) in nc_read_series().
         if numpy.ma.isMA(data): data,dummy = pfp_utils.MAtoSeries(data)
         # check for a QC flag
-        if ThisOne+'_QCFlag' in ncFile.variables.keys():
+        if ThisOne+'_QCFlag' in list(ncFile.variables.keys()):
             # load it from the netCDF file
             flag = ncFile.variables[ThisOne+'_QCFlag'][:,0,0]
         else:
@@ -1709,7 +1709,7 @@
     setattr(nc_var, 'standard_name', 'longitude')
     setattr(nc_var, 'units', 'degrees east')
     # get a list of variables to write to the netCDF file
-    labels = sorted([label for label in data_dict["variables"].keys() if label != "DateTime"])
+    labels = sorted([label for label in list(data_dict["variables"].keys()) if label != "DateTime"])
     # write the variables to the netCDF file object
     for label in labels:
         nc_var = nc_obj.createVariable(label, "d", dims)
@@ -1732,7 +1732,7 @@
     ds.globalattributes["end_date"] = str(ldt[-1])
     t = time.localtime()
     ds.globalattributes["nc_rundatetime"] = str(datetime.datetime(t[0],t[1],t[2],t[3],t[4],t[5]))
-    gattr_list = ds.globalattributes.keys()
+    gattr_list = list(ds.globalattributes.keys())
     gattr_list.sort()
     flag_list = []
     attr_list = []
@@ -1744,7 +1744,7 @@
     for item in attr_list:
         if isinstance(ds.globalattributes[item],str):
             attr = ds.globalattributes[item]
-        elif isinstance(ds.globalattributes[item],unicode):
+        elif isinstance(ds.globalattributes[item],str):
             attr = ds.globalattributes[item].encode('ascii','ignore')
         else:
             attr = str(ds.globalattributes[item])
@@ -1753,7 +1753,7 @@
         for item in flag_list:
             if isinstance(ds.globalattributes[item],str):
                 attr = ds.globalattributes[item]
-            elif isinstance(ds.globalattributes[item],unicode):
+            elif isinstance(ds.globalattributes[item],str):
                 attr = ds.globalattributes[item].encode('ascii','ignore')
             else:
                 attr = str(ds.globalattributes[item])
@@ -1776,7 +1776,7 @@
     nc_write_globalattributes(ncFile, ds)
     # we specify the size of the Time dimension because netCDF4 is slow to write files
     # when the Time dimension is unlimited
-    if "nc_nrecs" in ds.globalattributes.keys():
+    if "nc_nrecs" in list(ds.globalattributes.keys()):
         nRecs = int(ds.globalattributes['nc_nrecs'])
     else:
         nRecs = len(ds.series["DateTime"]["Data"])
@@ -1788,13 +1788,13 @@
     else:
         dims = ("time",)
     if outputlist is None:
-        outputlist = ds.series.keys()
+        outputlist = list(ds.series.keys())
     else:
         for ThisOne in outputlist:
-            if ThisOne not in ds.series.keys():
+            if ThisOne not in list(ds.series.keys()):
                 logger.warning(" Requested series "+ThisOne+" not found in data structure")
                 outputlist.remove(ThisOne)
-        if len(outputlist)==0: outputlist = ds.series.keys()
+        if len(outputlist)==0: outputlist = list(ds.series.keys())
     # can't write an array of Python datetime objects to a netCDF file
     # actually, this could be written as characters
     for ThisOne in ["DateTime","DateTime_UTC"]:
@@ -1919,7 +1919,7 @@
     # Instance the data structure object.
     ds = DataStructure()
     # add the global atributes
-    for gattr in cf['Global'].keys():
+    for gattr in list(cf['Global'].keys()):
         ds.globalattributes[gattr] = cf['Global'][gattr]
     # get the filename
     FileName = get_infilenamefromcf(cf)
@@ -1933,7 +1933,7 @@
         logger.error(msg)
         ds.returncodes = {"value":1,"message":msg}
         return ds
-    label_list = cf['Variables'].keys()
+    label_list = list(cf['Variables'].keys())
     if "xlDateTime" not in label_list and "DateTime" not in label_list:
         msg = " No xlDateTime or DateTime section found in control file"
         logger.error(msg)
@@ -1969,7 +1969,7 @@
                 if cf["Variables"][label]["xl"]["name"].lower() in header_list:
                     logger.info(" Getting "+label+" from sheet "+xlsheet_name)
                     last_data_row = int(active_sheet.nrows)
-                    ds.series[unicode(label)] = {}
+                    ds.series[str(label)] = {}
                     xl_col = header_list.index(cf["Variables"][label]["xl"]["name"].lower())
                     values = active_sheet.col_values(xl_col)[first_data_row:last_data_row]
                     types = active_sheet.col_types(xl_col)[first_data_row:last_data_row]
@@ -1981,9 +1981,9 @@
                             ds.series[label]["Data"][i] = numpy.float64(values[i])
                             ds.series[label]["Flag"][i] = numpy.int32(0)
                     ds.series[label]["Attr"] = {"nrecs":nrecs}
-                    for attr in cf["Variables"][label]["Attr"].keys():
+                    for attr in list(cf["Variables"][label]["Attr"].keys()):
                         ds.series[label]["Attr"][attr] = cf["Variables"][label]["Attr"][attr]
-                    if "missing_value" not in ds.series[label]["Attr"].keys():
+                    if "missing_value" not in list(ds.series[label]["Attr"].keys()):
                         ds.series[label]["Attr"]["missing_value"] = numpy.int32(c.missing_value)
                     info[label] = {"sheet":active_sheet, "nrecs":nrecs}
                 else:
@@ -1994,7 +1994,7 @@
                     logger.error("  Sheet "+xlsheet_name+" ("+label+") not found in Excel workbook")
     # check that all variables have the same length
     all_good = True
-    label_list = info.keys()
+    label_list = list(info.keys())
     nrecs0 = info[label_list[0]]["nrecs"]
     for label in label_list[1:]:
         if info[label]["nrecs"] != nrecs0:
@@ -2020,9 +2020,9 @@
     """
     result = False
     cf_label = cf["Variables"][label]
-    if "xl" in cf_label.keys():
-        if "xl" in cf_label.keys():
-            if "sheet" in cf_label["xl"].keys():
+    if "xl" in list(cf_label.keys()):
+        if "xl" in list(cf_label.keys()):
+            if "sheet" in list(cf_label["xl"].keys()):
                 result = True
             else:
                 logger.error("  Key 'sheet' not found in control file entry for "+label)
@@ -2042,7 +2042,7 @@
     label_list = sorted(l4a["outputs"].keys())
     for label in label_list:
         # get the list of values to output with the start and end dates removed
-        output_list = l4a["outputs"][label]["results"].keys()
+        output_list = list(l4a["outputs"][label]["results"].keys())
         for item in date_list:
             if item in output_list: output_list.remove(item)
         # add a sheet with the series label
@@ -2069,7 +2069,7 @@
     xlfile.save(l4a["info"]["xl_file_name"])
 
 def xl_write_SOLOStats(ds, l5_info):
-    if "solo" not in l5_info.keys():
+    if "solo" not in list(l5_info.keys()):
         return
     # get the output file name
     out_filename = get_outfilenamefromcf(l5_info["cf"])
@@ -2083,11 +2083,11 @@
     date_list = ["startdate", "enddate"]
     # loop over the series that have been gap filled using ACCESS data
     d_xf = xlwt.easyxf(num_format_str='dd/mm/yyyy hh:mm')
-    outputs = l5_info["solo"]["outputs"].keys()
+    outputs = list(l5_info["solo"]["outputs"].keys())
     outputs.sort()
     for output in outputs:
         # get the list of values to output with the start and end dates removed
-        stats = l5_info["solo"]["outputs"][output]["results"].keys()
+        stats = list(l5_info["solo"]["outputs"][output]["results"].keys())
         for item in date_list:
             if item in outputs:
                 outputs.remove(item)
@@ -2141,9 +2141,9 @@
     # get a workbook
     xl_book = xlwt.Workbook()
     # get a list of the sheets to add
-    site_list = data.keys()
-    year_list = data[site_list[0]].keys()
-    stat_list = data[site_list[0]][year_list[0]].keys()
+    site_list = list(data.keys())
+    year_list = list(data[site_list[0]].keys())
+    stat_list = list(data[site_list[0]][year_list[0]].keys())
     # loop over the statistics
     for stat in stat_list:
         # add a worksheet for the statistics
@@ -2190,7 +2190,7 @@
     """
     #xlCol = 0
     # write the data to the xl file
-    series_list = data.keys()
+    series_list = list(data.keys())
     xl_sheet.write(1,xlCol,data["DateTime"]["attr"]["units"])
     nrows = len(data["DateTime"]["data"])
     ncols = len(series_list)
@@ -2212,10 +2212,10 @@
             xl_sheet.write(j+2,xlCol,tmp[j],d_xf)
 
 def xl_write_series(ds, xlfullname, outputlist=None):
-    if "nc_nrecs" in ds.globalattributes.keys():
+    if "nc_nrecs" in list(ds.globalattributes.keys()):
         nRecs = int(ds.globalattributes["nc_nrecs"])
     else:
-        variablelist = ds.series.keys()
+        variablelist = list(ds.series.keys())
         nRecs = len(ds.series[variablelist[0]]["Data"])
     # open the Excel file
     msg = " Opening and writing Excel file " + os.path.basename(xlfullname)
@@ -2239,7 +2239,7 @@
     xlrow = 0
     xlAttrSheet.write(xlrow, xlcol, "Global attributes")
     xlrow = xlrow + 1
-    globalattrlist = ds.globalattributes.keys()
+    globalattrlist = list(ds.globalattributes.keys())
     globalattrlist.sort()
     for ThisOne in sorted([x for x in globalattrlist if "Flag" not in x]):
         xlAttrSheet.write(xlrow, xlcol, ThisOne)
@@ -2257,7 +2257,7 @@
     xlcol_varname = 0
     xlcol_attrname = 1
     xlcol_attrvalue = 2
-    variablelist = ds.series.keys()
+    variablelist = list(ds.series.keys())
     if outputlist is None:
         outputlist = variablelist
     else:
@@ -2272,7 +2272,7 @@
         if ThisOne in outputlist: outputlist.remove(ThisOne)
     for ThisOne in outputlist:
         xlAttrSheet.write(xlrow, xlcol_varname, ThisOne)
-        attributelist = ds.series[ThisOne]["Attr"].keys()
+        attributelist = list(ds.series[ThisOne]["Attr"].keys())
         attributelist.sort()
         for Attr in attributelist:
             xlAttrSheet.write(xlrow, xlcol_attrname, Attr)
@@ -2298,7 +2298,7 @@
         msg = " Writing " + ThisOne + " into column " + str(xlcol) + " of the Excel file"
         logger.info(msg)
         # write the units and the variable name to the header rows in the xl file
-        attrlist = ds.series[ThisOne]["Attr"].keys()
+        attrlist = list(ds.series[ThisOne]["Attr"].keys())
         if "long_name" in attrlist:
             longname = ds.series[ThisOne]["Attr"]["long_name"]
         elif "Description" in attrlist:
@@ -2318,7 +2318,7 @@
         for j in range(nRecs):
             xlDataSheet.write(j+3, xlcol, float(ds.series[ThisOne]["Data"][j]))
         # check to see if this variable has a quality control flag
-        if "Flag" in ds.series[ThisOne].keys():
+        if "Flag" in list(ds.series[ThisOne].keys()):
             # write the QC flag name to the xls file
             xlFlagSheet.write(2, xlcol, ThisOne)
             # specify the format of the QC flag (integer)
@@ -2331,10 +2331,10 @@
     xlfile.save(xlfullname)
 
 def xlsx_write_series(ds, xlsxfullname, outputlist=None):
-    if "nc_nrecs" in ds.globalattributes.keys():
+    if "nc_nrecs" in list(ds.globalattributes.keys()):
         nRecs = int(ds.globalattributes["nc_nrecs"])
     else:
-        variablelist = ds.series.keys()
+        variablelist = list(ds.series.keys())
         nRecs = len(ds.series[variablelist[0]]["Data"])
     # open the Excel file
     msg = " Opening and writing Excel file " + os.path.basename(xlsxfullname)
@@ -2358,7 +2358,7 @@
     xlrow = 0
     xlAttrSheet.write(xlrow, xlcol, "Global attributes")
     xlrow = xlrow + 1
-    globalattrlist = ds.globalattributes.keys()
+    globalattrlist = list(ds.globalattributes.keys())
     globalattrlist.sort()
     for ThisOne in sorted([x for x in globalattrlist if "Flag" not in x]):
         xlAttrSheet.write(xlrow, xlcol, ThisOne)
@@ -2376,7 +2376,7 @@
     xlcol_varname = 0
     xlcol_attrname = 1
     xlcol_attrvalue = 2
-    variablelist = ds.series.keys()
+    variablelist = list(ds.series.keys())
     if outputlist is None:
         outputlist = variablelist
     else:
@@ -2391,7 +2391,7 @@
         if ThisOne in outputlist: outputlist.remove(ThisOne)
     for ThisOne in outputlist:
         xlAttrSheet.write(xlrow, xlcol_varname, ThisOne)
-        attributelist = ds.series[ThisOne]["Attr"].keys()
+        attributelist = list(ds.series[ThisOne]["Attr"].keys())
         attributelist.sort()
         for Attr in attributelist:
             xlAttrSheet.write(xlrow, xlcol_attrname, Attr)
@@ -2415,7 +2415,7 @@
         # put up a progress message
         logger.info(" Writing " + ThisOne + " into column " + str(xlcol) + " of the Excel file")
         # write the units and the variable name to the header rows in the xl file
-        attrlist = ds.series[ThisOne]["Attr"].keys()
+        attrlist = list(ds.series[ThisOne]["Attr"].keys())
         if "long_name" in attrlist:
             longname = ds.series[ThisOne]["Attr"]["long_name"]
         elif "Description" in attrlist:
@@ -2435,7 +2435,7 @@
         for j in range(nRecs):
             xlDataSheet.write(j+3, xlcol, float(ds.series[ThisOne]["Data"][j]))
         # check to see if this variable has a quality control flag
-        if "Flag" in ds.series[ThisOne].keys():
+        if "Flag" in list(ds.series[ThisOne].keys()):
             # write the QC flag name to the Excel file
             xlFlagSheet.write(2, xlcol, ThisOne)
             # specify the format of the QC flag (integer)
RefactoringTool: Refactored ./scripts/pfp_levels.py
--- ./scripts/pfp_levels.py	(original)
+++ ./scripts/pfp_levels.py	(refactored)
@@ -174,7 +174,7 @@
     pfp_utils.CheckUnits(ds3, Fc_list, "umol/m2/s", convert_units=True)
     # merge Fc and Fc_storage series if required
     cfv = cf["Variables"]
-    merge_list = [l for l in cfv.keys() if l[0:2] == "Fc" and "MergeSeries" in cfv[l].keys()]
+    merge_list = [l for l in list(cfv.keys()) if l[0:2] == "Fc" and "MergeSeries" in list(cfv[l].keys())]
     for label in merge_list:
         pfp_ts.CombineSeries(cf, ds3, label, save_originals=True)
     # correct Fc for storage term - only recommended if storage calculated from profile available
@@ -350,7 +350,7 @@
     # check to see if we have any imports
     pfp_gf.ImportSeries(cf, ds6)
     # check units of Fc
-    Fc_list = [label for label in ds6.series.keys() if label[0:2] == "Fc"]
+    Fc_list = [label for label in list(ds6.series.keys()) if label[0:2] == "Fc"]
     pfp_utils.CheckUnits(ds6, Fc_list, "umol/m2/s", convert_units=True)
     # get ER from the observed Fc
     pfp_rp.GetERFromFc(cf, ds6)
RefactoringTool: No changes to ./scripts/pfp_log.py
RefactoringTool: Refactored ./scripts/pfp_mpt.py
--- ./scripts/pfp_mpt.py	(original)
+++ ./scripts/pfp_mpt.py	(refactored)
@@ -174,7 +174,7 @@
         xl_sheet.write(row, 0, n)
         xl_sheet.write(row, 1, mpt_results["seasonal"]["value"][n])
         xl_sheet.write(row, 2, mpt_results["seasonal"]["count"][n])
-        if n in mpt_results["bootstraps"].keys():
+        if n in list(mpt_results["bootstraps"].keys()):
             values = mpt_results["bootstraps"][n]["values"]
             values = numpy.ma.masked_values(values, -9999)
             if numpy.ma.count(values) > 0:
@@ -182,14 +182,14 @@
     # write the temperature class results
     row = 10
     # write the headers
-    number_seasons = len(mpt_results["bootstraps"].keys())
+    number_seasons = len(list(mpt_results["bootstraps"].keys()))
     header_list = ["Temperature classes"]
     for s in range(number_seasons):
         header_list.append(str(s))
     for col, item in enumerate(header_list):
         xl_sheet.write(row, col, item)
     # write the data
-    for i in range(len(mpt_results["temperature_classes"].keys())):
+    for i in range(len(list(mpt_results["temperature_classes"].keys()))):
         row = row + 1
         xl_sheet.write(row, 0, i)
         for j in range(len(mpt_results["temperature_classes"][0]["values"])):
@@ -233,7 +233,7 @@
         xl_sheet.write(n+1, 0, year)
         if ustar_results["Years"][year]["annual"]["value"] != -9999:
             xl_sheet.write(n+1, 1, ustar_results["Years"][year]["annual"]["value"])
-            season_list = ustar_results["Years"][year]["bootstraps"].keys()
+            season_list = list(ustar_results["Years"][year]["bootstraps"].keys())
             values = ustar_results["Years"][year]["bootstraps"][0]["values"]
             season_list.remove(0)
             for s in season_list:
RefactoringTool: Refactored ./scripts/pfp_plot.py
--- ./scripts/pfp_plot.py	(original)
+++ ./scripts/pfp_plot.py	(refactored)
@@ -46,19 +46,19 @@
         loc = mdt.MinuteLocator()
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(minutes=30):
-        loc = mdt.MinuteLocator(byminute=range(0,60,5))
+        loc = mdt.MinuteLocator(byminute=list(range(0,60,5)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(hours=1):
-        loc = mdt.MinuteLocator(byminute=range(0,60,15))
+        loc = mdt.MinuteLocator(byminute=list(range(0,60,15)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(hours=6):
         loc = mdt.HourLocator()
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(days=1):
-        loc = mdt.HourLocator(byhour=range(0,24,3))
+        loc = mdt.HourLocator(byhour=list(range(0,24,3)))
         fmt = mdt.DateFormatter('%H:%M')
     elif delta <= td(days=3):
-        loc = mdt.HourLocator(byhour=range(0,24,12))
+        loc = mdt.HourLocator(byhour=list(range(0,24,12)))
         fmt = mdt.DateFormatter('%d/%m %H')
     elif delta <= td(weeks=2):
         loc = mdt.DayLocator()
@@ -88,7 +88,7 @@
     return yarray,nRecs,nNotM,nMskd
 
 def get_yaxislimitsfromcf(cf,nFig,maxkey,minkey,nSer,YArray):
-    if maxkey in cf['Plots'][str(nFig)].keys():                               # Y axis minima specified
+    if maxkey in list(cf['Plots'][str(nFig)].keys()):                               # Y axis minima specified
         maxlist = ast.literal_eval(cf['Plots'][str(nFig)][maxkey])     # Evaluate the minima list
         if str(maxlist[nSer])=='Auto':             # This entry is 'Auto' ...
             YAxMax = numpy.ma.max(YArray)                        # ... so take the array minimum value
@@ -96,7 +96,7 @@
             YAxMax = float(maxlist[nSer])         # Evaluate the entry for this series
     else:
         YAxMax = numpy.ma.max(YArray)                            # Y axis minima not given, use auto
-    if minkey in cf['Plots'][str(nFig)].keys():                               # Y axis minima specified
+    if minkey in list(cf['Plots'][str(nFig)].keys()):                               # Y axis minima specified
         minlist = ast.literal_eval(cf['Plots'][str(nFig)][minkey])     # Evaluate the minima list
         if str(minlist[nSer])=='Auto':             # This entry is 'Auto' ...
             YAxMin = numpy.ma.min(YArray)                        # ... so take the array minimum value
@@ -266,7 +266,7 @@
         else:
             fp_info["variables"][var]["upper"] = c.large_value
     # get the start and end datetimes for all files and find the overlap period
-    var_list = fp_info["variables"].keys()
+    var_list = list(fp_info["variables"].keys())
     ds_0 = ds[fp_info["variables"][var_list[0]]["in_filename"]]
     fp_info["variables"][var_list[0]]["start_date"] = ds_0.series["DateTime"]["Data"][0]
     fp_info["variables"][var_list[0]]["end_date"] = ds_0.series["DateTime"]["Data"][-1]
@@ -295,7 +295,7 @@
     if "Files" in cf:
         infilename = pfp_io.get_infilenamefromcf(cf)
         ds[infilename] = pfp_io.nc_read_series(infilename)
-    for var in cf["Variables"].keys():
+    for var in list(cf["Variables"].keys()):
         if "in_filename" in cf["Variables"][var]:
             if cf["Variables"][var]["in_filename"] not in ds:
                 infilename = cf["Variables"][var]["in_filename"]
@@ -314,7 +314,7 @@
     overlap_start = fp_info["general"]["overlap_start"]
     overlap_end = fp_info["general"]["overlap_end"]
     # get a list of site names and remove duplicates
-    var_list = fp_info["variables"].keys()
+    var_list = list(fp_info["variables"].keys())
     site_name_list = [fp_info["variables"][var]["site_name"] for var in var_list]
     site_name_list = list(set(site_name_list))
     site_name = ','.join(str(x) for x in site_name_list)
@@ -350,12 +350,12 @@
             nPerDay = int(float(24)*nPerHr + 0.5)
             nDays = len(ldt)/nPerDay
             # let's check the named variable is in the data structure
-            if nc_varname not in ds[infilename].series.keys():
+            if nc_varname not in list(ds[infilename].series.keys()):
                 # if it isn't, let's look for an alias
-                if nc_varname in aliases.keys():
+                if nc_varname in list(aliases.keys()):
                     found_alias = False
                     for alias in aliases[nc_varname]:
-                        if alias in ds[infilename].series.keys():
+                        if alias in list(ds[infilename].series.keys()):
                             nc_varname = alias
                             found_alias = True
                     if not found_alias:
@@ -419,7 +419,7 @@
 def plot_fluxnet(cf):
     """ Plot the FluxNet style plots. """
 
-    series_list = cf["Variables"].keys()
+    series_list = list(cf["Variables"].keys())
     infilename = pfp_io.get_infilenamefromcf(cf)
 
     ds = pfp_io.nc_read_series(infilename)
@@ -432,7 +432,7 @@
     sdt = ldt[0]
     edt = ldt[-1]
     # Tumbarumba doesn't have RH in the netCDF files
-    if "RH" not in ds.series.keys():
+    if "RH" not in list(ds.series.keys()):
         Ah,f,a = pfp_utils.GetSeriesasMA(ds,'Ah')
         Ta,f,a = pfp_utils.GetSeriesasMA(ds,'Ta')
         RH = pfp_mf.RHfromabsolutehumidity(Ah, Ta)
@@ -443,7 +443,7 @@
     nFig = 0
     plt.ion()
     for series in series_list:
-        if series not in ds.series.keys():
+        if series not in list(ds.series.keys()):
             logger.error("Series "+series+" not found in input file, skipping ...")
             continue
         logger.info(" Doing plot for "+series)
@@ -502,8 +502,8 @@
         fig = plt.figure(figsize=(p['PlotWidth'],p['PlotHeight']))
     fig.canvas.set_window_title(p['PlotDescription'])
     plt.figtext(0.5,0.95,SiteName+': '+p['PlotDescription'],ha='center',size=16)
-    for ThisOne, n in zip(p['SeriesList'],range(p['nGraphs'])):
-        if ThisOne in dsa.series.keys():
+    for ThisOne, n in zip(p['SeriesList'],list(range(p['nGraphs']))):
+        if ThisOne in list(dsa.series.keys()):
             aflag = dsa.series[ThisOne]['Flag']
             p['Units'] = dsa.series[ThisOne]['Attr']['units']
             p['YAxOrg'] = p['ts_YAxOrg'] + n*p['yaxOrgOffset']
@@ -512,7 +512,7 @@
             nSer = p['SeriesList'].index(ThisOne)
             p['LYAxMax'],p['LYAxMin'] = get_yaxislimitsfromcf(cf,nFig,'YLMax','YLMin',nSer,L1YArray)
             plot_onetimeseries_left(fig,n,ThisOne,L1XArray,L1YArray,p)
-        if ThisOne in dsb.series.keys():
+        if ThisOne in list(dsb.series.keys()):
             bflag = dsb.series[ThisOne]['Flag']
             p['Units'] = dsb.series[ThisOne]['Attr']['units']
             p['YAxOrg'] = p['ts_YAxOrg'] + n*p['yaxOrgOffset']
@@ -534,7 +534,7 @@
             hr2_ax.plot(Hr2,Av2,'y-',Hr2,Mx2,'r-',Hr2,Mn2,'b-')
             section = pfp_utils.get_cfsection(cf, ThisOne, mode='quiet')
             if section != None:
-                if 'DiurnalCheck' in cf[section][ThisOne].keys():
+                if 'DiurnalCheck' in list(cf[section][ThisOne].keys()):
                     NSdarr = numpy.array(pfp_ck.parse_rangecheck_limit(cf[section][ThisOne]['DiurnalCheck']['numsd']))
                     nSd = NSdarr[Month-1]
                     hr2_ax.plot(Hr2,Av2+nSd*Sd2,'r.',Hr2,Av2-nSd*Sd2,'b.')
@@ -785,7 +785,7 @@
     ncfilename = pfp_io.get_infilenamefromcf(cf)
     # read the netCDF file and return the data structure "ds"
     ds = pfp_io.nc_read_series(ncfilename)
-    series_list = ds.series.keys()
+    series_list = list(ds.series.keys())
     # get the time step
     ts = int(ds.globalattributes["time_step"])
     # get the site name
RefactoringTool: Refactored ./scripts/pfp_rp.py
--- ./scripts/pfp_rp.py	(original)
+++ ./scripts/pfp_rp.py	(refactored)
@@ -39,7 +39,7 @@
     Date: June 2015
     """
     ts = int(ds.globalattributes["time_step"])
-    series_list = ds.series.keys()
+    series_list = list(ds.series.keys())
     Fe_list = [item for item in series_list if "Fe" in item[0:2]]
     for label in Fe_list:
         Fe, flag, attr = pfp_utils.GetSeriesasMA(ds, label)
@@ -71,7 +71,7 @@
     Fsd_threshold = float(pfp_utils.get_keyvaluefromcf(cf, ["Options"], "Fsd_threshold", default=10))
     # get the incoming shortwave radiation
     Fsd, _, _ = pfp_utils.GetSeriesasMA(ds, "Fsd")
-    for label in l6_info["NetEcosystemExchange"].keys():
+    for label in list(l6_info["NetEcosystemExchange"].keys()):
         if "Fc" not in l6_info["NetEcosystemExchange"][label] and "ER" not in l6_info["NetEcosystemExchange"][label]:
             continue
         Fc_label = l6_info["NetEcosystemExchange"][label]["Fc"]
@@ -109,7 +109,7 @@
     Author: PRI
     Date: May 2015
     """
-    for nee_name in cf["NetEcosystemExchange"].keys():
+    for nee_name in list(cf["NetEcosystemExchange"].keys()):
         nep_name = nee_name.replace("NEE", "NEP")
         nee, flag, attr = pfp_utils.GetSeriesasMA(ds, nee_name)
         nep = float(-1)*nee
@@ -177,7 +177,7 @@
     iel = l6_info["ERUsingLasslop"]
     ielo = iel["outputs"]
     # get a list of the required outputs
-    outputs = iel["outputs"].keys()
+    outputs = list(iel["outputs"].keys())
     # need to loop over more than 1 output
     output = outputs[0]
     drivers = ielo[output]["drivers"]
@@ -332,7 +332,7 @@
         fig_num = plt.get_fignums()[-1] + 1
     title = site_name+" : ER estimated using Lasslop et al"
     pd = pfp_rpLL.rpLL_initplot(site_name=site_name,label="ER",fig_num=fig_num,title=title,
-                         nDrivers=len(data.keys()),startdate=str(startdate),enddate=str(enddate))
+                         nDrivers=len(list(data.keys())),startdate=str(startdate),enddate=str(enddate))
     pfp_rpLL.rpLL_plot(pd, ds, output, drivers, target, l6_info)
 
 def ERUsingLloydTaylor(cf, ds, l6_info):
@@ -374,7 +374,7 @@
         logger.error(msg)
         return
     # loop over the series
-    outputs = iel["outputs"].keys()
+    outputs = list(iel["outputs"].keys())
     for output in outputs:
         # create dictionaries for the results
         E0_results = {"variables":{}}
@@ -398,9 +398,9 @@
         (step_date_index_dict,
          all_date_index_dict,
          year_index_dict) = pfp_rpLT.get_dates(datetime_array, configs_dict)
-        date_array = numpy.array(all_date_index_dict.keys())
+        date_array = numpy.array(list(all_date_index_dict.keys()))
         date_array.sort()
-        step_date_array = numpy.array(step_date_index_dict.keys())
+        step_date_array = numpy.array(list(step_date_index_dict.keys()))
         step_date_array.sort()
         # Create variable name lists for results output
         series_rslt_list = ['Nocturnally derived Re', 'GPP from nocturnal derived Re',
@@ -440,13 +440,13 @@
             index = numpy.where(year_array == yr)
             opt_params_dict['Eo'][index] = Eo_dict[yr]
             opt_params_dict['Eo error code'][index] = EoQC_dict[yr]
-        E0_results["variables"]["DateTime"] = {"data":[datetime.datetime(int(yr),1,1) for yr in Eo_dict.keys()],
+        E0_results["variables"]["DateTime"] = {"data":[datetime.datetime(int(yr),1,1) for yr in list(Eo_dict.keys())],
                                                "attr":{"units":"Year","format":"yyyy"}}
-        E0_results["variables"]["E0"] = {"data":[float(Eo_dict[yr]) for yr in Eo_dict.keys()],
+        E0_results["variables"]["E0"] = {"data":[float(Eo_dict[yr]) for yr in list(Eo_dict.keys())],
                                          "attr":{"units":"none","format":"0"}}
-        E0_raw_results["variables"]["DateTime"] = {"data":[datetime.datetime(int(yr),1,1) for yr in Eo_raw_dict.keys()],
+        E0_raw_results["variables"]["DateTime"] = {"data":[datetime.datetime(int(yr),1,1) for yr in list(Eo_raw_dict.keys())],
                                                    "attr":{"units":"Year","format":"yyyy"}}
-        E0_raw_results["variables"]["E0"] = {"data":[float(Eo_raw_dict[yr]) for yr in Eo_raw_dict.keys()],
+        E0_raw_results["variables"]["E0"] = {"data":[float(Eo_raw_dict[yr]) for yr in list(Eo_raw_dict.keys())],
                                              "attr":{"units":"none","format":"0"}}
         # write the E0 values to the Excel file
         pfp_io.xl_write_data(xl_sheet,E0_raw_results["variables"],xlCol=0)
@@ -667,7 +667,7 @@
         sa_threshold = int(pfp_utils.get_keyvaluefromcf(cf, ["Options"], "sa_threshold", default="-5"))
         attr["sa_threshold"] = str(sa_threshold)
         # we are using solar altitude to define day/night
-        if "solar_altitude" not in ds.series.keys():
+        if "solar_altitude" not in list(ds.series.keys()):
             pfp_ts.get_synthetic_fsd(ds)
         sa, _, _ = pfp_utils.GetSeriesasMA(ds, "solar_altitude")
         idx = numpy.ma.where(sa < sa_threshold)[0]
@@ -720,7 +720,7 @@
         sa_threshold = int(pfp_utils.get_keyvaluefromcf(cf, ["Options"], "sa_threshold", default="-5"))
         attr["sa_threshold"] = str(sa_threshold)
         # we are using solar altitude to define day/night
-        if "solar_altitude" not in ds.series.keys():
+        if "solar_altitude" not in list(ds.series.keys()):
             pfp_ts.get_synthetic_fsd(ds)
         sa, _, _ = pfp_utils.GetSeriesasMA(ds, "solar_altitude")
         index = numpy.ma.where(sa < sa_threshold)[0]
@@ -816,7 +816,7 @@
         sa_threshold = int(pfp_utils.get_keyvaluefromcf(cf, ["Options"], "sa_threshold", default="-5"))
         attr["sa_threshold"] = str(sa_threshold)
         # we are using solar altitude to define day/night
-        if "solar_altitude" not in ds.series.keys():
+        if "solar_altitude" not in list(ds.series.keys()):
             pfp_ts.get_synthetic_fsd(ds)
         sa, _, _ = pfp_utils.GetSeriesasMA(ds, "solar_altitude")
         index = numpy.ma.where(sa < sa_threshold)[0]
@@ -854,7 +854,7 @@
     Author: PRI
     Date: March 2016
     """
-    year_list = ustar_dict.keys()
+    year_list = list(ustar_dict.keys())
     year_list.sort()
     # now loop over the years in the data to apply the ustar threshold
     turbulence_indicator = {"values":numpy.zeros(len(ldt)), "attr":{}}
@@ -906,7 +906,7 @@
     # use this value to indicate the transition from day to night
     dinds = numpy.ediff1d(ind_day, to_begin=0)
     # get the list of years
-    year_list = ustar_dict.keys()
+    year_list = list(ustar_dict.keys())
     year_list.sort()
     # now loop over the years in the data to apply the ustar threshold
     # ustar >= threshold ==> ind_ustar = 1 else ind_ustar = 0
@@ -949,7 +949,7 @@
     ustar_threshold_list = []
     msg = " Using values from control file ustar_threshold section"
     logger.info(msg)
-    for n in cf["ustar_threshold"].keys():
+    for n in list(cf["ustar_threshold"].keys()):
         ustar_string = cf["ustar_threshold"][str(n)]
         ustar_list = ustar_string.split(",")
         ustar_threshold_list.append(ustar_list)
@@ -1110,7 +1110,7 @@
     """
     ddv = daily_dict["variables"]
     type_list = []
-    for item in ddv.keys():
+    for item in list(ddv.keys()):
         if item[0:2] == "ER": type_list.append(item[2:])
     for item in type_list:
         if "NEE" + item not in ddv or "GPP" + item not in ddv:
@@ -1191,11 +1191,11 @@
 def L6_summary_plotcumulative(cf, ds, cumulative_dict):
     # cumulative plots
     color_list = ["blue","red","green","yellow","magenta","black","cyan","brown"]
-    year_list = [y for y in cumulative_dict.keys() if y not in ["all"]]
+    year_list = [y for y in list(cumulative_dict.keys()) if y not in ["all"]]
     year_list.sort()
     cdy0 = cumulative_dict[year_list[0]]
     type_list = []
-    for item in cdy0["variables"].keys():
+    for item in list(cdy0["variables"].keys()):
         if item[0:2]=="ER": type_list.append(item[2:])
     for item in type_list:
         if "NEE"+item not in cdy0["variables"] or "GPP"+item not in cdy0["variables"]:
@@ -1304,11 +1304,11 @@
     series_dict = {"daily":{},"annual":{},"cumulative":{},"lists":{}}
     # adjust units of NEE, NEP, GPP and ER
     sdl = series_dict["lists"]
-    sdl["nee"] = [item for item in cf["NetEcosystemExchange"].keys() if "NEE" in item[0:3] and item in ds.series.keys()]
-    sdl["gpp"] = [item for item in cf["GrossPrimaryProductivity"].keys() if "GPP" in item[0:3] and item in ds.series.keys()]
-    sdl["fre"] = [item for item in cf["EcosystemRespiration"].keys() if "ER" in item[0:2] and item in ds.series.keys()]
+    sdl["nee"] = [item for item in list(cf["NetEcosystemExchange"].keys()) if "NEE" in item[0:3] and item in list(ds.series.keys())]
+    sdl["gpp"] = [item for item in list(cf["GrossPrimaryProductivity"].keys()) if "GPP" in item[0:3] and item in list(ds.series.keys())]
+    sdl["fre"] = [item for item in list(cf["EcosystemRespiration"].keys()) if "ER" in item[0:2] and item in list(ds.series.keys())]
     sdl["nep"] = [item.replace("NEE","NEP") for item in sdl["nee"]]
-    sdl["nep"] = [item for item in sdl["nep"] if item in ds.series.keys()]
+    sdl["nep"] = [item for item in sdl["nep"] if item in list(ds.series.keys())]
     sdl["co2"] = sdl["nee"]+sdl["nep"]+sdl["gpp"]+sdl["fre"]
     for item in sdl["co2"]:
         series_dict["daily"][item] = {}
@@ -1317,51 +1317,51 @@
         series_dict["daily"][item]["format"] = "0.00"
         series_dict["cumulative"][item]["operator"] = "sum"
         series_dict["cumulative"][item]["format"] = "0.00"
-    sdl["ET"] = [item for item in ds.series.keys() if "ET" in item[0:2]]
-    sdl["Precip"] = [item for item in ds.series.keys() if "Precip" in item[0:6]]
+    sdl["ET"] = [item for item in list(ds.series.keys()) if "ET" in item[0:2]]
+    sdl["Precip"] = [item for item in list(ds.series.keys()) if "Precip" in item[0:6]]
     sdl["h2o"] = sdl["ET"]+sdl["Precip"]
     for item in sdl["h2o"]:
         series_dict["daily"][item] = {"operator":"sum","format":"0.00"}
         series_dict["cumulative"][item] = {"operator":"sum","format":"0.00"}
-    if "Ah" in ds.series.keys():
+    if "Ah" in list(ds.series.keys()):
         series_dict["daily"]["Ah"] = {"operator":"average","format":"0.00"}
-    if "CO2" in ds.series.keys():
+    if "CO2" in list(ds.series.keys()):
         series_dict["daily"]["CO2"] = {"operator":"average","format":"0.0"}
-    if "Fc" in ds.series.keys():
+    if "Fc" in list(ds.series.keys()):
         series_dict["daily"]["Fc"] = {"operator":"average","format":"0.00"}
-    if "Fe" in ds.series.keys():
+    if "Fe" in list(ds.series.keys()):
         series_dict["daily"]["Fe"] = {"operator":"average","format":"0.0"}
-    if "Fh" in ds.series.keys():
+    if "Fh" in list(ds.series.keys()):
         series_dict["daily"]["Fh"] = {"operator":"average","format":"0.0"}
-    if "Fg" in ds.series.keys():
+    if "Fg" in list(ds.series.keys()):
         series_dict["daily"]["Fg"] = {"operator":"average","format":"0.0"}
-    if "Fn" in ds.series.keys():
+    if "Fn" in list(ds.series.keys()):
         series_dict["daily"]["Fn"] = {"operator":"average","format":"0.0"}
-    if "Fsd" in ds.series.keys():
+    if "Fsd" in list(ds.series.keys()):
         series_dict["daily"]["Fsd"] = {"operator":"average","format":"0.0"}
-    if "Fsu" in ds.series.keys():
+    if "Fsu" in list(ds.series.keys()):
         series_dict["daily"]["Fsu"] = {"operator":"average","format":"0.0"}
-    if "Fld" in ds.series.keys():
+    if "Fld" in list(ds.series.keys()):
         series_dict["daily"]["Fld"] = {"operator":"average","format":"0.0"}
-    if "Flu" in ds.series.keys():
+    if "Flu" in list(ds.series.keys()):
         series_dict["daily"]["Flu"] = {"operator":"average","format":"0.0"}
-    if "ps" in ds.series.keys():
+    if "ps" in list(ds.series.keys()):
         series_dict["daily"]["ps"] = {"operator":"average","format":"0.00"}
-    if "RH" in ds.series.keys():
+    if "RH" in list(ds.series.keys()):
         series_dict["daily"]["RH"] = {"operator":"average","format":"0"}
-    if "SH" in ds.series.keys():
+    if "SH" in list(ds.series.keys()):
         series_dict["daily"]["SH"] = {"operator":"average","format":"0.0000"}
-    if "Sws" in ds.series.keys():
+    if "Sws" in list(ds.series.keys()):
         series_dict["daily"]["Sws"] = {"operator":"average","format":"0.000"}
-    if "Ta" in ds.series.keys():
+    if "Ta" in list(ds.series.keys()):
         series_dict["daily"]["Ta"] = {"operator":"average","format":"0.00"}
-    if "Ts" in ds.series.keys():
+    if "Ts" in list(ds.series.keys()):
         series_dict["daily"]["Ts"] = {"operator":"average","format":"0.00"}
-    if "ustar" in ds.series.keys():
+    if "ustar" in list(ds.series.keys()):
         series_dict["daily"]["ustar"] = {"operator":"average","format":"0.00"}
-    if "VP" in ds.series.keys():
+    if "VP" in list(ds.series.keys()):
         series_dict["daily"]["VP"] = {"operator":"average","format":"0.000"}
-    if "Ws" in ds.series.keys():
+    if "Ws" in list(ds.series.keys()):
         series_dict["daily"]["Ws"] = {"operator":"average","format":"0.00"}
     series_dict["annual"] = series_dict["daily"]
     series_dict["monthly"] = series_dict["daily"]
@@ -1399,10 +1399,10 @@
                                            "flag":f0,
                                            "attr":{"units":"Days","format":"dd/mm/yyyy",
                                                    "time_step":"Daily"}}
-    series_list = series_dict["daily"].keys()
+    series_list = list(series_dict["daily"].keys())
     series_list.sort()
     for item in series_list:
-        if item not in ds.series.keys(): continue
+        if item not in list(ds.series.keys()): continue
         daily_dict["variables"][item] = {"data":[],"attr":{}}
         variable = pfp_utils.GetVariable(ds, item, start=si, end=ei)
         if item in series_dict["lists"]["co2"]:
@@ -1425,7 +1425,7 @@
         daily_dict["variables"][item]["attr"]["format"] = series_dict["daily"][item]["format"]
         # copy some of the variable attributes
         default_list = ["long_name", "standard_name", "height", "instrument", "group_name"]
-        descr_list = [d for d in variable["Attr"].keys() if "description" in d]
+        descr_list = [d for d in list(variable["Attr"].keys()) if "description" in d]
         vattr_list = default_list + descr_list
         for attr in vattr_list:
             if attr in variable["Attr"]:
@@ -1500,7 +1500,7 @@
     nc_var.setncattr('standard_name', 'longitude')
     nc_var.setncattr('units', 'degrees east')
     # get a list of variables to write to the netCDF file
-    labels = sorted([label for label in data_dict["variables"].keys() if label != "DateTime"])
+    labels = sorted([label for label in list(data_dict["variables"].keys()) if label != "DateTime"])
     # write the variables to the netCDF file object
     for label in labels:
         nc_var = nc_obj.createVariable(label, "d", dims)
@@ -1541,7 +1541,7 @@
                                              "attr":{"units":"Months", "format":"dd/mm/yyyy",
                                                      "time_step":"Monthly"}}
     # create arrays in monthly_dict
-    series_list = series_dict["monthly"].keys()
+    series_list = list(series_dict["monthly"].keys())
     series_list.sort()
     # create the data arrays
     for item in series_list:
@@ -1559,7 +1559,7 @@
         ei = pfp_utils.GetDateIndex(dt, str(end_date), ts=ts, default=len(dt)-1)
         monthly_dict["variables"]["DateTime"]["data"].append(dt[si])
         for item in series_list:
-            if item not in ds.series.keys(): continue
+            if item not in list(ds.series.keys()): continue
             variable = pfp_utils.GetVariable(ds, item, start=si, end=ei)
             if item in series_dict["lists"]["co2"]:
                 variable = pfp_utils.convert_units_func(ds, variable, "gC/m2")
@@ -1579,7 +1579,7 @@
             monthly_dict["variables"][item]["attr"]["format"] = series_dict["monthly"][item]["format"]
             # copy some of the variable attributes
             default_list = ["long_name", "standard_name", "height", "instrument", "group_name"]
-            descr_list = [d for d in variable["Attr"].keys() if "description" in d]
+            descr_list = [d for d in list(variable["Attr"].keys()) if "description" in d]
             vattr_list = default_list + descr_list
             for attr in vattr_list:
                 if attr in variable["Attr"]:
@@ -1612,7 +1612,7 @@
     ldt = dt[si:ei+1]
     start_year = ldt[0].year
     end_year = ldt[-1].year
-    year_list = range(start_year, end_year+1, 1)
+    year_list = list(range(start_year, end_year+1, 1))
     nYears = len(year_list)
     annual_dict = {"globalattributes":{}, "variables":{}}
     # copy the global attributes
@@ -1625,7 +1625,7 @@
                                          "flag":numpy.zeros(nYears, dtype=numpy.int32),
                                          "attr":{"units":"Number of days","format":"0"}}
     # create arrays in annual_dict
-    series_list = series_dict["annual"].keys()
+    series_list = list(series_dict["annual"].keys())
     series_list.sort()
     for item in series_list:
         annual_dict["variables"][item] = {"data":numpy.ma.array([float(-9999)]*len(year_list)),
@@ -1642,7 +1642,7 @@
         nDays = int((ei-si+1)/nperDay+0.5)
         annual_dict["variables"]["nDays"]["data"][i] = nDays
         for item in series_list:
-            if item not in ds.series.keys(): continue
+            if item not in list(ds.series.keys()): continue
             variable = pfp_utils.GetVariable(ds, item, start=si, end=ei)
             if item in series_dict["lists"]["co2"]:
                 variable = pfp_utils.convert_units_func(ds, variable, "gC/m2")
@@ -1660,7 +1660,7 @@
             annual_dict["variables"][item]["attr"]["format"] = series_dict["annual"][item]["format"]
             # copy some of the variable attributes
             default_list = ["long_name", "standard_name", "height", "instrument", "group_name"]
-            descr_list = [d for d in variable["Attr"].keys() if "description" in d]
+            descr_list = [d for d in list(variable["Attr"].keys()) if "description" in d]
             vattr_list = default_list + descr_list
             for attr in vattr_list:
                 if attr in variable["Attr"]:
@@ -1743,7 +1743,7 @@
     # subtract 1 time step from the datetime to avoid orphan years
     cdt = dt["Data"] - datetime.timedelta(minutes=ts)
     years = sorted(list(set([ldt.year for ldt in cdt])))
-    series_list = series_dict["cumulative"].keys()
+    series_list = list(series_dict["cumulative"].keys())
     cumulative_dict = {}
     for year in years:
         cumulative_dict[str(year)] = cdyr = {"globalattributes":{}, "variables":{}}
@@ -1771,7 +1771,7 @@
             cdyr["variables"][item]["attr"]["units"] = cdyr["variables"][item]["attr"]["units"]+"/year"
             # copy some of the variable attributes
             default_list = ["long_name", "standard_name", "height", "instrument", "group_name"]
-            descr_list = [d for d in variable["Attr"].keys() if "description" in d]
+            descr_list = [d for d in list(variable["Attr"].keys()) if "description" in d]
             vattr_list = default_list + descr_list
             for attr in vattr_list:
                 if attr in variable["Attr"]:
@@ -1795,7 +1795,7 @@
         cdyr["variables"][item]["attr"]["format"] = series_dict["cumulative"][item]["format"]
         # copy some of the variable attributes
         default_list = ["long_name", "standard_name", "height", "instrument", "group_name"]
-        descr_list = [d for d in variable["Attr"].keys() if "description" in d]
+        descr_list = [d for d in list(variable["Attr"].keys()) if "description" in d]
         vattr_list = default_list + descr_list
         for attr in vattr_list:
             if attr in variable["Attr"]:
@@ -1818,23 +1818,23 @@
     # add key for suppressing output of intermediate variables e.g. Ta_aws
     opt = pfp_utils.get_keyvaluefromcf(cf, ["Options"], "KeepIntermediateSeries", default="No")
     l6_info["RemoveIntermediateSeries"] = {"KeepIntermediateSeries": opt, "not_output": []}
-    if "EcosystemRespiration" in cf.keys():
-        for output in cf["EcosystemRespiration"].keys():
-            if "ERUsingSOLO" in cf["EcosystemRespiration"][output].keys():
+    if "EcosystemRespiration" in list(cf.keys()):
+        for output in list(cf["EcosystemRespiration"].keys()):
+            if "ERUsingSOLO" in list(cf["EcosystemRespiration"][output].keys()):
                 rpSOLO_createdict(cf, ds, l6_info, output, "ERUsingSOLO", 610)
-            if "ERUsingLloydTaylor" in cf["EcosystemRespiration"][output].keys():
+            if "ERUsingLloydTaylor" in list(cf["EcosystemRespiration"][output].keys()):
                 pfp_rpLT.rpLT_createdict(cf, ds, l6_info, output, "ERUsingLloydTaylor", 620)
-            if "ERUsingLasslop" in cf["EcosystemRespiration"][output].keys():
+            if "ERUsingLasslop" in list(cf["EcosystemRespiration"][output].keys()):
                 pfp_rpLL.rpLL_createdict(cf, ds, l6_info, output, "ERUsingLasslop", 630)
-            if "MergeSeries" in cf["EcosystemRespiration"][output].keys():
+            if "MergeSeries" in list(cf["EcosystemRespiration"][output].keys()):
                 rpMergeSeries_createdict(cf, ds, l6_info, output, "MergeSeries")
-    if "NetEcosystemExchange" in cf.keys():
+    if "NetEcosystemExchange" in list(cf.keys()):
         l6_info["NetEcosystemExchange"] = {}
-        for output in cf["NetEcosystemExchange"].keys():
+        for output in list(cf["NetEcosystemExchange"].keys()):
             rpNEE_createdict(cf, ds, l6_info["NetEcosystemExchange"], output)
-    if "GrossPrimaryProductivity" in cf.keys():
+    if "GrossPrimaryProductivity" in list(cf.keys()):
         l6_info["GrossPrimaryProductivity"] = {}
-        for output in cf["GrossPrimaryProductivity"].keys():
+        for output in list(cf["GrossPrimaryProductivity"].keys()):
             rpGPP_createdict(cf, ds, l6_info["GrossPrimaryProductivity"], output)
     return l6_info
 
@@ -1857,7 +1857,7 @@
     # make the L6 "description" attrubute for the target variable
     descr_level = "description_" + ds.globalattributes["nc_level"]
     # calculate GPP from NEE and ER
-    for label in l6_info["GrossPrimaryProductivity"].keys():
+    for label in list(l6_info["GrossPrimaryProductivity"].keys()):
         if ("NEE" not in l6_info["GrossPrimaryProductivity"][label] and
             "ER" not in l6_info["GrossPrimaryProductivity"][label]):
             continue
@@ -1895,7 +1895,7 @@
     opt = pfp_utils.get_keyvaluefromcf(cf, ["GrossPrimaryProductivity", label], "ER", default=default)
     info[label]["ER"] = opt
     # create an empty series in ds if the output series doesn't exist yet
-    if info[label]["output"] not in ds.series.keys():
+    if info[label]["output"] not in list(ds.series.keys()):
         data, flag, attr = pfp_utils.MakeEmptySeries(ds, info[label]["output"])
         pfp_utils.CreateSeries(ds, info[label]["output"], data, flag, attr)
     return
@@ -1916,7 +1916,7 @@
     opt = pfp_utils.get_keyvaluefromcf(cf, sl, "ER", default=default)
     info[label]["ER"] = opt
     # create an empty series in ds if the output series doesn't exist yet
-    if info[label]["output"] not in ds.series.keys():
+    if info[label]["output"] not in list(ds.series.keys()):
         data, flag, attr = pfp_utils.MakeEmptySeries(ds, info[label]["output"])
         pfp_utils.CreateSeries(ds, info[label]["output"], data, flag, attr)
     return
@@ -1928,7 +1928,7 @@
     # create the merge directory in the info dictionary
     if called_by not in l6_info:
         l6_info[called_by] = {}
-    if "standard" not in l6_info[called_by].keys():
+    if "standard" not in list(l6_info[called_by].keys()):
         l6_info[called_by]["standard"] = {}
     # create the dictionary keys for this series
     l6_info[called_by]["standard"][label] = {}
@@ -1940,7 +1940,7 @@
     sources = pfp_utils.GetMergeSeriesKeys(cf, label, section="EcosystemRespiration")
     l6_info[called_by]["standard"][label]["source"] = sources
     # create an empty series in ds if the output series doesn't exist yet
-    if l6_info[called_by]["standard"][label]["output"] not in ds.series.keys():
+    if l6_info[called_by]["standard"][label]["output"] not in list(ds.series.keys()):
         variable = pfp_utils.CreateEmptyVariable(label, nrecs)
         pfp_utils.CreateVariable(ds, variable)
     return
@@ -1959,7 +1959,7 @@
     # make the L6 "description" attrubute for the target variable
     descr_level = "description_" + ds.globalattributes["nc_level"]
     # create the dictionary keys for this series
-    if called_by not in l6_info.keys():
+    if called_by not in list(l6_info.keys()):
         l6_info[called_by] = {"outputs": {}, "info": {"source": "Fc", "target": "ER"}, "gui": {}}
         # only need to create the ["info"] dictionary on the first pass
         pfp_gf.gfSOLO_createdict_info(cf, ds, l6_info, called_by)
@@ -1971,9 +1971,9 @@
     pfp_gf.gfSOLO_createdict_outputs(cf, l6_info, output, called_by, flag_code)
     # create an empty series in ds if the SOLO output series doesn't exist yet
     Fc = pfp_utils.GetVariable(ds, l6_info[called_by]["info"]["source"])
-    model_outputs = cf["EcosystemRespiration"][output][called_by].keys()
+    model_outputs = list(cf["EcosystemRespiration"][output][called_by].keys())
     for model_output in model_outputs:
-        if model_output not in ds.series.keys():
+        if model_output not in list(ds.series.keys()):
             # create an empty variable
             variable = pfp_utils.CreateEmptyVariable(model_output, nrecs)
             variable["Attr"]["long_name"] = "Ecosystem respiration"
RefactoringTool: Refactored ./scripts/pfp_rpLL.py
--- ./scripts/pfp_rpLL.py	(original)
+++ ./scripts/pfp_rpLL.py	(refactored)
@@ -337,7 +337,7 @@
     # make the L6 "description" attrubute for the target variable
     descr_level = "description_" + ds.globalattributes["nc_level"]
     # create the Lasslop settings directory
-    if called_by not in l6_info.keys():
+    if called_by not in list(l6_info.keys()):
         l6_info[called_by] = {"outputs": {}, "info": {}, "gui": {}}
     # get the info section
     rpLL_createdict_info(cf, ds, l6_info[called_by], called_by)
@@ -347,9 +347,9 @@
     rpLL_createdict_outputs(cf, l6_info[called_by], output, called_by, flag_code)
     # create an empty series in ds if the output series doesn't exist yet
     Fc = pfp_utils.GetVariable(ds, l6_info[called_by]["info"]["source"])
-    model_outputs = cf["EcosystemRespiration"][output][called_by].keys()
+    model_outputs = list(cf["EcosystemRespiration"][output][called_by].keys())
     for model_output in model_outputs:
-        if model_output not in ds.series.keys():
+        if model_output not in list(ds.series.keys()):
             l6_info["RemoveIntermediateSeries"]["not_output"].append(model_output)
             # create an empty variable
             variable = pfp_utils.CreateEmptyVariable(model_output, nrecs)
@@ -434,7 +434,7 @@
     eo = erll["outputs"]
     # loop over the outputs listed in the control file
     section = "EcosystemRespiration"
-    outputs = cf[section][target][called_by].keys()
+    outputs = list(cf[section][target][called_by].keys())
     for output in outputs:
         # create the dictionary keys for this series
         eo[output] = {}
@@ -469,7 +469,7 @@
           "xy_height":0.20,"xy_width":0.20,"xyts_space":0.05,"xyts_space":0.05,
           "ts_width":0.9}
     # set the keyword arguments
-    for key, value in kwargs.iteritems():
+    for key, value in kwargs.items():
         pd[key] = value
     # calculate bottom of the first time series and the height of the time series plots
     pd["ts_bottom"] = pd["margin_bottom"]+pd["xy_height"]+pd["xyts_space"]
@@ -578,7 +578,7 @@
     ts_axes[0].text(0.05, 0.85, TextStr, color='b', horizontalalignment='left', transform=ts_axes[0].transAxes)
     TextStr = output + '(' + ds.series[output]['Attr']['units'] + ')'
     ts_axes[0].text(0.85, 0.85, TextStr, color='r', horizontalalignment='right', transform=ts_axes[0].transAxes)
-    for ThisOne, i in zip(drivers, range(1, pd["nDrivers"] + 1)):
+    for ThisOne, i in zip(drivers, list(range(1, pd["nDrivers"] + 1))):
         this_bottom = pd["ts_bottom"] + i*pd["ts_height"]
         rect = [pd["margin_left"], this_bottom, pd["ts_width"], pd["ts_height"]]
         ts_axes.append(plt.axes(rect, sharex=ts_axes[0]))
RefactoringTool: Refactored ./scripts/pfp_rpLT.py
--- ./scripts/pfp_rpLT.py	(original)
+++ ./scripts/pfp_rpLT.py	(refactored)
@@ -92,7 +92,7 @@
     start_date = datetime_array[0].date()
     end_date = datetime_array[-1].date()
     num_days = (end_date - start_date).days + 1 # Add 1 so is inclusive of both end members
-    all_dates_array = numpy.array([start_date + datetime.timedelta(i) for i in xrange(num_days)])
+    all_dates_array = numpy.array([start_date + datetime.timedelta(i) for i in range(num_days)])
 
     # Create a shifted array
     shift_mins = 60 * configs_dict['measurement_interval']
@@ -213,7 +213,7 @@
     Eo_pass_keys = []
     Eo_range_fail_keys = []
     Eo_nan_fail_keys = []
-    year_list = year_index_dict.keys()
+    year_list = list(year_index_dict.keys())
     logger.info(" E0 optimised using whole year is as follows")
     for yr in year_list:
 
@@ -307,7 +307,7 @@
     # make the L6 "description" attrubute for the target variable
     descr_level = "description_" + ds.globalattributes["nc_level"]
     # create the LT settings directory
-    if called_by not in l6_info.keys():
+    if called_by not in list(l6_info.keys()):
         l6_info[called_by] = {"outputs": {}, "info": {"source": "Fc", "target": "ER"}, "gui": {}}
     # get the info section
     rpLT_createdict_info(cf, ds, l6_info[called_by], called_by)
@@ -317,9 +317,9 @@
     rpLT_createdict_outputs(cf, l6_info[called_by], output, called_by, flag_code)
     # create an empty series in ds if the output series doesn't exist yet
     Fc = pfp_utils.GetVariable(ds, l6_info[called_by]["info"]["source"])
-    model_outputs = cf["EcosystemRespiration"][output][called_by].keys()
+    model_outputs = list(cf["EcosystemRespiration"][output][called_by].keys())
     for model_output in model_outputs:
-        if model_output not in ds.series.keys():
+        if model_output not in list(ds.series.keys()):
             l6_info["RemoveIntermediateSeries"]["not_output"].append(model_output)
             # create an empty variable
             variable = pfp_utils.CreateEmptyVariable(model_output, nrecs)
@@ -398,7 +398,7 @@
     eo = erlt["outputs"]
     # loop over the outputs listed in the control file
     section = "EcosystemRespiration"
-    outputs = cf[section][target][called_by].keys()
+    outputs = list(cf[section][target][called_by].keys())
     for output in outputs:
         # create the dictionary keys for this series
         eo[output] = {}
@@ -437,7 +437,7 @@
           "xy_height":0.20,"xy_width":0.20,"xyts_space":0.05,"xyts_space":0.05,
           "ts_width":0.9}
     # set the keyword arguments
-    for key, value in kwargs.iteritems():
+    for key, value in kwargs.items():
         pd[key] = value
     # calculate bottom of the first time series and the height of the time series plots
     pd["ts_bottom"] = pd["margin_bottom"]+pd["xy_height"]+pd["xyts_space"]
@@ -545,7 +545,7 @@
     ts_axes[0].text(0.05, 0.85, TextStr, color='b', horizontalalignment='left', transform=ts_axes[0].transAxes)
     TextStr = output + '(' + ds.series[output]['Attr']['units'] + ')'
     ts_axes[0].text(0.85, 0.85, TextStr, color='r', horizontalalignment='right', transform=ts_axes[0].transAxes)
-    for ThisOne, i in zip(drivers, range(1, pd["nDrivers"] + 1)):
+    for ThisOne, i in zip(drivers, list(range(1, pd["nDrivers"] + 1))):
         this_bottom = pd["ts_bottom"] + i*pd["ts_height"]
         rect = [pd["margin_left"], this_bottom, pd["ts_width"], pd["ts_height"]]
         ts_axes.append(plt.axes(rect, sharex=ts_axes[0]))
@@ -589,7 +589,7 @@
 def subset_window(data_dict, index_list):
     # Subset the arrays on basis of index list
     sub_dict = {}
-    for i in data_dict.keys():
+    for i in list(data_dict.keys()):
         sub_dict[i] = data_dict[i][index_list[0]: index_list[1] + 1]
 
     return sub_dict
RefactoringTool: Refactored ./scripts/pfp_top_level.py
--- ./scripts/pfp_top_level.py	(original)
+++ ./scripts/pfp_top_level.py	(refactored)
@@ -300,9 +300,9 @@
         pfp_io.nc_write_series(nc_file, ds2)
         logger.info("Finished L2 processing")
         logger.info("Plotting L1 and L2 data")
-        for nFig in cfg['Plots'].keys():
+        for nFig in list(cfg['Plots'].keys()):
             plt_cf = cfg['Plots'][str(nFig)]
-            if 'type' in plt_cf.keys():
+            if 'type' in list(plt_cf.keys()):
                 if str(plt_cf['type']).lower() =='xy':
                     pfp_plot.plotxy(cfg, nFig, plt_cf, ds1, ds2)
                 else:
@@ -348,9 +348,9 @@
         pfp_io.nc_write_series(nc_file, ds3)
         logger.info("Finished L3 processing")
         logger.info("Plotting L3 data")
-        for nFig in cfg['Plots'].keys():
+        for nFig in list(cfg['Plots'].keys()):
             plt_cf = cfg['Plots'][str(nFig)]
-            if 'type' in plt_cf.keys():
+            if 'type' in list(plt_cf.keys()):
                 if str(plt_cf['type']).lower() =='xy':
                     pfp_plot.plotxy(cfg, nFig, plt_cf, ds2, ds3)
                 else:
RefactoringTool: Refactored ./scripts/pfp_ts.py
--- ./scripts/pfp_ts.py	(original)
+++ ./scripts/pfp_ts.py	(refactored)
@@ -39,8 +39,8 @@
     nRecs = int(ds.globalattributes["nc_nrecs"])
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
-    if 'albedo' not in ds.series.keys():
-        if 'Fsd' in ds.series.keys() and 'Fsu' in ds.series.keys():
+    if 'albedo' not in list(ds.series.keys()):
+        if 'Fsd' in list(ds.series.keys()) and 'Fsu' in list(ds.series.keys()):
             Fsd,f,a = pfp_utils.GetSeriesasMA(ds,'Fsd')
             Fsu,f,a = pfp_utils.GetSeriesasMA(ds,'Fsu')
             albedo = Fsu / Fsd
@@ -52,7 +52,7 @@
             return
     else:
         albedo,f,a = pfp_utils.GetSeriesasMA(ds,'albedo')
-        if 'Fsd' in ds.series.keys():
+        if 'Fsd' in list(ds.series.keys()):
             Fsd,f,a = pfp_utils.GetSeriesasMA(ds,'Fsd')
         else:
             Fsd,f,a = pfp_utils.GetSeriesasMA(ds,'Fn')
@@ -81,13 +81,13 @@
         ds: data structure
         x: input/output variable in ds.  Example: 'Cc_7500_Av'
         """
-    if ThisOne not in ds.series.keys(): return
+    if ThisOne not in list(ds.series.keys()): return
     if pfp_utils.incf(cf,ThisOne) and pfp_utils.haskey(cf,ThisOne,'Linear'):
         logger.info('  Applying linear correction to '+ThisOne)
         data = numpy.ma.masked_where(ds.series[ThisOne]['Data']==float(c.missing_value),ds.series[ThisOne]['Data'])
         flag = ds.series[ThisOne]['Flag'].copy()
         ldt = ds.series['DateTime']['Data']
-        LinearList = cf['Variables'][ThisOne]['Linear'].keys()
+        LinearList = list(cf['Variables'][ThisOne]['Linear'].keys())
         for i in range(len(LinearList)):
             linear_dates_string = cf['Variables'][ThisOne]['Linear'][str(i)]
             linear_dates_list = linear_dates_string.split(",")
@@ -123,13 +123,13 @@
         ds: data structure
         x: input/output variable in ds.  Example: 'Cc_7500_Av'
         """
-    if ThisOne not in ds.series.keys(): return
+    if ThisOne not in list(ds.series.keys()): return
     if pfp_utils.incf(cf,ThisOne) and pfp_utils.haskey(cf,ThisOne,'Drift'):
         logger.info('  Applying linear drift correction to '+ThisOne)
         data = numpy.ma.masked_where(ds.series[ThisOne]['Data']==float(c.missing_value),ds.series[ThisOne]['Data'])
         flag = ds.series[ThisOne]['Flag']
         ldt = ds.series['DateTime']['Data']
-        DriftList = cf['Variables'][ThisOne]['Drift'].keys()
+        DriftList = list(cf['Variables'][ThisOne]['Drift'].keys())
         for i in range(len(DriftList)):
             DriftItemList = ast.literal_eval(cf['Variables'][ThisOne]['Drift'][str(i)])
             try:
@@ -169,13 +169,13 @@
         ds: data structure
         x: input/output variable in ds.  Example: 'Cc_7500_Av'
         """
-    if ThisOne not in ds.series.keys(): return
+    if ThisOne not in list(ds.series.keys()): return
     if pfp_utils.incf(cf,ThisOne) and pfp_utils.haskey(cf,ThisOne,'LocalDrift'):
         logger.info('  Applying linear drift correction to '+ThisOne)
         data = numpy.ma.masked_where(ds.series[ThisOne]['Data']==float(c.missing_value),ds.series[ThisOne]['Data'])
         flag = ds.series[ThisOne]['Flag']
         ldt = ds.series['DateTime']['Data']
-        DriftList = cf['Variables'][ThisOne]['LocalDrift'].keys()
+        DriftList = list(cf['Variables'][ThisOne]['LocalDrift'].keys())
         for i in range(len(DriftList)):
             DriftItemList = ast.literal_eval(cf['Variables'][ThisOne]['LocalDrift'][str(i)])
             try:
@@ -213,7 +213,7 @@
         Av_out: output variable to ds.  Example: 'Fg'
         Series_in: input variable series in ds.  Example: ['Fg_8cma','Fg_8cmb']
         """
-    if Av_out not in cf['Variables'].keys(): return
+    if Av_out not in list(cf['Variables'].keys()): return
     if Av_out in ds.averageserieslist: return
     srclist = pfp_utils.GetAverageSeriesKeys(cf,Av_out)
     logger.info(' Averaging '+str(srclist)+'==>'+Av_out)
@@ -264,10 +264,10 @@
         Fg_in: input ground heat flux in ds.  Example: 'Fg'
         """
     logger.info(' Calculating available energy from Fn and Fg')
-    if Fn_in not in ds.series.keys():
+    if Fn_in not in list(ds.series.keys()):
         logger.warning(" Series "+Fn_in+" not found in data file")
         return
-    if Fg_in not in ds.series.keys():
+    if Fg_in not in list(ds.series.keys()):
         logger.warning(" Series "+Fg_in+" not found in data file")
         return
     Fn,Fn_flag,a = pfp_utils.GetSeriesasMA(ds,Fn_in)
@@ -276,7 +276,7 @@
     Fa_calc_flag = numpy.zeros(len(Fa_calc),dtype=numpy.int32)
     idx = numpy.where((numpy.ma.getmaskarray(Fn)==True)|(numpy.ma.getmaskarray(Fg)==True))[0]
     Fa_calc_flag[idx] = numpy.int32(1)
-    if Fa_out not in ds.series.keys():
+    if Fa_out not in list(ds.series.keys()):
         attr = pfp_utils.MakeAttributeDictionary(long_name='Available energy using '+Fn_in+','+Fg_in,units='W/m2')
         pfp_utils.CreateSeries(ds,Fa_out,Fa_calc,Fa_calc_flag,attr)
     else:
@@ -311,7 +311,7 @@
     Lv = pfp_utils.GetVariable(ds, "Lv")
 
     logger.info(" Calculating fluxes from covariances")
-    if "wT" in ds.series.keys():
+    if "wT" in list(ds.series.keys()):
         ok_units = ["mC/s", "Cm/s"]
         wT = pfp_utils.GetVariable(ds, "wT")
         if wT["Attr"]["units"] in ok_units:
@@ -327,7 +327,7 @@
             logger.error(" CalculateFluxes: Incorrect units for wT, Fhv not calculated")
     else:
         logger.error("  CalculateFluxes: wT not found, Fhv not calculated")
-    if "wA" in ds.series.keys():
+    if "wA" in list(ds.series.keys()):
         wA = pfp_utils.GetVariable(ds, "wA")
         if wA["Attr"]["units"] == "g/m2/s":
             Fe = Lv["Data"]*wA["Data"]/float(1000)
@@ -342,7 +342,7 @@
             logger.error(" CalculateFluxes: Incorrect units for wA, Fe not calculated")
     else:
         logger.error("  CalculateFluxes: wA not found, Fe not calculated")
-    if "wC" in ds.series.keys():
+    if "wC" in list(ds.series.keys()):
         wC = pfp_utils.GetVariable(ds, "wC")
         if wC["Attr"]["units"] == "mg/m2/s":
             Fc = wC["Data"]
@@ -357,8 +357,8 @@
             logger.error(" CalculateFluxes: Incorrect units for wC, Fc not calculated")
     else:
         logger.error("  CalculateFluxes: wC not found, Fc not calculated")
-    if "uw" in ds.series.keys():
-        if "vw" in ds.series.keys():
+    if "uw" in list(ds.series.keys()):
+        if "vw" in list(ds.series.keys()):
             uw = pfp_utils.GetVariable(ds, "uw")
             vw = pfp_utils.GetVariable(ds, "vw")
             vs = uw["Data"]*uw["Data"] + vw["Data"]*vw["Data"]
@@ -426,20 +426,20 @@
      March 2015
     Author: PRI
     """
-    if "Ah" not in ds.series.keys():
-        if "SH" in ds.series.keys():
+    if "Ah" not in list(ds.series.keys()):
+        if "SH" in list(ds.series.keys()):
             AbsoluteHumidityFromq(ds)    # calculate Ah from q
-        elif "RH" in ds.series.keys():
+        elif "RH" in list(ds.series.keys()):
             AbsoluteHumidityFromRH(ds)   # calculate Ah from RH
-    if "SH" not in ds.series.keys():
-        if "Ah" in ds.series.keys():
+    if "SH" not in list(ds.series.keys()):
+        if "Ah" in list(ds.series.keys()):
             SpecificHumidityFromAh(ds)
-        elif "RH" in ds.series.keys():
+        elif "RH" in list(ds.series.keys()):
             SpecificHumidityFromRH(ds)
-    if "RH" not in ds.series.keys():
-        if "Ah" in ds.series.keys():
+    if "RH" not in list(ds.series.keys()):
+        if "Ah" in list(ds.series.keys()):
             RelativeHumidityFromAh(ds)
-        elif "SH" in ds.series.keys():
+        elif "SH" in list(ds.series.keys()):
             RelativeHumidityFromq(ds)
 
 def CalculateHumiditiesAfterGapFill(ds, info):
@@ -457,17 +457,17 @@
     # create an empty list
     alt_list = []
     # check to see if there was any gap filling using data from alternate sources
-    if "GapFillFromAlternate" in info.keys():
+    if "GapFillFromAlternate" in list(info.keys()):
         ia = info["GapFillFromAlternate"]
         # if so, get a list of the quantities gap filled from alternate sources
-        alt_list = list(set([ia["outputs"][item]["target"] for item in ia["outputs"].keys()]))
+        alt_list = list(set([ia["outputs"][item]["target"] for item in list(ia["outputs"].keys())]))
     # create an empty list
     cli_list = []
     # check to see if there was any gap filling from climatology
-    if "GapFillFromClimatology" in info.keys():
+    if "GapFillFromClimatology" in list(info.keys()):
         ic = info["GapFillFromClimatology"]
         # if so, get a list of the quantities gap filled using climatology
-        cli_list = list(set([ic["outputs"][item]["target"] for item in ic["outputs"].keys()]))
+        cli_list = list(set([ic["outputs"][item]["target"] for item in list(ic["outputs"].keys())]))
     # one list to rule them, one list to bind them ...
     gf_list = list(set(alt_list+cli_list))
     # clear out if there was no gap filling
@@ -496,7 +496,7 @@
     RH,RH_flag,a = pfp_utils.GetSeriesasMA(ds,"RH")
     Ah_new_flag = pfp_utils.MergeQCFlag([Ta_flag,RH_flag])
     Ah_new = pfp_mf.absolutehumidityfromRH(Ta,RH)
-    if "Ah" in ds.series.keys():
+    if "Ah" in list(ds.series.keys()):
         Ah,Ah_flag,Ah_attr = pfp_utils.GetSeriesasMA(ds,"Ah")
         index = numpy.where(numpy.ma.getmaskarray(Ah)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(Ah)==True)[0]
@@ -523,7 +523,7 @@
     Ah_new_flag = pfp_utils.MergeQCFlag([Ta_flag,ps_flag,q_flag])
     RH = pfp_mf.RHfromspecifichumidity(q,Ta,ps)
     Ah_new = pfp_mf.absolutehumidityfromRH(Ta,RH)
-    if "Ah" in ds.series.keys():
+    if "Ah" in list(ds.series.keys()):
         Ah,Ah_flag,Ah_attr = pfp_utils.GetSeriesasMA(ds,"Ah")
         index = numpy.where(numpy.ma.getmaskarray(Ah)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(Ah)==True)[0]
@@ -549,7 +549,7 @@
     q, q_flag, a = pfp_utils.GetSeriesasMA(ds, "SH")
     RH_new_flag = pfp_utils.MergeQCFlag([Ta_flag,ps_flag,q_flag])
     RH_new = pfp_mf.RHfromspecifichumidity(q,Ta,ps)
-    if "RH" in ds.series.keys():
+    if "RH" in list(ds.series.keys()):
         RH,RH_flag,RH_attr = pfp_utils.GetSeriesasMA(ds,"RH")
         index = numpy.where(numpy.ma.getmaskarray(RH)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(RH)==True)[0]
@@ -574,7 +574,7 @@
     Ah,Ah_flag,a = pfp_utils.GetSeriesasMA(ds,"Ah")
     RH_new_flag = pfp_utils.MergeQCFlag([Ta_flag,Ah_flag])
     RH_new = pfp_mf.RHfromabsolutehumidity(Ah,Ta)     # relative humidity in units of percent
-    if "RH" in ds.series.keys():
+    if "RH" in list(ds.series.keys()):
         RH,RH_flag,RH_attr = pfp_utils.GetSeriesasMA(ds,"RH")
         index = numpy.where(numpy.ma.getmaskarray(RH)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(RH)==True)[0]
@@ -621,13 +621,13 @@
         Lifted from scipy Cookbook (http://wiki.scipy.org/Cookbook/SignalSmooth)
     """
     if x.ndim != 1:
-        raise ValueError, "smooth only accepts 1 dimension arrays."
+        raise ValueError("smooth only accepts 1 dimension arrays.")
     if x.size < window_len:
-        raise ValueError, "Input vector needs to be bigger than window size."
+        raise ValueError("Input vector needs to be bigger than window size.")
     if window_len<3:
         return x
     if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:
-        raise ValueError, "Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'"
+        raise ValueError("Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'")
     s=numpy.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]
     if window == 'flat': #moving average
         w=numpy.ones(window_len,'d')
@@ -647,7 +647,7 @@
     q_new_flag = pfp_utils.MergeQCFlag([Ta_flag,ps_flag,Ah_flag])
     RH = pfp_mf.RHfromabsolutehumidity(Ah,Ta)
     q_new = pfp_mf.specifichumidityfromRH(RH, Ta, ps)
-    if "SH" in ds.series.keys():
+    if "SH" in list(ds.series.keys()):
         q, q_flag, q_attr = pfp_utils.GetSeriesasMA(ds, "SH")
         index = numpy.where(numpy.ma.getmaskarray(q)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(q)==True)[0]
@@ -673,7 +673,7 @@
     RH,RH_flag,a = pfp_utils.GetSeriesasMA(ds,"RH")
     q_new_flag = pfp_utils.MergeQCFlag([Ta_flag,ps_flag,RH_flag])
     q_new = pfp_mf.specifichumidityfromRH(RH,Ta,ps)   # specific humidity in units of kg/kg
-    if "SH" in ds.series.keys():
+    if "SH" in list(ds.series.keys()):
         q, q_flag, q_attr = pfp_utils.GetSeriesasMA(ds, "SH")
         index = numpy.where(numpy.ma.getmaskarray(q)==True)[0]
         #index = numpy.ma.where(numpy.ma.getmaskarray(q)==True)[0]
@@ -718,7 +718,7 @@
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     for item in [Ta_name,ps_name,Ah_name,q_name]:
-        if item not in ds.series.keys():
+        if item not in list(ds.series.keys()):
             msg = " CalculateMeteorologicalVariables: series "
             msg = msg + item + " not found, returning ..."
             logger.warning(msg)
@@ -728,10 +728,10 @@
     # get the required data series
     Ta,f,a = pfp_utils.GetSeriesasMA(ds,Ta_name)
     # deal with possible aliases for the sonic temperature for the time being
-    if Tv_name not in ds.series.keys():
-        if "Tv_CSAT_Av" in ds.series.keys():
+    if Tv_name not in list(ds.series.keys()):
+        if "Tv_CSAT_Av" in list(ds.series.keys()):
             Tv_name = "Tv_CSAT_Av"
-        elif "Tv_CSAT" in ds.series.keys():
+        elif "Tv_CSAT" in list(ds.series.keys()):
             Tv_name = "Tv_CSAT"
         else:
             Tv_name = Ta_name   # use Tv_CSAT if it is in the data structure, otherwise use Ta
@@ -894,13 +894,13 @@
     nRecs = int(ds.globalattributes["nc_nrecs"])
     zeros = numpy.zeros(nRecs,dtype=numpy.int32)
     ones = numpy.ones(nRecs,dtype=numpy.int32)
-    if Fsd_in in ds.series.keys() and Fsu_in in ds.series.keys() and Fld_in in ds.series.keys() and Flu_in in ds.series.keys():
+    if Fsd_in in list(ds.series.keys()) and Fsu_in in list(ds.series.keys()) and Fld_in in list(ds.series.keys()) and Flu_in in list(ds.series.keys()):
         Fsd,f,a = pfp_utils.GetSeriesasMA(ds,Fsd_in)
         Fsu,f,a = pfp_utils.GetSeriesasMA(ds,Fsu_in)
         Fld,f,a = pfp_utils.GetSeriesasMA(ds,Fld_in)
         Flu,f,a = pfp_utils.GetSeriesasMA(ds,Flu_in)
         Fn_calc = (Fsd - Fsu) + (Fld - Flu)
-        if Fn_out not in ds.series.keys():
+        if Fn_out not in list(ds.series.keys()):
             attr = pfp_utils.MakeAttributeDictionary(long_name='Calculated net radiation using '+Fsd_in+','+Fsu_in+','+Fld_in+','+Flu_in,
                                  standard_name='surface_net_downwawrd_radiative_flux',units='W/m2')
             flag = numpy.where(numpy.ma.getmaskarray(Fn_calc)==True,ones,zeros)
@@ -931,7 +931,7 @@
     co2_list = ["UxC","UyC","UzC"]
     h2o_list = ["UxA","UyA","UzA","UxH","UyH","UzH"]
     for item in co2_list:
-        if item not in ds.series.keys(): continue
+        if item not in list(ds.series.keys()): continue
         data,flag,attr = pfp_utils.GetSeriesasMA(ds,item)
         if "umol" in attr["units"]:
             Ta,f,a = pfp_utils.GetSeriesasMA(ds,"Ta")
@@ -940,7 +940,7 @@
             attr["units"] = "mg/m2/s"
             pfp_utils.CreateSeries(ds,item,data,flag,attr)
     for item in h2o_list:
-        if item not in ds.series.keys(): continue
+        if item not in list(ds.series.keys()): continue
         data,flag,attr = pfp_utils.GetSeriesasMA(ds,item)
         if "mmol" in attr["units"]:
             Ta,f,a = pfp_utils.GetSeriesasMA(ds,"Ta")
@@ -1019,7 +1019,7 @@
     UyT = pfp_utils.GetVariable(ds, "UyT")        # covariance(Uy,T)
     # apply 2D coordinate rotation unless otherwise specified in control file
     rotate = True
-    if ("Options" in cf) and ("2DCoordRotation" in cf["Options"].keys()):
+    if ("Options" in cf) and ("2DCoordRotation" in list(cf["Options"].keys())):
         if not cf["Options"].as_bool("2DCoordRotation"):
             rotate = False
     if rotate:
@@ -1190,7 +1190,7 @@
     Parameters loaded from control file:
         zms: measurement height from surface, m
     """
-    if Fc_out not in ds.series.keys():
+    if Fc_out not in list(ds.series.keys()):
         logger.info(" Calculating Fc storage (single height)")
         nRecs = int(ds.globalattributes["nc_nrecs"])
         zeros = numpy.zeros(nRecs, dtype=numpy.int32)
@@ -1202,8 +1202,8 @@
         ldt = pfp_utils.GetVariable(ds, "DateTime")
         Fc_single = pfp_utils.CreateEmptyVariable(Fc_out, nRecs, datetime=ldt["Data"])
         # get the input data
-        if CO2_in not in ds.series.keys():
-            if "Cc" in ds.series.keys():
+        if CO2_in not in list(ds.series.keys()):
+            if "Cc" in list(ds.series.keys()):
                 CO2_in = "Cc"
             else:
                 msg = "  Neither CO2 nor Cc not in data structure, storage not calculated"
@@ -1228,7 +1228,7 @@
                 got_zms = True
             except:
                 pass
-        if "tower_height" in ds.globalattributes.keys() and not got_zms:
+        if "tower_height" in list(ds.globalattributes.keys()) and not got_zms:
             try:
                 zms = float(pfp_utils.strip_non_numeric(ds.globalattributes["tower_height"]))
                 got_zms = True
@@ -1304,24 +1304,24 @@
     # check to see if applying the Fc storage term has been requested for any
     # individual variables
     apply_storage = {}
-    for label in cf["Variables"].keys():
+    for label in list(cf["Variables"].keys()):
         if "ApplyFcStorage" in cf["Variables"][label]:
             source = str(cf["Variables"][label]["ApplyFcStorage"]["source"])
             apply_storage[label] = source
     # if no individual series have been specified, do the default
-    if len(apply_storage.keys()) == 0:
+    if len(list(apply_storage.keys())) == 0:
         # check to see if correction for storage has been requested in [Options]
         if not pfp_utils.get_optionskeyaslogical(cf, "ApplyFcStorage"):
             return
         # check to see if we have the required data series
-        if (Fc_in not in ds.series.keys()) or (Fc_storage_in not in ds.series.keys()):
+        if (Fc_in not in list(ds.series.keys())) or (Fc_storage_in not in list(ds.series.keys())):
             msg = "CorrectFcForStorage: Fc or Fc_storage not found, skipping ..."
             logger.warning(msg)
             return
         # check to see if we have an Fc_profile series
-        if "Fc_profile" in ds.series.keys():
+        if "Fc_profile" in list(ds.series.keys()):
             Fc_storage_in = "Fc_profile"
-        elif "Fc_storage" in ds.series.keys():
+        elif "Fc_storage" in list(ds.series.keys()):
             Fc_storage_in = "Fc_storage"
         logger.info(" ***!!! Applying Fc storage term !!!***")
         Fc_raw,Fc_flag,Fc_attr = pfp_utils.GetSeriesasMA(ds,Fc_in)
@@ -1341,16 +1341,16 @@
         pfp_utils.CreateSeries(ds,Fc_out,Fc,flag,Fc_attr)
     else:
         # loop over the series for which apply Fc storage was requested
-        for label in apply_storage.keys():
+        for label in list(apply_storage.keys()):
             # check to make sure the requested series is in the data structure
-            if label not in ds.series.keys():
+            if label not in list(ds.series.keys()):
                 # skip if it isn't
                 msg = " Requested series "+label+" not found in data structure"
                 logger.error(msg)
                 continue
             # get the storage flux label
             source = apply_storage[label]
-            if source not in ds.series.keys():
+            if source not in list(ds.series.keys()):
                 msg = " Requested series "+source+" not found in data structure"
                 logger.error(msg)
                 continue
@@ -1387,7 +1387,7 @@
 
 def CorrectIndividualFgForStorage(cf,ds):
     if pfp_utils.cfkeycheck(cf,Base='FunctionArgs',ThisOne='CFgArgs'):
-        List = cf['FunctionArgs']['CFgArgs'].keys()
+        List = list(cf['FunctionArgs']['CFgArgs'].keys())
         for i in range(len(List)):
             CFgArgs = ast.literal_eval(cf['FunctionArgs']['CFgArgs'][str(i)])
             CorrectFgForStorage(cf,ds,Fg_out=CFgArgs[0],Fg_in=CFgArgs[1],Ts_in=CFgArgs[2],Sws_in=CFgArgs[3])
@@ -1417,9 +1417,9 @@
         logger.info(' CorrectFgForStorage: storage correction disabled in control file')
         return
     # check to see if there is a [Soil] section in the control file
-    if 'Soil' not in cf.keys():
+    if 'Soil' not in list(cf.keys()):
         # if there isn't, check to see if the soil information is in the netCDF global attributes
-        if "FgDepth" in ds.globalattributes.keys():
+        if "FgDepth" in list(ds.globalattributes.keys()):
             # if it is, read it into the control file object so we can use it later
             cf["Soil"] = {}
             cf["Soil"]["FgDepth"] = ds.globalattributes["FgDepth"]
@@ -1430,12 +1430,12 @@
             # tell the user if we can't find the information needed
             logger.warning(' CorrectFgForStorage: [Soil] section not found in control file or global attributes, Fg not corrected')
             return
-    if Fg_in not in ds.series.keys() or Ts_in not in ds.series.keys():
+    if Fg_in not in list(ds.series.keys()) or Ts_in not in list(ds.series.keys()):
         logger.warning(' CorrectFgForStorage: '+Fg_in+' or '+Ts_in+' not found in data structure, Fg not corrected')
         return
     logger.info(' Correcting soil heat flux for storage')
     # put the contents of the soil section into the global attributes
-    for item in cf["Soil"].keys(): ds.globalattributes[item] = cf["Soil"][item]
+    for item in list(cf["Soil"].keys()): ds.globalattributes[item] = cf["Soil"][item]
     d = max(0.0,min(0.5,float(cf['Soil']['FgDepth'])))
     bd = max(1200.0,min(2500.0,float(cf['Soil']['BulkDensity'])))
     oc = max(0.0,min(1.0,float(cf['Soil']['OrganicContent'])))
@@ -1618,7 +1618,7 @@
     ts = int(ds.globalattributes["time_step"])
     Wd,f,a = pfp_utils.GetSeriesasMA(ds,Wd_in)
     ldt = ds.series["DateTime"]["Data"]
-    KeyList = cf["Variables"][Wd_in]["CorrectWindDirection"].keys()
+    KeyList = list(cf["Variables"][Wd_in]["CorrectWindDirection"].keys())
     for i in range(len(KeyList)):
         correct_wd_string = cf["Variables"][Wd_in]["CorrectWindDirection"][str(i)]
         correct_wd_list = correct_wd_string.split(",")
@@ -1658,8 +1658,8 @@
         ds: data structure
         """
     logger.info(' Getting the attributes given in control file')
-    if 'Global' in cf.keys():
-        for gattr in cf['Global'].keys():
+    if 'Global' in list(cf.keys()):
+        for gattr in list(cf['Global'].keys()):
             ds.globalattributes[gattr] = cf['Global'][gattr]
         ds.globalattributes['Flag00'] = 'Good data'
         ds.globalattributes['Flag10'] = 'Corrections: Apply Linear'
@@ -1701,13 +1701,13 @@
         #ds.globalattributes['Flag80'] = 'Partitioning Day: GPP/Re computed from light-response curves, GPP = Re - Fc'
         #ds.globalattributes['Flag81'] = 'Partitioning Day: GPP night mask'
         #ds.globalattributes['Flag82'] = 'Partitioning Day: Fc > Re, GPP = 0, Re = Fc'
-    for ThisOne in ds.series.keys():
+    for ThisOne in list(ds.series.keys()):
         if ThisOne in cf['Variables']:
-            if 'Attr' in cf['Variables'][ThisOne].keys():
+            if 'Attr' in list(cf['Variables'][ThisOne].keys()):
                 ds.series[ThisOne]['Attr'] = {}
-                for attr in cf['Variables'][ThisOne]['Attr'].keys():
+                for attr in list(cf['Variables'][ThisOne]['Attr'].keys()):
                     ds.series[ThisOne]['Attr'][attr] = cf['Variables'][ThisOne]['Attr'][attr]
-                if "missing_value" not in ds.series[ThisOne]['Attr'].keys():
+                if "missing_value" not in list(ds.series[ThisOne]['Attr'].keys()):
                     ds.series[ThisOne]['Attr']["missing_value"] = numpy.int32(c.missing_value)
 
 def DoFunctions(ds, info):
@@ -1722,11 +1722,11 @@
     functions = {}
     convert_vars = []
     function_vars = []
-    for var in info["Variables"].keys():
+    for var in list(info["Variables"].keys()):
         # datetime functions handled elsewhere for now
         if var == "DateTime": continue
-        if "Function" not in info["Variables"][var].keys(): continue
-        if "func" not in info["Variables"][var]["Function"].keys():
+        if "Function" not in list(info["Variables"][var].keys()): continue
+        if "func" not in list(info["Variables"][var]["Function"].keys()):
             msg = " DoFunctions: 'func' keyword not found in [Functions] for "+var
             logger.error(msg)
             continue
@@ -1757,82 +1757,82 @@
 
 def CalculateStandardDeviations(ds):
     logger.info(' Getting variances from standard deviations & vice versa')
-    if 'AhAh' in ds.series.keys() and 'Ah_7500_Sd' not in ds.series.keys():
+    if 'AhAh' in list(ds.series.keys()) and 'Ah_7500_Sd' not in list(ds.series.keys()):
         AhAh,flag,attr = pfp_utils.GetSeriesasMA(ds,'AhAh')
         Ah_7500_Sd = numpy.ma.sqrt(AhAh)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, standard deviation',units='g/m3')
         pfp_utils.CreateSeries(ds,'Ah_7500_Sd',Ah_7500_Sd,flag,attr)
-    if 'Ah_7500_Sd' in ds.series.keys() and 'AhAh' not in ds.series.keys():
+    if 'Ah_7500_Sd' in list(ds.series.keys()) and 'AhAh' not in list(ds.series.keys()):
         Ah_7500_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Ah_7500_Sd')
         AhAh = Ah_7500_Sd*Ah_7500_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, variance',units='(g/m3)2')
         pfp_utils.CreateSeries(ds,'AhAh',AhAh,flag,attr)
-    if 'Ah_IRGA_Vr' in ds.series.keys() and 'Ah_IRGA_Sd' not in ds.series.keys():
+    if 'Ah_IRGA_Vr' in list(ds.series.keys()) and 'Ah_IRGA_Sd' not in list(ds.series.keys()):
         Ah_IRGA_Vr,flag,attr = pfp_utils.GetSeriesasMA(ds,'Ah_IRGA_Vr')
         Ah_IRGA_Sd = numpy.ma.sqrt(Ah_IRGA_Vr)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, standard deviation',units='g/m3')
         pfp_utils.CreateSeries(ds,'Ah_IRGA_Sd',Ah_IRGA_Sd,flag,attr)
-    if 'Ah_IRGA_Sd' in ds.series.keys() and 'Ah_IRGA_Vr' not in ds.series.keys():
+    if 'Ah_IRGA_Sd' in list(ds.series.keys()) and 'Ah_IRGA_Vr' not in list(ds.series.keys()):
         Ah_IRGA_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Ah_IRGA_Sd')
         Ah_IRGA_Vr = Ah_IRGA_Sd*Ah_IRGA_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, variance',units='(g/m3)2')
         pfp_utils.CreateSeries(ds,'Ah_IRGA_Vr',Ah_IRGA_Vr,flag,attr)
-    if 'H2O_IRGA_Vr' in ds.series.keys() and 'H2O_IRGA_Sd' not in ds.series.keys():
+    if 'H2O_IRGA_Vr' in list(ds.series.keys()) and 'H2O_IRGA_Sd' not in list(ds.series.keys()):
         H2O_IRGA_Vr,flag,attr = pfp_utils.GetSeriesasMA(ds,'H2O_IRGA_Vr')
         H2O_IRGA_Sd = numpy.ma.sqrt(H2O_IRGA_Vr)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, standard deviation',units='g/m3')
         pfp_utils.CreateSeries(ds,'H2O_IRGA_Sd',H2O_IRGA_Sd,flag,attr)
-    if 'H2O_IRGA_Sd' in ds.series.keys() and 'H2O_IRGA_Vr' not in ds.series.keys():
+    if 'H2O_IRGA_Sd' in list(ds.series.keys()) and 'H2O_IRGA_Vr' not in list(ds.series.keys()):
         H2O_IRGA_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'H2O_IRGA_Sd')
         H2O_IRGA_Vr = H2O_IRGA_Sd*H2O_IRGA_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Absolute humidity from IRGA, variance',units='(g/m3)2')
         pfp_utils.CreateSeries(ds,'H2O_IRGA_Vr',H2O_IRGA_Vr,flag,attr)
-    if 'CcCc' in ds.series.keys() and 'Cc_7500_Sd' not in ds.series.keys():
+    if 'CcCc' in list(ds.series.keys()) and 'Cc_7500_Sd' not in list(ds.series.keys()):
         CcCc,flag,attr = pfp_utils.GetSeriesasMA(ds,'CcCc')
         Cc_7500_Sd = numpy.ma.sqrt(CcCc)
         attr = pfp_utils.MakeAttributeDictionary(long_name='CO2 concentration from IRGA, standard deviation',units='mg/m3')
         pfp_utils.CreateSeries(ds,'Cc_7500_Sd',Cc_7500_Sd,flag,attr)
-    if 'CO2_IRGA_Sd' in ds.series.keys() and 'CO2_IRGA_Vr' not in ds.series.keys():
+    if 'CO2_IRGA_Sd' in list(ds.series.keys()) and 'CO2_IRGA_Vr' not in list(ds.series.keys()):
         CO2_IRGA_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'CO2_IRGA_Sd')
         CO2_IRGA_Vr = CO2_IRGA_Sd*CO2_IRGA_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='CO2 concentration from IRGA, variance',units='(mg/m3)2')
         pfp_utils.CreateSeries(ds,'CO2_IRGA_Vr',CO2_IRGA_Vr,flag,attr)
-    if 'Cc_7500_Sd' in ds.series.keys() and 'CcCc' not in ds.series.keys():
+    if 'Cc_7500_Sd' in list(ds.series.keys()) and 'CcCc' not in list(ds.series.keys()):
         Cc_7500_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Cc_7500_Sd')
         CcCc = Cc_7500_Sd*Cc_7500_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='CO2 concentration from IRGA, variance',units='(mg/m3)2')
         pfp_utils.CreateSeries(ds,'CcCc',CcCc,flag,attr)
-    if 'CO2_IRGA_Vr' in ds.series.keys() and 'CO2_IRGA_Sd' not in ds.series.keys():
+    if 'CO2_IRGA_Vr' in list(ds.series.keys()) and 'CO2_IRGA_Sd' not in list(ds.series.keys()):
         CO2_IRGA_Vr,flag,attr = pfp_utils.GetSeriesasMA(ds,'CO2_IRGA_Vr')
         CO2_IRGA_Sd = numpy.ma.sqrt(CO2_IRGA_Vr)
         attr = pfp_utils.MakeAttributeDictionary(long_name='CO2 concentration from IRGA, standard deviation',units='mg/m3')
         pfp_utils.CreateSeries(ds,'CO2_IRGA_Sd',CO2_IRGA_Sd,flag,attr)
-    if 'Ux_Sd' in ds.series.keys() and 'UxUx' not in ds.series.keys():
+    if 'Ux_Sd' in list(ds.series.keys()) and 'UxUx' not in list(ds.series.keys()):
         Ux_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Ux_Sd')
         UxUx = Ux_Sd*Ux_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Longitudinal velocity component from CSAT, variance',units='(m/s)2')
         pfp_utils.CreateSeries(ds,'UxUx',UxUx,flag,attr)
-    if 'UxUx' in ds.series.keys() and 'Ux_Sd' not in ds.series.keys():
+    if 'UxUx' in list(ds.series.keys()) and 'Ux_Sd' not in list(ds.series.keys()):
         UxUx,flag,attr = pfp_utils.GetSeriesasMA(ds,'UxUx')
         Ux_Sd = numpy.ma.sqrt(UxUx)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Longitudinal velocity component from CSAT, standard deviation',units='m/s')
         pfp_utils.CreateSeries(ds,'Ux_Sd',Ux_Sd,flag,attr)
-    if 'Uy_Sd' in ds.series.keys() and 'UyUy' not in ds.series.keys():
+    if 'Uy_Sd' in list(ds.series.keys()) and 'UyUy' not in list(ds.series.keys()):
         Uy_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Uy_Sd')
         UyUy = Uy_Sd*Uy_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Lateral velocity component from CSAT, variance',units='(m/s)2')
         pfp_utils.CreateSeries(ds,'UyUy',UyUy,flag,attr)
-    if 'UyUy' in ds.series.keys() and 'Uy_Sd' not in ds.series.keys():
+    if 'UyUy' in list(ds.series.keys()) and 'Uy_Sd' not in list(ds.series.keys()):
         UyUy,flag,attr = pfp_utils.GetSeriesasMA(ds,'UyUy')
         Uy_Sd = numpy.ma.sqrt(UyUy)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Lateral velocity component from CSAT, standard deviation',units='m/s')
         pfp_utils.CreateSeries(ds,'Uy_Sd',Uy_Sd,flag,attr)
-    if 'Uz_Sd' in ds.series.keys() and 'UzUz' not in ds.series.keys():
+    if 'Uz_Sd' in list(ds.series.keys()) and 'UzUz' not in list(ds.series.keys()):
         Uz_Sd,flag,attr = pfp_utils.GetSeriesasMA(ds,'Uz_Sd')
         UzUz = Uz_Sd*Uz_Sd
         attr = pfp_utils.MakeAttributeDictionary(long_name='Vertical velocity component from CSAT, variance',units='(m/s)2')
         pfp_utils.CreateSeries(ds,'UzUz',UzUz,flag,attr)
-    if 'UzUz' in ds.series.keys() and 'Uz_Sd' not in ds.series.keys():
+    if 'UzUz' in list(ds.series.keys()) and 'Uz_Sd' not in list(ds.series.keys()):
         UzUz,flag,attr = pfp_utils.GetSeriesasMA(ds,'UzUz')
         Uz_Sd = numpy.ma.sqrt(UzUz)
         attr = pfp_utils.MakeAttributeDictionary(long_name='Vertical velocity component from CSAT, standard deviation',units='m/s')
@@ -1841,7 +1841,7 @@
 def do_mergeseries(ds,target,srclist,mode="verbose"):
     if mode.lower()!="quiet":
         logger.info(' Merging '+str(srclist)+' ==> '+target)
-    if srclist[0] not in ds.series.keys():
+    if srclist[0] not in list(ds.series.keys()):
         if mode.lower()!="quiet":
             logger.error('  MergeSeries: primary input series '+srclist[0]+' not found')
             return
@@ -1853,7 +1853,7 @@
     tmplist = list(srclist)
     tmplist.remove(tmplist[0])
     for label in tmplist:
-        if label in ds.series.keys():
+        if label in list(ds.series.keys()):
             SeriesNameString = SeriesNameString+', '+label
             index = numpy.where(numpy.mod(flag1,10)==0)[0]         # find the elements with flag = 0, 10, 20 etc
             flag2[index] = 0                                        # set them all to 0
@@ -1907,9 +1907,9 @@
     RhoCp = pfp_utils.GetVariable(ds, "RhoCp")
     Lv = pfp_utils.GetVariable(ds, "Lv")
     # deal with aliases for CO2 concentration
-    if "Cc" in ds.series.keys():
+    if "Cc" in list(ds.series.keys()):
         CO2_in = "Cc"
-    elif "CO2" in ds.series.keys():
+    elif "CO2" in list(ds.series.keys()):
         CO2_in = "CO2"
     else:
         msg = " Fc_WPL: did not find CO2 in data structure"
@@ -2016,10 +2016,10 @@
     ones = numpy.ones(nRecs,dtype=numpy.int32)
     # deal with sonic temperature aliases
     Tv_in = "Tv_SONIC_Av"
-    if Tv_in not in ds.series.keys():
-        if "Tv_CSAT" in ds.series.keys():
+    if Tv_in not in list(ds.series.keys()):
+        if "Tv_CSAT" in list(ds.series.keys()):
             Tv_in = "Tv_CSAT"
-        elif "Tv_CSAT_Av" in ds.series.keys():
+        elif "Tv_CSAT_Av" in list(ds.series.keys()):
             Tv_in = "Tv_CSAT_Av"
         else:
             logger.error(" FhvtoFh: sonic virtual temperature not found in data structure")
@@ -2258,7 +2258,7 @@
         """
     logger.info(' Setting up the QC flags')
     nRecs = len(ds.series['xlDateTime']['Data'])
-    for ThisOne in ds.series.keys():
+    for ThisOne in list(ds.series.keys()):
         ds.series[ThisOne]['Flag'] = numpy.zeros(nRecs,dtype=numpy.int32)
         index = numpy.where(ds.series[ThisOne]['Data']==c.missing_value)[0]
         ds.series[ThisOne]['Flag'][index] = numpy.int32(1)
@@ -2321,7 +2321,7 @@
     # check to see if we need to do anything
     if max_length_hours == 0:
         return
-    if isinstance(labels, basestring):
+    if isinstance(labels, str):
         labels = [labels]
     elif isinstance(labels, list):
         pass
@@ -2333,7 +2333,7 @@
     max_length_points = int((max_length_hours * float(60)/float(ts)) + 0.5)
     for label in labels:
         # check that series is in the data structure
-        if label not in ds.series.keys():
+        if label not in list(ds.series.keys()):
             msg = " Variable " + label + " not found in data structure"
             logger.error(msg)
             continue
@@ -2447,11 +2447,11 @@
     wT = pfp_utils.GetVariable(ds, "wT")
     wC = pfp_utils.GetVariable(ds, "wC")
     wA = pfp_utils.GetVariable(ds, "wA")
-    if ustar_in not in ds.series.keys():
+    if ustar_in not in list(ds.series.keys()):
         ustarm = numpy.ma.sqrt(numpy.ma.sqrt(uw["Data"] ** 2 + vw["Data"] ** 2))
     else:
         ustarm, _, _ = pfp_utils.GetSeriesasMA(ds, ustar_in)
-    if L_in not in ds.series.keys():
+    if L_in not in list(ds.series.keys()):
         Lm = pfp_mf.molen(Ta["Data"], Ah["Data"], ps["Data"], ustarm, wT["Data"], fluxtype="kinematic")
     else:
         Lm, _, _ = pfp_utils.GetSeriesasMA(ds, L_in)
@@ -2571,10 +2571,10 @@
         logger.warning(msg)
         return
     # loop over the entries in merge
-    for target in merge[merge_order].keys():
+    for target in list(merge[merge_order].keys()):
         srclist = merge[merge_order][target]["source"]
         logger.info(" Merging "+str(srclist)+"==>"+target)
-        if srclist[0] not in ds.series.keys():
+        if srclist[0] not in list(ds.series.keys()):
             logger.error("  MergeSeries: primary input series "+srclist[0]+" not found")
             continue
         data = ds.series[srclist[0]]["Data"].copy()
@@ -2586,7 +2586,7 @@
         tmplist.remove(tmplist[0])
         s2add = ""
         for label in tmplist:
-            if label in ds.series.keys():
+            if label in list(ds.series.keys()):
                 SeriesNameString = SeriesNameString+", "+label
                 index = numpy.where(numpy.mod(flag1, 10) == 0)[0]       # find the elements with flag = 0, 10, 20 etc
                 flag2[index] = 0                                        # set them all to 0
@@ -2742,7 +2742,7 @@
     if section == None:
         return
     # check to see if the entry for series in the control file has the MergeSeries key
-    if 'MergeSeries' not in cf[section][series].keys():
+    if 'MergeSeries' not in list(cf[section][series].keys()):
         return
     # check to see if the series has already been merged
     if series in ds.mergeserieslist: return
@@ -2756,7 +2756,7 @@
         msg = ' Merging ' + str(srclist) + '==>' + series
         logger.info(msg)
         primary_series = srclist[0]
-        if primary_series not in ds.series.keys():
+        if primary_series not in list(ds.series.keys()):
             msg = "  MergeSeries: primary input series " + primary_series
             msg = msg + " not found for " + str(series)
             logger.warning(msg)
@@ -2770,12 +2770,12 @@
     else:
         msg = " Merging " + str(srclist) + "==>" + series
         logger.info(msg)
-        if srclist[0] not in ds.series.keys():
+        if srclist[0] not in list(ds.series.keys()):
             msg = "  MergeSeries: primary input series " + srclist[0] + " not found for " + str(series)
             logger.warning(msg)
             return
         primary_series = srclist[0]
-        if primary_series not in ds.series.keys():
+        if primary_series not in list(ds.series.keys()):
             msg = "  MergeSeries: primary input series " + primary_series
             msg = msg + " not found for " + str(series)
             logger.warning(msg)
@@ -2789,7 +2789,7 @@
         SeriesNameString = primary_series
         srclist.remove(primary_series)
         for secondary_series in srclist:
-            if secondary_series in ds.series.keys():
+            if secondary_series in list(ds.series.keys()):
                 secondary = pfp_utils.GetVariable(ds, secondary_series)
                 s_recs = len(secondary["Data"])
                 if (secondary_series == series) and save_originals:
@@ -2890,10 +2890,10 @@
             # has ReplaceOnDiff been specified for this series?
             if pfp_utils.incf(cf,ThisOne) and pfp_utils.haskey(cf,ThisOne,'ReplaceOnDiff'):
                 # loop over all entries in the ReplaceOnDiff section
-                for Alt in cf['Variables'][ThisOne]['ReplaceOnDiff'].keys():
-                    if 'FileName' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                for Alt in list(cf['Variables'][ThisOne]['ReplaceOnDiff'].keys()):
+                    if 'FileName' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                         alt_filename = cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt]['FileName']
-                        if 'AltVarName' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                        if 'AltVarName' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                             alt_varname = cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt]['AltVarName']
                         else:
                             alt_varname = ThisOne
@@ -2903,21 +2903,21 @@
                             ds_alt[n] = pfp_io.nc_read_series(alt_filename)
                         else:
                             n = open_ncfiles.index(alt_filename)
-                        if 'Transform' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                        if 'Transform' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                             AltDateTime = ds_alt[n].series['DateTime']['Data']
                             AltSeriesData = ds_alt[n].series[alt_varname]['Data']
                             TList = ast.literal_eval(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt]['Transform'])
                             for TListEntry in TList:
                                 TransformAlternate(TListEntry,AltDateTime,AltSeriesData,ts=ts)
-                        if 'Range' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                        if 'Range' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                             RList = ast.literal_eval(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt]['Range'])
                             for RListEntry in RList:
                                 ReplaceWhenDiffExceedsRange(ds.series['DateTime']['Data'],ds.series[ThisOne],
                                                             ds.series[ThisOne],ds_alt[n].series[alt_varname],
                                                             RListEntry)
-                    elif 'AltVarName' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                    elif 'AltVarName' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                         alt_varname = ThisOne
-                        if 'Range' in cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys():
+                        if 'Range' in list(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt].keys()):
                             RList = ast.literal_eval(cf['Variables'][ThisOne]['ReplaceOnDiff'][Alt]['Range'])
                             for RListEntry in RList:
                                 ReplaceWhenDiffExceedsRange(ds.series['DateTime']['Data'],ds.series[ThisOne],
@@ -2992,13 +2992,13 @@
     try:
         window_size = numpy.abs(numpy.int(window_size))
         order = numpy.abs(numpy.int(order))
-    except ValueError, msg:
+    except ValueError as msg:
         raise ValueError("window_size and order have to be of type int")
     if window_size % 2 != 1 or window_size < 1:
         raise TypeError("window_size size must be a positive odd number")
     if window_size < order + 2:
         raise TypeError("window_size is too small for the polynomials order")
-    order_range = range(order+1)
+    order_range = list(range(order+1))
     half_window = (window_size -1) // 2
     # precompute coefficients
     b = numpy.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])
@@ -3031,35 +3031,35 @@
     logger.info(' Calculating Ta from Tv')
     # check to see if we have enough data to proceed
     # deal with possible aliases for the sonic temperature
-    if Tv_in not in ds.series.keys():
-        if "Tv_CSAT_Av" in ds.series.keys():
+    if Tv_in not in list(ds.series.keys()):
+        if "Tv_CSAT_Av" in list(ds.series.keys()):
             Tv_in = "Tv_CSAT_Av"
             Ta_out = "Ta_CSAT_Av"
-        elif "Tv_CSAT" in ds.series.keys():
+        elif "Tv_CSAT" in list(ds.series.keys()):
             Tv_in = "Tv_CSAT"
             Ta_out = "Ta_CSAT"
         else:
             logger.error(" TaFromTv: sonic virtual temperature not found in data structure")
             return
-    if Ah_in not in ds.series.keys() and RH_in not in ds.series.keys() and q_in not in ds.series.keys():
+    if Ah_in not in list(ds.series.keys()) and RH_in not in list(ds.series.keys()) and q_in not in list(ds.series.keys()):
         labstr = str(Ah_in)+","+str(RH_in)+","+str(q_in)
         logger.error(" TaFromTv: no humidity data ("+labstr+") found in data structure")
         return
-    if ps_in not in ds.series.keys():
+    if ps_in not in list(ds.series.keys()):
         logger.error(" TaFromTv: pressure ("+str(ps_in)+") not found in data structure")
         return
     # we seem to have enough to continue
     Tv,f,a = pfp_utils.GetSeriesasMA(ds,Tv_in)
     ps,f,a = pfp_utils.GetSeriesasMA(ds,ps_in)
-    if Ah_in in ds.series.keys():
+    if Ah_in in list(ds.series.keys()):
         Ah,f,a = pfp_utils.GetSeriesasMA(ds,Ah_in)
         vp = pfp_mf.vapourpressure(Ah,Tv)
         mr = pfp_mf.mixingratio(ps,vp)
         q = pfp_mf.specifichumidity(mr)
-    elif RH_in in ds.series.keys():
+    elif RH_in in list(ds.series.keys()):
         RH,f,a = pfp_utils.GetSeriesasMA(ds,RH_in)
         q = pfp_mf.specifichumidityfromRH(RH,Tv,ps)
-    elif q_in in ds.series.keys():
+    elif q_in in list(ds.series.keys()):
         q,f,a = pfp_utils.GetSeriesasMA(ds,q_in)
     Ta_data = pfp_mf.tafromtv(Tv,q)
     nRecs = int(ds.globalattributes['nc_nrecs'])
RefactoringTool: Refactored ./scripts/pfp_utils.py
--- ./scripts/pfp_utils.py	(original)
+++ ./scripts/pfp_utils.py	(refactored)
@@ -79,13 +79,13 @@
     if len(ThisOne) == 0:
         return
     if len(key) == 0:
-        if Base in cf.keys() and ThisOne in cf[Base].keys():
-            return ThisOne in cf[Base].keys()
+        if Base in list(cf.keys()) and ThisOne in list(cf[Base].keys()):
+            return ThisOne in list(cf[Base].keys())
         else:
             return
     else:
-        if Base in cf.keys() and ThisOne in cf[Base].keys():
-            return key in cf[Base][ThisOne].keys()
+        if Base in list(cf.keys()) and ThisOne in list(cf[Base].keys()):
+            return key in list(cf[Base][ThisOne].keys())
         else:
             return
 
@@ -107,7 +107,7 @@
     """
     msg = " Checking missing data and QC flags are consistent"
     logger.info(msg)
-    labels = [label for label in ds.series.keys() if label not in ["DateTime"]]
+    labels = [label for label in list(ds.series.keys()) if label not in ["DateTime"]]
     # force any values of -9999 with QC flags of 0 to have a QC flag of 8
     for label in labels:
         var = GetVariable(ds, label)
@@ -175,7 +175,7 @@
     Author: PRI
     Date: January 2016
     """
-    if isinstance(label, basestring):
+    if isinstance(label, str):
         label_list = [label]
     elif isinstance(label, list):
         label_list = label
@@ -184,7 +184,7 @@
         logger.error(msg)
         return
     for label in label_list:
-        if label not in ds.series.keys():
+        if label not in list(ds.series.keys()):
             continue
         variable = GetVariable(ds, label)
         if variable["Attr"]["units"] != units and convert_units:
@@ -274,7 +274,7 @@
     # get the Fc units requested by the user
     Fc_units_out = get_keyvaluefromcf(cf, ['Options'], "FcUnits", default="umol/m2/s")
     # get a list of Fc series
-    Fc_list = [label for label in ds.series.keys() if label[0:2] == "Fc"]
+    Fc_list = [label for label in list(ds.series.keys()) if label[0:2] == "Fc"]
     # convert units of Fc as required
     units_list = ["mg/m2/s", "umol/m2/s"]
     for label in Fc_list:
@@ -936,7 +936,7 @@
     quadlist = ["N", "E", "S", "W"]
     direction = {'N':1, 'S':-1, 'E': 1, 'W':-1}
     # replace the degrees, minutes and seconds symbols with spaces
-    new = anglestring.replace(u'\B0', ' ').replace('\'', ' ').replace('"', ' ').strip()
+    new = anglestring.replace('\B0', ' ').replace('\'', ' ').replace('"', ' ').strip()
     try:
         # simple casting may work, who knows?
         anglefloat = float(new)
@@ -1004,10 +1004,10 @@
     V["Attr"]["long_name"] = "V component of wind velocity, positive north"
     V["Attr"]["units"] = "m/s"
     # copy the datetime if it is available
-    if "DateTime" in WS.keys():
+    if "DateTime" in list(WS.keys()):
         U["DateTime"] = copy.deepcopy(WS["DateTime"])
         V["DateTime"] = copy.deepcopy(WS["DateTime"])
-    elif "DateTime" in WD.keys():
+    elif "DateTime" in list(WD.keys()):
         U["DateTime"] = copy.deepcopy(WD["DateTime"])
         V["DateTime"] = copy.deepcopy(WD["DateTime"])
     return U, V
@@ -1047,10 +1047,10 @@
     WD["Attr"]["long_name"] = "Wind direction"
     WD["Attr"]["units"] = "deg"
     # copy the datetime if it is available
-    if "DateTime" in U.keys():
+    if "DateTime" in list(U.keys()):
         WS["DateTime"] = copy.deepcopy(U["DateTime"])
         WD["DateTime"] = copy.deepcopy(U["DateTime"])
-    elif "DateTime" in V.keys():
+    elif "DateTime" in list(V.keys()):
         WS["DateTime"] = copy.deepcopy(V["DateTime"])
         WD["DateTime"] = copy.deepcopy(V["DateTime"])
     return WS, WD
@@ -1086,19 +1086,19 @@
         ds.series['_tmp_']['Flag'] = Flag.astype(numpy.int32)
     # do the attributes
     ds.series['_tmp_']['Attr'] = {}
-    if Label in ds.series.keys():                 # check to see if the series already exists
+    if Label in list(ds.series.keys()):                 # check to see if the series already exists
         for attr in ds.series[Label]['Attr']:     # if it does, copy the existing attributes
             if attr in Attr and ds.series[Label]['Attr'][attr]!=Attr[attr]:
                 ds.series['_tmp_']['Attr'][attr] = Attr[attr]
             else:
                 ds.series['_tmp_']['Attr'][attr] = ds.series[Label]['Attr'][attr]
         for attr in Attr:
-            if attr not in ds.series['_tmp_']['Attr'].keys():
+            if attr not in list(ds.series['_tmp_']['Attr'].keys()):
                 ds.series['_tmp_']['Attr'][attr] = Attr[attr]
     else:
         for item in Attr:
             ds.series['_tmp_']['Attr'][item] = Attr[item]
-    ds.series[unicode(Label)] = ds.series['_tmp_']     # copy temporary series to new series
+    ds.series[str(Label)] = ds.series['_tmp_']     # copy temporary series to new series
     del ds.series['_tmp_']                        # delete the temporary series
 
 def CopyVariable(var):
@@ -1257,7 +1257,7 @@
     Author: PRI
     Date: November 2019
     """
-    if isinstance(variable, basestring):
+    if isinstance(variable, str):
         label = variable
     elif isinstance(variable, dict):
         label = variable["Label"]
@@ -1361,7 +1361,7 @@
     """ Remove duplicate records."""
     # the ds.series["DateTime"]["Data"] series is actually a list
     for item in ["DateTime","DateTime_UTC"]:
-        if item in ds.series.keys():
+        if item in list(ds.series.keys()):
             ldt,ldt_flag,ldt_attr = GetSeries(ds,item)
             # ldt_nodups is returned as an ndarray
             ldt_nodups,idx_nodups = numpy.unique(ldt,return_index=True)
@@ -1369,7 +1369,7 @@
             ds.series[item]["Data"] = ldt_nodups
             ds.series[item]["Flag"] = ldt_flag[idx_nodups]
     # get a list of the series in the data structure
-    series_list = [item for item in ds.series.keys() if '_QCFlag' not in item]
+    series_list = [item for item in list(ds.series.keys()) if '_QCFlag' not in item]
     # remove the DateTime
     for item in ["DateTime","DateTime_UTC"]:
         if item in series_list: series_list.remove(item)
@@ -1402,7 +1402,7 @@
     dt_diffs = numpy.array([(ldt[i]-rounddttots(ldt[i],ts=ts)).total_seconds() for i in range(1,len(ldt))])
     logger.info(" Maximum drift is "+str(numpy.max(dt_diffs))+" seconds, minimum drift is "+str(numpy.min(dt_diffs))+" seconds")
     ans = fixtimestepmethod
-    if ans=="": ans = raw_input("Do you want to [Q]uit, [I]nterploate or [R]ound? ")
+    if ans=="": ans = input("Do you want to [Q]uit, [I]nterploate or [R]ound? ")
     if ans.lower()[0]=="q":
         msg = "Quiting ..."
         logger.error(msg)
@@ -1452,7 +1452,7 @@
     ds.series['DateTime']['Data'] = ldt_nogaps
     ds.series['DateTime']['Flag'] = numpy.zeros(len(ldt_nogaps),dtype=numpy.int32)
     # get a list of series in the data structure
-    series_list = [item for item in ds.series.keys() if '_QCFlag' not in item]
+    series_list = [item for item in list(ds.series.keys()) if '_QCFlag' not in item]
     # remove the datetime-related series from data structure
     datetime_list = ["DateTime","DateTime_UTC"]
     for item in datetime_list:
@@ -1526,7 +1526,7 @@
         section = "Variables"
     src_list = []
     got_source = False
-    for key in cf[section][label]['AverageSeries'].keys():
+    for key in list(cf[section][label]['AverageSeries'].keys()):
         if key.lower() == "source":
             got_source = True
             src_string = cf[section][label]['AverageSeries'][key]
@@ -1547,10 +1547,10 @@
     If it isn't, check the control file to see if an alternate name has been specified
      and return the alternate name if one exists.
     '''
-    if ThisOne not in ds.series.keys():
-        if ThisOne in cf['Variables'].keys():
+    if ThisOne not in list(ds.series.keys()):
+        if ThisOne in list(cf['Variables'].keys()):
             ThisOne = cf['Variables'][ThisOne]['AltVarName']
-            if ThisOne not in ds.series.keys():
+            if ThisOne not in list(ds.series.keys()):
                 logger.error('GetAltName: alternate variable name not in ds')
         else:
             logger.error('GetAltName: cant find ',ThisOne,' in ds or control file')
@@ -1560,8 +1560,8 @@
     '''
     Get an alternate variable name from the control file.
     '''
-    if ThisOne in cf['Variables'].keys():
-        if 'AltVarName' in cf['Variables'][ThisOne].keys():
+    if ThisOne in list(cf['Variables'].keys()):
+        if 'AltVarName' in list(cf['Variables'][ThisOne].keys()):
             ThisOne = str(cf['Variables'][ThisOne]['AltVarName'])
         else:
             msg = 'GetAltNameFromCF: AltVarName key not in control file for '+str(ThisOne)
@@ -1574,7 +1574,7 @@
 def GetAttributeDictionary(ds,ThisOne):
     attr = {}
     # if series ThisOne is in the data structure
-    if ThisOne in ds.series.keys():
+    if ThisOne in list(ds.series.keys()):
         attr = ds.series[ThisOne]['Attr']
     else:
         attr = MakeAttributeDictionary()
@@ -1584,8 +1584,8 @@
     '''
     Get colour bar tick labels from the control file.
     '''
-    if ThisOne in cf['Variables'].keys():
-        if 'Ticks' in cf['Variables'][ThisOne].keys():
+    if ThisOne in list(cf['Variables'].keys()):
+        if 'Ticks' in list(cf['Variables'][ThisOne].keys()):
             Ticks = eval(cf['Variables'][ThisOne]['Ticks'])
         else:
             msg = 'GetcbTicksFromCF: Ticks key not in control file for '+str(ThisOne)
@@ -1599,15 +1599,15 @@
     '''
     Get lower and upper range limits from the control file.
     '''
-    if ThisOne in cf['Variables'].keys():
-        if 'Lower' in cf['Variables'][ThisOne].keys():
+    if ThisOne in list(cf['Variables'].keys()):
+        if 'Lower' in list(cf['Variables'][ThisOne].keys()):
             lower = float(cf['Variables'][ThisOne]['Lower'])
         else:
             if mode.lower()!="quiet":
                 msg = "GetRangesFromCF: Lower key not in control file for "+str(ThisOne)
                 logger.info(msg)
             lower = None
-        if 'Upper' in cf['Variables'][ThisOne].keys():
+        if 'Upper' in list(cf['Variables'][ThisOne].keys()):
             upper = float(cf['Variables'][ThisOne]['Upper'])
         else:
             if mode.lower()!="quiet":
@@ -1732,8 +1732,8 @@
     return i
 
 def GetGlobalAttributeValue(cf,ds,ThisOne):
-    if ThisOne not in ds.globalattributes.keys():
-        if ThisOne in cf['General'].keys():
+    if ThisOne not in list(ds.globalattributes.keys()):
+        if ThisOne in list(cf['General'].keys()):
             ds.globalattributes[ThisOne] = cf['General'][ThisOne]
         else:
             logger.error('  GetGlobalAttributeValue: global attribute '+ThisOne+' was not found in the netCDF file or in the control file')
@@ -1752,7 +1752,7 @@
         section = 'Variables'
     src_list = []
     got_source = False
-    for key in cf[section][ThisOne]['MergeSeries'].keys():
+    for key in list(cf[section][ThisOne]['MergeSeries'].keys()):
         if key.lower() == "source":
             got_source = True
             src_string = cf[section][ThisOne]['MergeSeries'][key]
@@ -1803,7 +1803,7 @@
     # number of records
     nRecs = int(ds.globalattributes["nc_nrecs"])
     # check the series requested is in the data structure
-    if ThisOne in ds.series.keys():
+    if ThisOne in list(ds.series.keys()):
         # series is in the data structure
         if isinstance(ds.series[ThisOne]['Data'],list):
             # return a list if the series is a list
@@ -1812,14 +1812,14 @@
             # return a numpy array if series is an array
             Series = ds.series[ThisOne]['Data'].copy()
         # now get the QC flag
-        if 'Flag' in ds.series[ThisOne].keys():
+        if 'Flag' in list(ds.series[ThisOne].keys()):
             # return the QC flag if it exists
             Flag = ds.series[ThisOne]['Flag'].copy()
         else:
             # create a QC flag if one does not exist
             Flag = numpy.zeros(nRecs,dtype=numpy.int32)
         # now get the attribute dictionary
-        if "Attr" in ds.series[ThisOne].keys():
+        if "Attr" in list(ds.series[ThisOne].keys()):
             Attr = GetAttributeDictionary(ds,ThisOne)
         else:
             Attr = MakeAttributeDictionary()
@@ -2014,7 +2014,7 @@
     for ThisGroup, ThisLabel in zip([rad,met,flux,soil],['radiation','meteorology','flux','soil']):
         sum_coverage = float(0); count = float(0)
         for ThisOne in ThisGroup:
-            if ThisOne in ds.series.keys():
+            if ThisOne in list(ds.series.keys()):
                 sum_coverage = sum_coverage + float(ds.series[ThisOne]['Attr']['coverage_'+level])
                 count = count + 1
         if count!=0:
@@ -2027,7 +2027,7 @@
     level = "L1"
     if "nc_level" in ds.globalattributes:
         level = str(ds.globalattributes["nc_level"])
-    SeriesList = ds.series.keys()
+    SeriesList = list(ds.series.keys())
     for ThisOne in ["DateTime","DateTime_UTC"]:
         if ThisOne in SeriesList: SeriesList.remove(ThisOne)
     for ThisOne in SeriesList:
@@ -2043,9 +2043,9 @@
     Author: PRI
     Date: August 2018
     """
-    if "xlDateTime" in ds.series.keys():
+    if "xlDateTime" in list(ds.series.keys()):
         get_datetimefromxldate(ds)
-    elif "DateTime" in cf["Variables"].keys():
+    elif "DateTime" in list(cf["Variables"].keys()):
         if "Function" in cf["Variables"]["DateTime"]:
             # call the function given in the control file to convert the date/time string to a datetime object
             # NOTE: the function being called needs to deal with missing date values and empty lines
@@ -2070,7 +2070,7 @@
     ts = int(ds.globalattributes["time_step"])
     nRecs = int(ds.globalattributes["nc_nrecs"])
     dt = netCDF4.num2date(time,time_units)
-    ds.series[unicode("DateTime")] = {}
+    ds.series[str("DateTime")] = {}
     ds.series["DateTime"]["Data"] = dt
     ds.series["DateTime"]["Flag"] = numpy.zeros(nRecs)
     ds.series["DateTime"]["Attr"] = {}
@@ -2093,7 +2093,7 @@
     xldate = ds.series['xlDateTime']['Data']
     nRecs = len(ds.series['xlDateTime']['Data'])
     datemode = int(ds.globalattributes['xl_datemode'])
-    ds.series[unicode('DateTime')] = {}
+    ds.series[str('DateTime')] = {}
     basedate = datetime.datetime(1899, 12, 30)
     dt = [None]*nRecs
     for i in range(nRecs):
@@ -2107,7 +2107,7 @@
 def get_datetimefromymdhms(ds):
     ''' Creates a series of Python datetime objects from the year, month,
     day, hour, minute and second series stored in the netCDF file.'''
-    SeriesList = ds.series.keys()
+    SeriesList = list(ds.series.keys())
     if ('Year' not in SeriesList or 'Month' not in SeriesList or 'Day' not in SeriesList or
         'Hour' not in SeriesList or 'Minute' not in SeriesList or 'Second' not in SeriesList):
         logger.info(' get_datetimefromymdhms: unable to find all datetime fields required')
@@ -2160,7 +2160,7 @@
     Author: PRI
     Date: October 2016
     """
-    if isinstance(end, basestring):
+    if isinstance(end, str):
         try:
             end = dateutil.parser.parse(end)
             if end <= ldt[-1] and end >= ldt[0]:
@@ -2245,11 +2245,11 @@
            label_list is a list of variable labels referenced in the control file.
     """
     if "Variables" in cf:
-        label_list = cf["Variables"].keys()
+        label_list = list(cf["Variables"].keys())
     elif "Drivers" in cf:
-        label_list = cf["Drivers"].keys()
+        label_list = list(cf["Drivers"].keys())
     elif "Fluxes" in cf:
-        label_list = cf["Fluxes"].keys()
+        label_list = list(cf["Fluxes"].keys())
     else:
         label_list = []
         msg = "No Variables, Drivers or Fluxes section found in control file"
@@ -2272,17 +2272,17 @@
     # create an empty list
     alt_list = []
     # check to see if there was any gap filling using data from alternate sources
-    if "GapFillFromAlternate" in l4_info.keys():
+    if "GapFillFromAlternate" in list(l4_info.keys()):
         l4a = l4_info["GapFillFromAlternate"]
         # if so, get a list of the quantities gap filled from alternate sources
-        alt_list = list(set([l4a["outputs"][item]["target"] for item in l4a["outputs"].keys()]))
+        alt_list = list(set([l4a["outputs"][item]["target"] for item in list(l4a["outputs"].keys())]))
     # create an empty list
     cli_list = []
     # check to see if there was any gap filling from climatology
-    if "GapFillFromClimatology" in l4_info.keys():
+    if "GapFillFromClimatology" in list(l4_info.keys()):
         l4c = l4_info["GapFillFromClimatology"]
         # if so, get a list of the quantities gap filled using climatology
-        cli_list = list(set([l4c["outputs"][item]["target"] for item in l4c["outputs"].keys()]))
+        cli_list = list(set([l4c["outputs"][item]["target"] for item in list(l4c["outputs"].keys())]))
     # one list to rule them, one list to bind them ...
     gf_list = list(set(alt_list + cli_list))
     # clear out if there was no gap filling
@@ -2291,7 +2291,7 @@
     # loop over the series to be checked
     gap_found = False
     for series in gf_list:
-        if series not in ds.series.keys(): continue
+        if series not in list(ds.series.keys()): continue
         data,flag,attr = GetSeriesasMA(ds,series)
         idx = numpy.ma.where(data.mask == True)[0]
         if len(idx) != 0:
@@ -2339,12 +2339,12 @@
     return
 
 def get_nrecs(ds):
-    if 'nc_nrecs' in ds.globalattributes.keys():
+    if 'nc_nrecs' in list(ds.globalattributes.keys()):
         nRecs = int(ds.globalattributes['nc_nrecs'])
-    elif 'NumRecs' in ds.globalattributes.keys():
+    elif 'NumRecs' in list(ds.globalattributes.keys()):
         nRecs = int(ds.globalattributes['NumRecs'])
     else:
-        series_list = ds.series.keys()
+        series_list = list(ds.series.keys())
         nRecs = len(ds.series[series_list[0]]['Data'])
     return nRecs
 
@@ -2355,7 +2355,7 @@
     Author: PRI
     Date: October 2016
     """
-    if isinstance(start, basestring):
+    if isinstance(start, str):
         try:
             start = dateutil.parser.parse(start)
             if start >= ldt[0] and start <= ldt[-1]:
@@ -2408,7 +2408,7 @@
     found = False
     # strip out spaces and commas from the site name
     site_name = site_name.replace(" ","").replace(",","")
-    for item in c.tz_dict.keys():
+    for item in list(c.tz_dict.keys()):
         if item in site_name.lower():
             time_zone = c.tz_dict[item]
             found = True
@@ -2426,9 +2426,9 @@
     Author: PRI
     '''
     # check the time_zone global attribute is set, we cant continue without it
-    if "time_zone" not in ds.globalattributes.keys():
+    if "time_zone" not in list(ds.globalattributes.keys()):
         logger.warning("get_UTCfromlocaltime: time_zone not in global attributes, checking elsewhere ...")
-        if "site_name" in ds.globalattributes.keys():
+        if "site_name" in list(ds.globalattributes.keys()):
             site_name = ds.globalattributes["site_name"]
         else:
             logger.warning("get_UTCfromlocaltime: site_name not in global attributes, skipping UTC calculation ...")
@@ -2469,7 +2469,7 @@
     Author: PRI
     '''
     # get the datemode of the original Excel spreadsheet
-    if "xl_datemode" in ds.globalattributes.keys():
+    if "xl_datemode" in list(ds.globalattributes.keys()):
         datemode = int(ds.globalattributes["xl_datemode"])
     else:
         datemode = int(0)
@@ -2583,10 +2583,10 @@
     CreateSeries(ds,'Ddd',Ddd,flag,MakeAttributeDictionary(long_name='Decimal day of the year',units='none'))
 
 def haskey(cf,ThisOne,key):
-    return key in cf['Variables'][ThisOne].keys()
+    return key in list(cf['Variables'][ThisOne].keys())
 
 def incf(cf,ThisOne):
-    return ThisOne in cf['Variables'].keys()
+    return ThisOne in list(cf['Variables'].keys())
 
 def linear_function(B,x):
     """
@@ -2652,13 +2652,13 @@
         #log.info('  MakeQCFlag: no series list specified')
         pass
     if len(SeriesList)==1:
-        if SeriesList[0] in ds.series.keys():
+        if SeriesList[0] in list(ds.series.keys()):
             flag = ds.series[SeriesList[0]]['Flag'].copy()
         else:
             logger.error('  MakeQCFlag: series '+str(SeriesList[0])+' not in ds.series')
     if len(SeriesList)>1:
         for ThisOne in SeriesList:
-            if ThisOne in ds.series.keys():
+            if ThisOne in list(ds.series.keys()):
                 if len(flag)==0:
                     #flag = numpy.ones(numpy.size(ds.series[ThisOne]['Flag']))
                     flag = ds.series[ThisOne]['Flag'].copy()
@@ -2791,7 +2791,7 @@
     # initialise the returned list to an empty list
     l = []
     # check to see that a string was passed in
-    if not isinstance(s, basestring):
+    if not isinstance(s, str):
         # error message and return if input was not a string
         msg = "parse_rangecheck_limits: argument must be a string"
         logger.error(msg)
@@ -2990,8 +2990,8 @@
     if "controlfile_name" in cf:
         ds.globalattributes["controlfile_name"] = cf["controlfile_name"]
     if "Global" in cf:
-        for item in cf["Global"].keys():
-            if item not in ds.globalattributes.keys():
+        for item in list(cf["Global"].keys()):
+            if item not in list(ds.globalattributes.keys()):
                 ds.globalattributes[item] = cf["Global"][item].replace("\n"," ").replace("\r","")
 
 def update_progress(progress):
RefactoringTool: Refactored ./scripts/pysolar.py
--- ./scripts/pysolar.py	(original)
+++ ./scripts/pysolar.py	(refactored)
@@ -48,7 +48,8 @@
 
 """
 
-def buildPolyFit((a, b, c, d)): 
+def buildPolyFit(xxx_todo_changeme): 
+    (a, b, c, d) = xxx_todo_changeme
     return (lambda x: a + b * x + c * x ** 2 + (x ** 3) / d)
 
 def buildPolyDict():
@@ -418,7 +419,7 @@
                 azimuth_deg = GetAzimuth(latitude_deg, longitude_deg, d)
                 power = GetRadiationDirect(d, altitude_deg)
                 if (altitude_deg > 0):
-                        print timestamp, "UTC", altitude_deg, azimuth_deg, power
+                        print(timestamp, "UTC", altitude_deg, azimuth_deg, power)
                 d = d + thirty_minutes
 
 def EquationOfTime(day):
RefactoringTool: No changes to ./scripts/split_dialog.py
RefactoringTool: Refactored ./utilities/access_concatenate_daily.py
--- ./utilities/access_concatenate_daily.py	(original)
+++ ./utilities/access_concatenate_daily.py	(refactored)
@@ -90,7 +90,7 @@
     ds_60minutes.globalattributes["latitude"] = f.variables["lat"][1]
     ds_60minutes.globalattributes["longitude"] = f.variables["lon"][1]
     # list of variables to process
-    var_list = cf["Variables"].keys()
+    var_list = list(cf["Variables"].keys())
     # get a series of Python datetimes and put this into the data structure
     valid_date = f.variables["valid_date"][:]
     nRecs = len(valid_date)
@@ -132,7 +132,7 @@
         # get the name of the variable in the ACCESS file
         access_name = qcutils.get_keyvaluefromcf(cf,["Variables",label],"access_name",default=label)
         # warn the user if the variable not found
-        if access_name not in f.variables.keys():
+        if access_name not in list(f.variables.keys()):
             msg = "Requested variable "+access_name
             msg = msg+" not found in ACCESS data"
             logging.error(msg)
@@ -342,7 +342,7 @@
 def interpolate_to_30minutes(ds_60minutes):
     ds_30minutes = qcio.DataStructure()
     # copy the global attributes
-    for this_attr in ds_60minutes.globalattributes.keys():
+    for this_attr in list(ds_60minutes.globalattributes.keys()):
         ds_30minutes.globalattributes[this_attr] = ds_60minutes.globalattributes[this_attr]
     # update the global attribute "time_step"
     ds_30minutes.globalattributes["time_step"] = 30
@@ -370,7 +370,7 @@
     nRecs_30 = len(ds_30minutes.series["DateTime"]["Data"])
     x_60minutes = numpy.arange(0,nRecs_60,1)
     x_30minutes = numpy.arange(0,nRecs_60-0.5,0.5)
-    varlist_60 = ds_60minutes.series.keys()
+    varlist_60 = list(ds_60minutes.series.keys())
     # strip out the date and time variables already done
     for item in ["DateTime","DateTime_UTC","xlDateTime","Year","Month","Day","Hour","Minute","Second","Hdh","Hr_UTC"]:
         if item in varlist_60: varlist_60.remove(item)
@@ -459,7 +459,7 @@
     f = ACCESSData()
     # check that we have a list of files to process
     if len(file_list)==0:
-        print "access_read_mfiles: empty file_list received, returning ..."
+        print("access_read_mfiles: empty file_list received, returning ...")
         return f
     # make sure latitude and longitude are read
     if "lat" not in var_list: var_list.append("lat")
@@ -475,15 +475,15 @@
         shape = (len(dims["time"]),len(dims["lat"]),len(dims["lon"]))
         # move to the next file if this file doesn't have 25 time records
         if shape[0]!=1:
-            print "access_read_mfiles: length of time dimension in "+file_name+" is "+str(shape[0])+" (expected 1)"
+            print("access_read_mfiles: length of time dimension in "+file_name+" is "+str(shape[0])+" (expected 1)")
             continue
         # move to the next file if this file doesn't have 3 latitude records
         if shape[1]!=3:
-            print "access_read_mfiles: length of lat dimension in "+file_name+" is "+str(shape[1])+" (expected 3)"
+            print("access_read_mfiles: length of lat dimension in "+file_name+" is "+str(shape[1])+" (expected 3)")
             continue
         # move to the next file if this file doesn't have 3 longitude records
         if shape[2]!=3:
-            print "access_read_mfiles: length of lon dimension in "+file_name+" is "+str(shape[2])+" (expected 3)"
+            print("access_read_mfiles: length of lon dimension in "+file_name+" is "+str(shape[2])+" (expected 3)")
             continue
         # seems OK to continue with this file ...
         # add the file name to the file_list in the global attributes
@@ -494,15 +494,15 @@
                 f.globalattr[gattr] = getattr(ncfile,gattr)
         # if no variable list was passed to this routine, use all variables
         if len(var_list)==0:
-            var_list=ncfile.variables.keys()
+            var_list=list(ncfile.variables.keys())
         # load the data into the data structure
         for var in var_list:
             # get the name of the variable in the ACCESS file
             access_name = qcutils.get_keyvaluefromcf(cf,["Variables",var],"access_name",default=var)
             # check that the requested variable exists in the ACCESS file
-            if access_name in ncfile.variables.keys():
+            if access_name in list(ncfile.variables.keys()):
                 # check to see if the variable is already in the data structure
-                if access_name not in f.variables.keys():
+                if access_name not in list(f.variables.keys()):
                     f.variables[access_name] = ncfile.variables[access_name][:]
                 else:
                     f.variables[access_name] = numpy.concatenate((f.variables[access_name],ncfile.variables[access_name][:]),axis=0)
@@ -512,12 +512,12 @@
                 # loop over the variable attributes
                 for this_attr in ncfile.variables[access_name].ncattrs():
                     # check to see if the attribute has already 
-                    if this_attr not in f.varattr[access_name].keys():
+                    if this_attr not in list(f.varattr[access_name].keys()):
                         # add the variable attribute if it's not there already
                         f.varattr[access_name][this_attr] = getattr(ncfile.variables[access_name],this_attr)
             else:
-                print "access_read_mfiles: ACCESS variable "+access_name+" not found in "+file_name
-                if access_name not in f.variables.keys():
+                print("access_read_mfiles: ACCESS variable "+access_name+" not found in "+file_name)
+                if access_name not in list(f.variables.keys()):
                     f.variables[access_name] = makedummyseries(shape)
                 else:
                     f.variables[access_name] = numpy.concatenate((f.variables[access_name],makedummyseries(shape)),axis=0)
@@ -546,8 +546,8 @@
 cf = configobj.ConfigObj(cf_name)
 # get stuff from the control file
 logging.info('Getting control file contents')
-site_list = cf["Sites"].keys()
-var_list = cf["Variables"].keys()
+site_list = list(cf["Sites"].keys())
+var_list = list(cf["Variables"].keys())
 # loop over sites
 #site_list = ["AdelaideRiver"]
 for site in site_list:
RefactoringTool: Refactored ./utilities/access_concatenate_monthly.py
--- ./utilities/access_concatenate_monthly.py	(original)
+++ ./utilities/access_concatenate_monthly.py	(refactored)
@@ -73,7 +73,7 @@
 # read the site master file and get a list of sites to process
 logger.info("Reading the site master file")
 site_info = read_site_master(xl_file_path, sheet_name)
-site_list = site_info.keys()
+site_list = list(site_info.keys())
 for site in site_list:
     logger.info("Processing site "+site)
     access_file_path_list = []
RefactoringTool: Refactored ./utilities/aws2nc.py
--- ./utilities/aws2nc.py	(original)
+++ ./utilities/aws2nc.py	(refactored)
@@ -60,7 +60,7 @@
     site_latitude = bom_sites_info[site_name]["latitude"]
     site_longitude = bom_sites_info[site_name]["longitude"]
     site_elevation = bom_sites_info[site_name]["elevation"]
-    site_number_list = bom_sites_info[site_name].keys()
+    site_number_list = list(bom_sites_info[site_name].keys())
     for item in ["latitude","longitude","elevation"]:
         if item in site_number_list: site_number_list.remove(item)
     # read the CSV files and put the contents into data_dict
@@ -94,7 +94,7 @@
     # now pull the data out and put it in separate data structures, one per station, all
     # of which are held in a data structure dictionary
     ds_dict = {}
-    for bom_id in data_dict.keys():
+    for bom_id in list(data_dict.keys()):
         log.info("Processing BoM station: "+str(bom_id))
         # create a data structure
         ds=qcio.DataStructure()
@@ -161,7 +161,7 @@
 
     # get the earliest start datetime and the latest end datetime
     log.info("Finding the start and end dates")
-    bom_id_list = ds_dict.keys()
+    bom_id_list = list(ds_dict.keys())
     ds0 = ds_dict[bom_id_list[0]]
     ldt = ds0.series["DateTime"]["Data"]
     #print bom_id_list[0],":",ldt[0],ldt[-1]
@@ -220,7 +220,7 @@
             qcutils.CreateSeries(ds_all,output_label,data_all,flag_all,attr)
     # get precipitation per time step
     # now get precipitation per time step from the interpolated precipitation accumulated over the day
-    precip_list = [x for x in ds_all.series.keys() if ("Precip" in x) and ("_QCFlag" not in x)]
+    precip_list = [x for x in list(ds_all.series.keys()) if ("Precip" in x) and ("_QCFlag" not in x)]
     #print precip_list
     log.info("Converting 24 hour accumulated precipitation")
     for output_label in precip_list:
@@ -261,9 +261,9 @@
         # put the precipitation per time step back into the data struicture
         qcutils.CreateSeries(ds_all,output_label,precip,accum_flag,accum_attr)
     # calculate missing humidities
-    RH_list = sorted([x for x in ds_all.series.keys() if ("RH" in x) and ("_QCFlag" not in x)])
-    Ta_list = sorted([x for x in ds_all.series.keys() if ("Ta" in x) and ("_QCFlag" not in x)])
-    ps_list = sorted([x for x in ds_all.series.keys() if ("ps" in x) and ("_QCFlag" not in x)])
+    RH_list = sorted([x for x in list(ds_all.series.keys()) if ("RH" in x) and ("_QCFlag" not in x)])
+    Ta_list = sorted([x for x in list(ds_all.series.keys()) if ("Ta" in x) and ("_QCFlag" not in x)])
+    ps_list = sorted([x for x in list(ds_all.series.keys()) if ("ps" in x) and ("_QCFlag" not in x)])
     for RH_label,Ta_label,ps_label in zip(RH_list,Ta_list,ps_list):
         Ta,f,a = qcutils.GetSeriesasMA(ds_all,Ta_label)
         RH,f,a = qcutils.GetSeriesasMA(ds_all,RH_label)
@@ -295,4 +295,4 @@
     qcio.nc_write_series(ncfile,ds_all,ndims=1)
     log.info("Finished site: "+site_name)
 
-print "aws2nc: All done"
+print("aws2nc: All done")
RefactoringTool: Refactored ./utilities/aws_averageto60minutes.py
--- ./utilities/aws_averageto60minutes.py	(original)
+++ ./utilities/aws_averageto60minutes.py	(refactored)
@@ -3,7 +3,7 @@
 import sys
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "erai2nc: the scripts directory is missing"
+    print("erai2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -15,12 +15,12 @@
 ds_aws_30minute = qcio.nc_read_series(aws_name)
 has_gaps = qcutils.CheckTimeStep(ds_aws_30minute)
 if has_gaps:
-    print "Problems found with time step"
+    print("Problems found with time step")
     qcutils.FixTimeStep(ds_aws_30minute)
     qcutils.get_ymdhmsfromdatetime(ds_aws_30minute)
 dt_aws_30minute = ds_aws_30minute.series["DateTime"]["Data"]
 ddt=[dt_aws_30minute[i+1]-dt_aws_30minute[i] for i in range(0,len(dt_aws_30minute)-1)]
-print "Minimum time step is",min(ddt)," Maximum time step is",max(ddt)
+print("Minimum time step is",min(ddt)," Maximum time step is",max(ddt))
 
 dt_aws_30minute = ds_aws_30minute.series["DateTime"]["Data"]
 start_date = dt_aws_30minute[0]
@@ -35,14 +35,14 @@
 dt_aws_60minute = list(dt_aws_2d[:,1])
 nRecs_60minute = len(dt_aws_60minute)
 
-series_list = ds_aws_30minute.series.keys()
+series_list = list(ds_aws_30minute.series.keys())
 for item in ["DateTime","Ddd","Day","Minute","xlDateTime","Hour","time","Month","Second","Year"]:
     if item in series_list: series_list.remove(item)
 
     # get the 60 minute data structure
     ds_aws_60minute = qcio.DataStructure()
     # get the global attributes
-    for item in ds_aws_30minute.globalattributes.keys():
+    for item in list(ds_aws_30minute.globalattributes.keys()):
         ds_aws_60minute.globalattributes[item] = ds_aws_30minute.globalattributes[item]
     # overwrite with 60 minute values as appropriate
     ds_aws_60minute.globalattributes["nc_nrecs"] = str(nRecs_60minute)
RefactoringTool: Refactored ./utilities/aws_update.py
--- ./utilities/aws_update.py	(original)
+++ ./utilities/aws_update.py	(refactored)
@@ -4,7 +4,7 @@
 import sys
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "erai2nc: the scripts directory is missing"
+    print("erai2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -21,11 +21,11 @@
     # open the output file
     out_file = open(out_filename,"a")
     # open the first input file and append
-    print "Adding contents of "+in_filename+" to "+out_filename
+    print("Adding contents of "+in_filename+" to "+out_filename)
     for line in open(in_filename):
         out_file.write(line)
     # now loop over the folders containing the files to be appended
-    add_path_list = [cf["Files"]["Add"][i] for i in cf["Files"]["Add"].keys()]
+    add_path_list = [cf["Files"]["Add"][i] for i in list(cf["Files"]["Add"].keys())]
     bom_id = in_filename.split("_")[2]
     for add_path in add_path_list:
         # build the file name
@@ -34,7 +34,7 @@
         for add_filename in add_filename_list:
             #print "Appending contents of "+add_filename+" to "+out_filename
             add_file = open(add_filename)
-            add_file.next()
+            next(add_file)
             for line in add_file:
                 out_file.write(line)
             add_file.close()
RefactoringTool: Refactored ./utilities/bios2nc.py
--- ./utilities/bios2nc.py	(original)
+++ ./utilities/bios2nc.py	(refactored)
@@ -17,8 +17,8 @@
 if len(cf)==0: sys.exit()
 start_date = cf["General"]["start_date"]
 end_date = cf["General"]["end_date"]
-var_list = cf["Variables"].keys()
-site_list = cf["Sites"].keys()
+var_list = list(cf["Variables"].keys())
+site_list = list(cf["Sites"].keys())
 for site in site_list:
     # get the input file mask
     infilename = cf["Sites"][site]["in_filepath"]+cf["Sites"][site]["in_filename"]
@@ -152,7 +152,7 @@
         ds_60.series["DateTime"]["Flag"] = flag_60
         ds_60.series["DateTime"]["Attr"] = ds_30.series["DateTime"]["Attr"]
         # copy across the global attributes
-        for gattr in ds_30.globalattributes.keys():
+        for gattr in list(ds_30.globalattributes.keys()):
             ds_60.globalattributes[gattr] = ds_30.globalattributes[gattr]
         # set some hourly specific global attributes
         ds_60.globalattributes["nc_nrecs"] = nRecs_60
@@ -169,7 +169,7 @@
         precip_60 = numpy.sum(precip_30_2d,axis=1)
         qcutils.CreateSeries(ds_60,"Precip",precip_60,flag_60,attr)
         # get a list of the variables, exclude the QC flags
-        series_list = [item for item in ds_30.series.keys() if "_QCFlag" not in item]
+        series_list = [item for item in list(ds_30.series.keys()) if "_QCFlag" not in item]
         # remove the datetime variables
         for item in ["DateTime","DateTime_UTC","time","Precip","xlDateTime","xlDateTime_UTC"
                      "Year","Month","Day","Hour","Minute","Second"]:
@@ -193,4 +193,4 @@
         qcio.nc_write_series(ncfile,ds_30,ndims=1)
     log.info("Finished site: "+site)
 
-print "All done"
+print("All done")
RefactoringTool: Refactored ./utilities/cleanup_netcdf_files.py
--- ./utilities/cleanup_netcdf_files.py	(original)
+++ ./utilities/cleanup_netcdf_files.py	(refactored)
@@ -12,7 +12,7 @@
 # check the scripts folder exists
 scripts_path = os.path.join("..", "scripts", "")
 if not os.path.exists(scripts_path):
-    print "cleanup_netcdf_files: the scripts directory is missing"
+    print("cleanup_netcdf_files: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append(scripts_path)
@@ -39,23 +39,23 @@
     # check site_name is in ds.globalattributes
     gattr_list = list(ds.globalattributes.keys())
     if "site_name" not in gattr_list:
-        print "Global attributes: site_name not found"
+        print("Global attributes: site_name not found")
     # check latitude and longitude are in ds.globalattributes
     if "latitude" not in gattr_list:
-        print "Global attributes: latitude not found"
+        print("Global attributes: latitude not found")
     else:
         lat_string = str(ds.globalattributes["latitude"])
         if len(lat_string) == 0:
-            print "Global attributes: latitude empty"
+            print("Global attributes: latitude empty")
         else:
             lat = pfp_utils.convert_anglestring(lat_string)
         ds.globalattributes["latitude"] = str(lat)
     if "longitude" not in gattr_list:
-        print "Global attributes: longitude not found"
+        print("Global attributes: longitude not found")
     else:
         lon_string = str(ds.globalattributes["longitude"])
         if len(lon_string) == 0:
-            print "Global attributes: longitude empty"
+            print("Global attributes: longitude empty")
         else:
             lon = pfp_utils.convert_anglestring(lon_string)
         ds.globalattributes["longitude"] = str(lon)
@@ -77,14 +77,14 @@
                     tz = tf.timezone_at(lng=lon, lat=lat)
                     ds.globalattributes["time_zone"] = tz
                 else:
-                    print "Global attributes: unable to define time zone"
+                    print("Global attributes: unable to define time zone")
                     ds.globalattributes["time_zone"] = ""
     # add or change global attributes as required
     gattr_list = sorted(list(cfg["Global"].keys()))
     for gattr in gattr_list:
         ds.globalattributes[gattr] = cfg["Global"][gattr]
     # remove deprecated global attributes
-    flag_list = [g for g in ds.globalattributes.keys() if "Flag" in g]
+    flag_list = [g for g in list(ds.globalattributes.keys()) if "Flag" in g]
     others_list = ["end_datetime", "start_datetime", "Functions", "doi"]
     remove_list = others_list + flag_list
     for gattr in list(ds.globalattributes.keys()):
@@ -178,7 +178,7 @@
         #pfp_utils.CreateVariable(ds, variable)
         #pfp_utils.DeleteVariable(ds, "Fc_single")
     # do nothing if Fc_single exists
-    labels = ds.series.keys()
+    labels = list(ds.series.keys())
     if "Fc_single" in labels:
         pass
     # Fc_single may be called Fc_storage
@@ -292,7 +292,7 @@
     """
     for attr in attributes:
         value = attributes[attr]
-        if not isinstance(value, basestring):
+        if not isinstance(value, str):
             continue
         if attr in ["rangecheck_lower", "rangecheck_upper", "diurnalcheck_numsd"]:
             if ("[" in value) and ("]" in value) and ("*" in value):
@@ -327,7 +327,7 @@
     xl_book = xlrd.open_workbook(xls_name)
     if site_name not in xl_book.sheet_names():
         msg = " Site " + str(site_name) + " not found in site information workbook"
-        print msg
+        print(msg)
         return {}
     xl_sheet = xl_book.sheet_by_name(site_name)
     nrows = xl_sheet.nrows
@@ -345,7 +345,7 @@
 if os.path.exists(cfg_name):
     cfg = ConfigObj(cfg_name)
 else:
-    print " 'map_old_to_new' control file not found"
+    print(" 'map_old_to_new' control file not found")
 
 rp = os.path.join(os.sep, "mnt", "OzFlux", "Sites")
 #sites = sorted([d for d in os.listdir(rp) if os.path.isdir(os.path.join(rp,d))])
@@ -360,15 +360,15 @@
     sp = os.path.join(rp, site, "Data", "Portal")
     op = os.path.join(rp, site, "Data", "Processed")
     if not os.path.isdir(sp):
-        print sp + " , skipping site ..."
+        print(sp + " , skipping site ...")
         continue
     files = sorted([f for f in os.listdir(sp) if ("L3" in f and ".nc" in f)])
     if len(files) == 0:
-        print "No files found in " + sp + " , skipping ..."
+        print("No files found in " + sp + " , skipping ...")
         continue
     for fn in files:
         ifp = os.path.join(sp, fn)
-        print "Converting " + fn
+        print("Converting " + fn)
         cfg["Files"]["in_filename"] = ifp
         # read the input file
         ds1 = pfp_io.nc_read_series(ifp)
RefactoringTool: Refactored ./utilities/erai2nc.py
--- ./utilities/erai2nc.py	(original)
+++ ./utilities/erai2nc.py	(refactored)
@@ -14,7 +14,7 @@
 import xlrd
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "erai2nc: the scripts directory is missing"
+    print("erai2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -66,7 +66,7 @@
 # get the site information from the site master spreadsheet
 site_info = read_site_master(xl_file_path, xl_sheet_name)
 # get a list of sites
-site_list = site_info.keys()
+site_list = list(site_info.keys())
 # and a list of the ERAI files to be processed
 erai_list = sorted(glob.glob(erai_path))
 #erai_list = ["/home/peter/OzFlux/ERAI/ERAI_2014.nc",
RefactoringTool: Refactored ./utilities/evi2nc.py
--- ./utilities/evi2nc.py	(original)
+++ ./utilities/evi2nc.py	(refactored)
@@ -8,7 +8,7 @@
 import sys
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "erai2nc: the scripts directory is missing"
+    print("erai2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -42,7 +42,7 @@
 for site in site_list:
     # get the site information from the control file
     site_name = cf["Sites"][site]["site_name"]
-    print "Processing "+site_name
+    print("Processing "+site_name)
     out_filepath = cf["Sites"][site]["out_filepath"]
     if not os.path.exists(out_filepath):
         os.makedirs(out_filepath)
@@ -100,7 +100,7 @@
     elif evi_interpolate.lower()=="cubic":
         k = 3
     else:
-        print "Unrecognised interpolation option, using linear ..."
+        print("Unrecognised interpolation option, using linear ...")
         k = 1
     s = scipy.interpolate.InterpolatedUnivariateSpline(modis_time_raw,evi_raw,k=k)
     evi_interp = s(modis_time)
@@ -108,7 +108,7 @@
     if evi_smooth_filter.lower()=="savitsky-golay":
         evi_interp_smooth = scipy.signal.savgol_filter(evi_interp,sg_num_points,sg_order,mode="mirror")
     else:
-        print "No smoothing applied to EVI"
+        print("No smoothing applied to EVI")
         evi_interp_smooth = numpy.array(evi_interp)
     # interpolate the smoothed EVI from the MODIS 16 day time step to the tower time step
     start_date = modis_dt_raw[0]
RefactoringTool: Refactored ./utilities/get_erai_from_ecmwf.py
--- ./utilities/get_erai_from_ecmwf.py	(original)
+++ ./utilities/get_erai_from_ecmwf.py	(refactored)
@@ -18,11 +18,11 @@
 erai_info["format"] = "netcdf"
 
 if len(sys.argv)==1:
-    print "Command line syntax is:"
-    print " python get_erai.py <country>"
-    print "where <country> can be;"
-    print " Australia"
-    print " USA"
+    print("Command line syntax is:")
+    print(" python get_erai.py <country>")
+    print("where <country> can be;")
+    print(" Australia")
+    print(" USA")
     sys.exit
 
 if sys.argv[1].lower()=="australia":
@@ -36,10 +36,10 @@
     start_date = "2016-01-01"
     end_date = "2017-01-01"
 else:
-    print "Unrecognised country option entered on command line"
-    print "Valid country options are:"
-    print " australia"
-    print " usa"
+    print("Unrecognised country option entered on command line")
+    print("Valid country options are:")
+    print(" australia")
+    print(" usa")
     sys.exit()
 
 server = ECMWFDataServer()
@@ -47,9 +47,9 @@
 ed = dateutil.parser.parse(end_date)
 start_year = sd.year
 end_year = ed.year
-year_list = range(start_year,end_year+1)
+year_list = list(range(start_year,end_year+1))
 for year in year_list:
-    print " Processing year: ",str(year)
+    print(" Processing year: ",str(year))
     sds = str(year)+"-01-01"
     edc = datetime.datetime(year+1,1,1,0,0,0)
     eds = edc.strftime("%Y-%m-%d")
RefactoringTool: Refactored ./utilities/get_isd_from_noaa.py
--- ./utilities/get_isd_from_noaa.py	(original)
+++ ./utilities/get_isd_from_noaa.py	(refactored)
@@ -19,7 +19,7 @@
 import xlrd
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "erai2nc: the scripts directory is missing"
+    print("erai2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -67,7 +67,7 @@
 logger.info("Processing site master spreadsheet")
 site_info = read_site_master(xl_file_path, xl_sheet_name)
 # get a list of sites
-site_list = site_info.keys()
+site_list = list(site_info.keys())
 # parse the control file to get a dictionary containing lists of the ISD sites
 # required for each year
 ftp_site_list = {}
RefactoringTool: Refactored ./utilities/isd2nc.py
--- ./utilities/isd2nc.py	(original)
+++ ./utilities/isd2nc.py	(refactored)
@@ -22,7 +22,7 @@
 import xlwt
 # pfp modules
 if not os.path.exists("../scripts/"):
-    print "PyFluxPro: the scripts directory is missing"
+    print("PyFluxPro: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -184,7 +184,7 @@
     # instance the output data structure
     ds_out = qcio.DataStructure()
     # copy the global attributes
-    for key in ds_in.globalattributes.keys():
+    for key in list(ds_in.globalattributes.keys()):
         ds_out.globalattributes[key] = ds_in.globalattributes[key]
     # add the time step
     ds_out.globalattributes["time_step"] = str(ts)
@@ -359,9 +359,9 @@
     # get a workbook
     xl_book = xlwt.Workbook()
     # get a list of the sheets to add
-    site_list = data.keys()
-    year_list = data[site_list[0]].keys()
-    stat_list = data[site_list[0]][year_list[0]].keys()
+    site_list = list(data.keys())
+    year_list = list(data[site_list[0]].keys())
+    stat_list = list(data[site_list[0]][year_list[0]].keys())
     # loop over the statistics
     for stat in stat_list:
         # add a worksheet for the statistics
@@ -373,7 +373,7 @@
         for row, site in enumerate(site_list):
             xl_sheet.write(row+1,0,site)
             for col, year in enumerate(year_list):
-                if stat in data[site][year].keys():
+                if stat in list(data[site][year].keys()):
                     xl_sheet.write(row+1,col+1,data[site][year][stat])
                 else:
                     xl_sheet.write(row+1,col+1,"")
@@ -391,7 +391,7 @@
 # read the site master spreadsheet
 site_info = read_site_master(xl_file_path, xl_sheet_name)
 # get a list of sites
-site_list = site_info.keys()
+site_list = list(site_info.keys())
 # creat a dictionary to hold the ISD site time steps
 isd_time_steps = OrderedDict()
 
@@ -431,15 +431,15 @@
     start_year = int(site_info[site]["Start year"])
     end_year = int(site_info[site]["End year"])
     # get the list of years to process
-    year_list = range(start_year,end_year+1)
+    year_list = list(range(start_year,end_year+1))
     for n, year in enumerate(year_list):
         # we will collect the data for each site for this year into a single dictionary
         ds_out = {}
         isd_year_path = os.path.join(isd_base_path,str(year))
         for isd_site in isd_site_list:
-            if isd_site not in isd_time_steps.keys():
+            if isd_site not in list(isd_time_steps.keys()):
                 isd_time_steps[isd_site] = OrderedDict()
-            if year not in isd_time_steps[isd_site].keys():
+            if year not in list(isd_time_steps[isd_site].keys()):
                 isd_time_steps[isd_site][year] = OrderedDict()
             isd_file_path = os.path.join(isd_year_path,str(isd_site)+"-"+str(year)+".gz")
             if not os.path.isfile(isd_file_path):
@@ -485,7 +485,7 @@
         for i in list(ds_out.keys()):
             start_datetime.append(ds_out[i].series["DateTime"]["Data"][0])
             end_datetime.append(ds_out[i].series["DateTime"]["Data"][-1])
-        print site, year
+        print(site, year)
         start = min(start_datetime)
         end = max(end_datetime)
         # now make a datetime series at the required time step from the earliest start
@@ -521,7 +521,7 @@
             ldt_one = ds_out[i].series["DateTime"]["Data"]
             idx = numpy.searchsorted(ldt_all, numpy.intersect1d(ldt_all, ldt_one))
             # then we get a list of the variables to copy
-            series_list = ds_out[i].series.keys()
+            series_list = list(ds_out[i].series.keys())
             # and remove the datetime
             if "DateTime" in series_list:
                 series_list.remove("DateTime")
RefactoringTool: Refactored ./utilities/modis2nc.py
--- ./utilities/modis2nc.py	(original)
+++ ./utilities/modis2nc.py	(refactored)
@@ -39,7 +39,7 @@
             bom_sites_info[str(xlrow[0])][str(int(xlrow[i+1]))]["elevation"] = xlrow[i+4]
             bom_sites_info[str(xlrow[0])][str(int(xlrow[i+1]))]["distance"] = xlrow[i+5]
 
-site_list = bom_sites_info.keys()
+site_list = list(bom_sites_info.keys())
 # loop over sites
 for site in site_list:
     # create a data structure
@@ -59,7 +59,7 @@
     # get a Python datetime series from the netCDF time
     modis_time_units = getattr(nc_file.variables["time"],"units")
     modis_dt =  netCDF4.num2date(modis_time,modis_time_units)
-    print modis_dt[0],modis_dt[-1]
+    print(modis_dt[0],modis_dt[-1])
     # get the index of points within the latitude and longitude bounds
     lat_index = numpy.where((lat>=lat_bound_lower)&(lat<=lat_bound_upper))[0]
     lon_index = numpy.where((lon>=lon_bound_lower)&(lon<=lon_bound_upper))[0]
RefactoringTool: Refactored ./utilities/modis_evi2nc.py
--- ./utilities/modis_evi2nc.py	(original)
+++ ./utilities/modis_evi2nc.py	(refactored)
@@ -11,7 +11,7 @@
 import sys
 # check the scripts directory is present
 if not os.path.exists("../scripts/"):
-    print "modis_evi2nc: the scripts directory is missing"
+    print("modis_evi2nc: the scripts directory is missing")
     sys.exit()
 # since the scripts directory is there, try importing the modules
 sys.path.append('../scripts')
@@ -50,7 +50,7 @@
 evi_time_units = "seconds since 1970-01-01 00:00:00.0"
 do_plots = True
 
-sites = cf["Sites"].keys()
+sites = list(cf["Sites"].keys())
 site = sites[0]
 site_timezone = cf["Sites"][site]["site_timezone"]
 site_latitude = float(cf["Sites"][site]["site_latitude"])
@@ -64,22 +64,22 @@
 mod_file_name = site+"_"+cf["Files"]["mod_file"]
 mod_file_path = os.path.join(cf["Files"]["base_path"],site,"Data","MODIS","MOD13Q1",mod_file_name)
 if not os.path.exists(mod_file_path):
-    print "modis_evi2nc: " + mod_file_path
-    print "modis_evi2nc: MOD13Q1 file not found"
+    print("modis_evi2nc: " + mod_file_path)
+    print("modis_evi2nc: MOD13Q1 file not found")
     sys.exit()
 evi_mod = read_evi_file(mod_file_path)
-names_mod = evi_mod.keys()
+names_mod = list(evi_mod.keys())
 # MYD13 (Aqua)
 myd_file_name = site+"_"+cf["Files"]["myd_file"]
 myd_file_path = os.path.join(cf["Files"]["base_path"],site,"Data","MODIS","MYD13Q1",myd_file_name)
 if not os.path.exists(myd_file_path):
-    print "modis_evi2nc: " + myd_file_path
-    print "modis_evi2nc: MYD13Q1 file not found"
+    print("modis_evi2nc: " + myd_file_path)
+    print("modis_evi2nc: MYD13Q1 file not found")
     sys.exit()
 evi_myd = read_evi_file(myd_file_path)
-names_myd = evi_myd.keys()
+names_myd = list(evi_myd.keys())
 if not (names_mod == names_myd):
-    print "modis_evi2nc: column names in MOD and MYD do not agree"
+    print("modis_evi2nc: column names in MOD and MYD do not agree")
     sys.exit()
 names = list(names_mod)
 if "DateTime" in names:
@@ -94,7 +94,7 @@
 for item in names:
     evi[item] = evi[item][idx]
 # QC the pixel values
-pixels = [l for l in evi.keys() if "EVI_pixel" in l]
+pixels = [l for l in list(evi.keys()) if "EVI_pixel" in l]
 pixel_values = numpy.vstack([evi[p] for p in pixels])
 # mask if below 0.1 or above 0.45
 pixel_values = numpy.ma.masked_where((pixel_values < evi_min)|(pixel_values > evi_max), pixel_values)
@@ -222,4 +222,4 @@
 out_file = pfp_io.nc_open_write(out_name)
 pfp_io.nc_write_series(out_file, ds, ndims=1)
 
-print "modis_evi2nc: finished"
+print("modis_evi2nc: finished")
RefactoringTool: Refactored ./utilities/read_example.py
--- ./utilities/read_example.py	(original)
+++ ./utilities/read_example.py	(refactored)
@@ -41,7 +41,7 @@
     # was a variable list passed in as variable_list?
     if len(variable_list)==0:
         # if not, get the variable list from the netCDF file contents
-        variable_list = nc_file.variables.keys()
+        variable_list = list(nc_file.variables.keys())
     else:
         # if so, add the QC flags to the list entered as an argument
         flag_list = []
@@ -76,11 +76,11 @@
 # read the variables from the local netCDF file
 nc_full_name = "../../Sites/Whroo/Data/Processed/all/Whroo_2011_to_2014_L6.nc"
 variable_list = ['Fsd','Ta','VPD','NEE_SOLO']
-print "reading local netCDF file"
+print("reading local netCDF file")
 df,attr = read_netcdf(nc_full_name,variable_list=variable_list)
 
 # plot the variables
-print "plotting local netCDF file"
+print("plotting local netCDF file")
 fig = plt.figure(1)
 plt.figtext(0.5,0.95,"Local file",horizontalalignment='center')
 ax1 = plt.subplot(411)
@@ -96,11 +96,11 @@
 # read the variables from the remote netCDF file
 nc_dap_name = "http://dap.ozflux.org.au/thredds/dodsC/ozflux/sites/Whroo/L6/Whroo_2011_to_2014_L6.nc"
 variable_list = ['Fsd','Ta','VPD','NEE_SOLO']
-print "reading remote netCDF file"
+print("reading remote netCDF file")
 df,attr = read_netcdf(nc_dap_name,variable_list=variable_list)
 
 # plot the variables
-print "plotting remote netCDF file"
+print("plotting remote netCDF file")
 fig = plt.figure(2)
 plt.figtext(0.5,0.95,"OPeNDAP file",horizontalalignment='center')
 ax1 = plt.subplot(411)
RefactoringTool: Files that need to be modified:
RefactoringTool: ./PyFluxPro.py
RefactoringTool: ./pfp_batch.py
RefactoringTool: ./scripts/cfg.py
RefactoringTool: ./scripts/constants.py
RefactoringTool: ./scripts/meteorologicalfunctions.py
RefactoringTool: ./scripts/pfp_cfg.py
RefactoringTool: ./scripts/pfp_ck.py
RefactoringTool: ./scripts/pfp_clim.py
RefactoringTool: ./scripts/pfp_compliance.py
RefactoringTool: ./scripts/pfp_cpd.py
RefactoringTool: ./scripts/pfp_cpd2.py
RefactoringTool: ./scripts/pfp_func.py
RefactoringTool: ./scripts/pfp_gf.py
RefactoringTool: ./scripts/pfp_gfALT.py
RefactoringTool: ./scripts/pfp_gfMDS.py
RefactoringTool: ./scripts/pfp_gfSOLO.py
RefactoringTool: ./scripts/pfp_gui.py
RefactoringTool: ./scripts/pfp_io.py
RefactoringTool: ./scripts/pfp_levels.py
RefactoringTool: ./scripts/pfp_log.py
RefactoringTool: ./scripts/pfp_mpt.py
RefactoringTool: ./scripts/pfp_plot.py
RefactoringTool: ./scripts/pfp_rp.py
RefactoringTool: ./scripts/pfp_rpLL.py
RefactoringTool: ./scripts/pfp_rpLT.py
RefactoringTool: ./scripts/pfp_top_level.py
RefactoringTool: ./scripts/pfp_ts.py
RefactoringTool: ./scripts/pfp_utils.py
RefactoringTool: ./scripts/pysolar.py
RefactoringTool: ./scripts/split_dialog.py
RefactoringTool: ./utilities/access_concatenate_daily.py
RefactoringTool: ./utilities/access_concatenate_monthly.py
RefactoringTool: ./utilities/aws2nc.py
RefactoringTool: ./utilities/aws_averageto60minutes.py
RefactoringTool: ./utilities/aws_update.py
RefactoringTool: ./utilities/bios2nc.py
RefactoringTool: ./utilities/cleanup_netcdf_files.py
RefactoringTool: ./utilities/erai2nc.py
RefactoringTool: ./utilities/evi2nc.py
RefactoringTool: ./utilities/get_erai_from_ecmwf.py
RefactoringTool: ./utilities/get_isd_from_noaa.py
RefactoringTool: ./utilities/isd2nc.py
RefactoringTool: ./utilities/modis2nc.py
RefactoringTool: ./utilities/modis_evi2nc.py
RefactoringTool: ./utilities/read_example.py
